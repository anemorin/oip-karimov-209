<!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 skin-theme-clientpref-day vector-sticky-header-enabled wp25eastereggs-enable-clientpref-1 vector-toc-available" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>Attention (machine learning) - Wikipedia</title>
<script>(function(){var className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 skin-theme-clientpref-day vector-sticky-header-enabled wp25eastereggs-enable-clientpref-1 vector-toc-available";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"c71f81dc-4f86-4b51-868c-33b3ab0a453f","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Attention_(machine_learning)","wgTitle":"Attention (machine learning)","wgCurRevisionId":1338022964,"wgRevisionId":1338022964,"wgArticleId":66001552,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","Short description matches Wikidata","Articles that may contain original research from June 2025","All articles that may contain original research","All articles with unsourced statements","Articles with unsourced statements from June 2025","Machine learning"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Attention_(machine_learning)","wgRelevantArticleId":66001552,"wgTempUserName":null,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgNoticeProject":"wikipedia","wgFlaggedRevsParams":{"tags":{"status":{"levels":1}}},"wgConfirmEditCaptchaNeededForGenericEdit":"hcaptcha","wgConfirmEditHCaptchaVisualEditorOnLoadIntegrationEnabled":false,"wgConfirmEditHCaptchaSiteKey":"5d0c670e-a5f4-4258-ad16-1f42792c9c62","wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":0,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":40000,"wgEditSubmitButtonLabelPublish":true,"wgVisualEditorPageIsDisambiguation":false,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":false,"wgVector2022LanguageInHeader":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q103701642","wgCheckUserClientHintsHeadersJsApi":["brands","architecture","bitness","fullVersionList","mobile","model","platform","platformVersion"],"GEHomepageSuggestedEditsEnableTopics":true,"wgGESuggestedEditsTaskTypes":{"taskTypes":["copyedit","link-recommendation"],"unavailableTaskTypes":[]},"wgGETopicsMatchModeEnabled":false,"wgGELevelingUpEnabledForUser":false,"wgGEUseTestKitchenExtension":true,"wgMetricsPlatformUserExperiments":{"active_experiments":[],"overrides":[],"enrolled":[],"assigned":[],"subject_ids":[],"sampling_units":[],"coordinator":[]},"wgTestKitchenUserExperiments":{"active_experiments":[],"overrides":[],"enrolled":[],"assigned":[],"subject_ids":[],"sampling_units":[],"coordinator":[]}};
RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.wikimediamessages.styles":"ready","ext.cite.styles":"ready","ext.math.styles":"ready","ext.tmh.player.styles":"ready","skins.vector.search.codex.styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","jquery.makeCollapsible.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready","ext.wp25EasterEggs.styles":"ready"};RLPAGEMODULES=["ext.parsermigration.survey","ext.cite.ux-enhancements","ext.math.polyfills","mediawiki.page.media","ext.tmh.player","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.js","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips","ext.gadget.switcher","ext.urlShortener.toolbar","ext.centralauth.centralautologin","mmv.bootstrap","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.echo.centralauth","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.cx.uls.quick.actions","wikibase.client.vector-2022","wikibase.databox.fromWikidata","ext.checkUser.clientHints","ext.quicksurveys.init","ext.growthExperiments.SuggestedEditSession","ext.xLab","ext.testKitchen","ext.wp25EasterEggs"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.tmh.player.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediamessages.styles%7Cext.wp25EasterEggs.styles%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.46.0-wmf.16">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Attention_mechanism_overview.svg/1280px-Attention_mechanism_overview.svg.png">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="894">
<meta name="viewport" content="width=1120">
<meta property="og:title" content="Attention (machine learning) - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/rest.php/v1/search" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="auth.wikimedia.org">
</head>
<body class="skin--responsive skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Attention_machine_learning rootpage-Attention_machine_learning skin-vector-2022 action-view">
<div id="mw-aria-live-region" class="mw-aria-live-region" aria-live="polite"></div><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header no-font-mode-scale">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  title="Main menu" >
	<input type="checkbox" id="vector-main-menu-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-main-menu-dropdown" class="vector-dropdown-checkbox "  aria-label="Main menu"  >
	<label id="vector-main-menu-dropdown-label" for="vector-main-menu-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>

<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li>
		</ul>
		
	</div>
</div>

	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li><li id="n-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li>
		</ul>
		
	</div>
</div>

</div>

				</div>

	</div>
</div>

		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/enwiki-25.svg" alt="" aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container skin-invert">
		<img class="mw-logo-wordmark" alt="Wikipedia" src="/static/images/mobile/copyright/wikipedia-wordmark-en-25.svg" style="width: 8.75em; height: 1.375em;">
		<img class="mw-logo-tagline" alt="The Free Encyclopedia" src="/static/images/mobile/copyright/wikipedia-tagline-en-25.svg" width="140" height="11" style="width: 8.75em; height: 0.6875em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle" title="Search Wikipedia [f]" accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input mw-searchInput" autocomplete="off"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="none" spellcheck="false" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links vector-user-links-wide" aria-label="Personal tools">
	<div class="vector-user-links-main">
	
<div id="p-vector-user-menu-preferences" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-userpage" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	<nav class="vector-appearance-landmark" aria-label="Appearance">
		
<div id="vector-appearance-dropdown" class="vector-dropdown "  title="Change the appearance of the page&#039;s font size, width, and color" >
	<input type="checkbox" id="vector-appearance-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-appearance-dropdown" class="vector-dropdown-checkbox "  aria-label="Appearance"  >
	<label id="vector-appearance-dropdown-label" for="vector-appearance-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-appearance mw-ui-icon-wikimedia-appearance"></span>

<span class="vector-dropdown-label-text">Appearance</span>
	</label>
	<div class="vector-dropdown-content">


			<div id="vector-appearance-unpinned-container" class="vector-unpinned-container">
				
			</div>
		
	</div>
</div>

	</nav>
	
<div id="p-vector-user-menu-notifications" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="pt-sitesupport-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en" class=""><span>Donate</span></a>
</li>
<li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="/w/index.php?title=Special:CreateAccount&amp;returnto=Attention+%28machine+learning%29" title="You are encouraged to create an account and log in; however, it is not mandatory" class=""><span>Create account</span></a>
</li>
<li id="pt-login-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="/w/index.php?title=Special:UserLogin&amp;returnto=Attention+%28machine+learning%29" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o" class=""><span>Log in</span></a>
</li>

			
		</ul>
		
	</div>
</div>

	</div>
	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out user-links-collapsible-item"  title="Log in and more options" >
	<input type="checkbox" id="vector-user-links-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-user-links-dropdown" class="vector-dropdown-checkbox "  aria-label="Personal tools"  >
	<label id="vector-user-links-dropdown-label" for="vector-user-links-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>

<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-sitesupport" class="user-links-collapsible-item mw-list-item"><a href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en"><span>Donate</span></a></li><li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Attention+%28machine+learning%29" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Attention+%28machine+learning%29" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

	
	</div>
</div>

</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-sitenotice-container">
			<div id="siteNotice"><!-- CentralNotice --><div class="wp25eastereggs-sitenotice"><div class="wp25eastereggs-sitenotice-landmark"></div></div></div>
		</div>
		<div class="vector-column-start">
			<div class="vector-main-menu-container">
		<div id="mw-navigation">
			<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site">
				<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
				</div>
		</nav>
		</div>
	</div>
	<div class="vector-sticky-pinned-container">
				<nav id="mw-panel-toc" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark">
					<div id="vector-toc-pinned-container" class="vector-pinned-container">
					<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-History"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#History">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">1</span>
				<span>History</span>
			</div>
		</a>
		
		<ul id="toc-History-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Overview"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Overview">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">2</span>
				<span>Overview</span>
			</div>
		</a>
		
			<button aria-controls="toc-Overview-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Overview subsection</span>
			</button>
		
		<ul id="toc-Overview-sublist" class="vector-toc-list">
			<li id="toc-Interpreting_attention_weights"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Interpreting_attention_weights">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">2.1</span>
					<span>Interpreting attention weights</span>
				</div>
			</a>
			
			<ul id="toc-Interpreting_attention_weights-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Variants"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Variants">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">3</span>
				<span>Variants</span>
			</div>
		</a>
		
		<ul id="toc-Variants-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Optimizations"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Optimizations">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">4</span>
				<span>Optimizations</span>
			</div>
		</a>
		
			<button aria-controls="toc-Optimizations-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Optimizations subsection</span>
			</button>
		
		<ul id="toc-Optimizations-sublist" class="vector-toc-list">
			<li id="toc-Flash_attention"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Flash_attention">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.1</span>
					<span>Flash attention</span>
				</div>
			</a>
			
			<ul id="toc-Flash_attention-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-FlexAttention"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#FlexAttention">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.2</span>
					<span>FlexAttention</span>
				</div>
			</a>
			
			<ul id="toc-FlexAttention-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Applications"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Applications">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">5</span>
				<span>Applications</span>
			</div>
		</a>
		
			<button aria-controls="toc-Applications-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Applications subsection</span>
			</button>
		
		<ul id="toc-Applications-sublist" class="vector-toc-list">
			<li id="toc-Attention_maps_as_explanations_for_vision_transformers"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Attention_maps_as_explanations_for_vision_transformers">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">5.1</span>
					<span>Attention maps as explanations for vision transformers</span>
				</div>
			</a>
			
			<ul id="toc-Attention_maps_as_explanations_for_vision_transformers-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Mathematical_representation"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Mathematical_representation">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">6</span>
				<span>Mathematical representation</span>
			</div>
		</a>
		
			<button aria-controls="toc-Mathematical_representation-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Mathematical representation subsection</span>
			</button>
		
		<ul id="toc-Mathematical_representation-sublist" class="vector-toc-list">
			<li id="toc-Standard_scaled_dot-product_attention"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Standard_scaled_dot-product_attention">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">6.1</span>
					<span>Standard scaled dot-product attention</span>
				</div>
			</a>
			
			<ul id="toc-Standard_scaled_dot-product_attention-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Masked_attention"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Masked_attention">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">6.2</span>
					<span>Masked attention</span>
				</div>
			</a>
			
			<ul id="toc-Masked_attention-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Multi-head_attention"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Multi-head_attention">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">6.3</span>
					<span>Multi-head attention</span>
				</div>
			</a>
			
			<ul id="toc-Multi-head_attention-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Bahdanau_(additive)_attention"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Bahdanau_(additive)_attention">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">6.4</span>
					<span>Bahdanau (additive) attention</span>
				</div>
			</a>
			
			<ul id="toc-Bahdanau_(additive)_attention-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Luong_attention_(general)"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Luong_attention_(general)">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">6.5</span>
					<span>Luong attention (general)</span>
				</div>
			</a>
			
			<ul id="toc-Luong_attention_(general)-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Self-attention"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Self-attention">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">6.6</span>
					<span>Self-attention</span>
				</div>
			</a>
			
			<ul id="toc-Self-attention-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Masking"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Masking">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">6.7</span>
					<span>Masking</span>
				</div>
			</a>
			
			<ul id="toc-Masking-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">7</span>
				<span>See also</span>
			</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">8</span>
				<span>References</span>
			</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-External_links"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#External_links">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">9</span>
				<span>External links</span>
			</div>
		</a>
		
		<ul id="toc-External_links-sublist" class="vector-toc-list">
		</ul>
	</li>
</ul>
</div>

					</div>
		</nav>
			</div>
		</div>
		<div class="mw-content-container">
			<main id="content" class="mw-body">
				<header class="mw-body-header vector-page-titlebar no-font-mode-scale">
					<nav aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  title="Table of Contents" >
	<input type="checkbox" id="vector-page-titlebar-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-titlebar-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
	<label id="vector-page-titlebar-toc-label" for="vector-page-titlebar-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>

					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">Attention (machine learning)</span></h1>
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox" id="p-lang-btn-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-lang-btn" class="vector-dropdown-checkbox mw-interlanguage-selector" aria-label="Go to an article in another language. Available in 14 languages"   >
	<label id="p-lang-btn-label" for="p-lang-btn-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-14" aria-hidden="true"  ><span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>

<span class="vector-dropdown-label-text">14 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list">
				
				<li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D8%A7%D9%86%D8%AA%D8%A8%D8%A7%D9%87_(%D8%AA%D8%B9%D9%84%D9%85_%D8%A7%D9%84%D8%A2%D9%84%D8%A9)" title="انتباه (تعلم الآلة) – Arabic" lang="ar" hreflang="ar" data-title="انتباه (تعلم الآلة)" data-language-autonym="العربية" data-language-local-name="Arabic" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Atenci%C3%B3_(aprenentatge_autom%C3%A0tic)" title="Atenció (aprenentatge automàtic) – Catalan" lang="ca" hreflang="ca" data-title="Atenció (aprenentatge automàtic)" data-language-autonym="Català" data-language-local-name="Catalan" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/Atenci%C3%B3n_(aprendizaje_autom%C3%A1tico)" title="Atención (aprendizaje automático) – Spanish" lang="es" hreflang="es" data-title="Atención (aprendizaje automático)" data-language-autonym="Español" data-language-local-name="Spanish" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/%D8%AA%D9%88%D8%AC%D9%87_(%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D9%85%D8%A7%D8%B4%DB%8C%D9%86)" title="توجه (یادگیری ماشین) – Persian" lang="fa" hreflang="fa" data-title="توجه (یادگیری ماشین)" data-language-autonym="فارسی" data-language-local-name="Persian" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/Attention_(apprentissage_automatique)" title="Attention (apprentissage automatique) – French" lang="fr" hreflang="fr" data-title="Attention (apprentissage automatique)" data-language-autonym="Français" data-language-local-name="French" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-he mw-list-item"><a href="https://he.wikipedia.org/wiki/Attention_(%D7%91%D7%99%D7%A0%D7%94_%D7%9E%D7%9C%D7%90%D7%9B%D7%95%D7%AA%D7%99%D7%AA)" title="Attention (בינה מלאכותית) – Hebrew" lang="he" hreflang="he" data-title="Attention (בינה מלאכותית)" data-language-autonym="עברית" data-language-local-name="Hebrew" class="interlanguage-link-target"><span>עברית</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%86%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%B3_(%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92)" title="アテンション (機械学習) – Japanese" lang="ja" hreflang="ja" data-title="アテンション (機械学習)" data-language-autonym="日本語" data-language-local-name="Japanese" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/%EC%96%B4%ED%85%90%EC%85%98_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)" title="어텐션 (기계 학습) – Korean" lang="ko" hreflang="ko" data-title="어텐션 (기계 학습)" data-language-autonym="한국어" data-language-local-name="Korean" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-pl mw-list-item"><a href="https://pl.wikipedia.org/wiki/Mechanizm_uwagi_(uczenie_maszynowe)" title="Mechanizm uwagi (uczenie maszynowe) – Polish" lang="pl" hreflang="pl" data-title="Mechanizm uwagi (uczenie maszynowe)" data-language-autonym="Polski" data-language-local-name="Polish" class="interlanguage-link-target"><span>Polski</span></a></li><li class="interlanguage-link interwiki-sr mw-list-item"><a href="https://sr.wikipedia.org/wiki/Pa%C5%BEnja_(ma%C5%A1insko_u%C4%8Denje)" title="Pažnja (mašinsko učenje) – Serbian" lang="sr" hreflang="sr" data-title="Pažnja (mašinsko učenje)" data-language-autonym="Српски / srpski" data-language-local-name="Serbian" class="interlanguage-link-target"><span>Српски / srpski</span></a></li><li class="interlanguage-link interwiki-th mw-list-item"><a href="https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%A5%E0%B9%84%E0%B8%81%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B9%83%E0%B8%AA%E0%B9%88%E0%B9%83%E0%B8%88" title="กลไกความใส่ใจ – Thai" lang="th" hreflang="th" data-title="กลไกความใส่ใจ" data-language-autonym="ไทย" data-language-local-name="Thai" class="interlanguage-link-target"><span>ไทย</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%A3%D0%B2%D0%B0%D0%B3%D0%B0_(%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%B5_%D0%BD%D0%B0%D0%B2%D1%87%D0%B0%D0%BD%D0%BD%D1%8F)" title="Увага (машинне навчання) – Ukrainian" lang="uk" hreflang="uk" data-title="Увага (машинне навчання)" data-language-autonym="Українська" data-language-local-name="Ukrainian" class="interlanguage-link-target"><span>Українська</span></a></li><li class="interlanguage-link interwiki-zh-yue mw-list-item"><a href="https://zh-yue.wikipedia.org/wiki/%E9%97%9C%E6%B3%A8%E6%A9%9F%E5%88%B6" title="關注機制 – Cantonese" lang="yue" hreflang="yue" data-title="關注機制" data-language-autonym="粵語" data-language-local-name="Cantonese" class="interlanguage-link-target"><span>粵語</span></a></li><li class="interlanguage-link interwiki-zh mw-list-item"><a href="https://zh.wikipedia.org/wiki/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6" title="注意力机制 – Chinese" lang="zh" hreflang="zh" data-title="注意力机制" data-language-autonym="中文" data-language-local-name="Chinese" class="interlanguage-link-target"><span>中文</span></a></li>
			</ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q103701642#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
</header>
				<div class="vector-page-toolbar vector-feature-custom-font-size-clientpref--excluded">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Attention_(machine_learning)" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="vector-tab-noicon mw-list-item"><a href="/wiki/Talk:Attention_(machine_learning)" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

								
<div id="vector-variants-dropdown" class="vector-dropdown emptyPortlet"  >
	<input type="checkbox" id="vector-variants-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-variants-dropdown" class="vector-dropdown-checkbox " aria-label="Change language variant"   >
	<label id="vector-variants-dropdown-label" for="vector-variants-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">


					
<div id="p-variants" class="vector-menu mw-portlet mw-portlet-variants emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

				
	</div>
</div>

							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Attention_(machine_learning)"><span>Read</span></a></li><li id="ca-edit" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="Page tools">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox" id="vector-page-tools-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-tools-dropdown" class="vector-dropdown-checkbox "  aria-label="Tools"  >
	<label id="vector-page-tools-dropdown-label" for="vector-page-tools-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/Attention_(machine_learning)"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=history"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Attention_(machine_learning)" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Attention_(machine_learning)" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Attention_(machine_learning)&amp;oldid=1338022964" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Attention_%28machine_learning%29&amp;id=1338022964&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-urlshortener" class="mw-list-item"><a href="/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAttention_%28machine_learning%29"><span>Get shortened URL</span></a></li><li id="t-urlshortener-qrcode" class="mw-list-item"><a href="/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAttention_%28machine_learning%29"><span>Download QR code</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Attention_%28machine_learning%29&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Attention_(machine_learning)&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-wikibase-otherprojects" class="vector-menu mw-portlet mw-portlet-wikibase-otherprojects"  >
	<div class="vector-menu-heading">
		In other projects
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-wikibase" class="wb-otherproject-link wb-otherproject-wikibase-dataitem mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q103701642" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li>
		</ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>

							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end no-font-mode-scale">
					<div class="vector-sticky-pinned-container">
						<div class="wp25eastereggs-vector-sitenotice-landmark"></div>
						<nav class="vector-page-tools-landmark" aria-label="Page tools">
							<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
				
							</div>
		</nav>
						<nav class="vector-appearance-landmark" aria-label="Appearance">
							<div id="vector-appearance-pinned-container" class="vector-pinned-container">
				<div id="vector-appearance" class="vector-appearance vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-appearance-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="appearance-pinned"
	data-pinnable-element-id="vector-appearance"
	data-pinned-container-id="vector-appearance-pinned-container"
	data-unpinned-container-id="vector-appearance-unpinned-container"
>
	<div class="vector-pinnable-header-label">Appearance</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-appearance.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-appearance.unpin">hide</button>
</div>


</div>

							</div>
		</nav>
					</div>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content"><div class="mw-subjectpageheader">
</div><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Machine learning technique</div>
<style data-mw-deduplicate="TemplateStyles:r1333133064">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:"\a0 · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1246091330">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}}</style><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886047488" /><table class="sidebar sidebar-collapse nomobile nowraplinks" role="navigation"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br />and <a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Paradigms</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" class="mw-redirect" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Meta-learning_(computer_science)" title="Meta-learning (computer science)">Meta-learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Batch_learning" class="mw-redirect" title="Batch learning">Batch learning</a></li>
<li><a href="/wiki/Curriculum_learning" title="Curriculum learning">Curriculum learning</a></li>
<li><a href="/wiki/Rule-based_machine_learning" title="Rule-based machine learning">Rule-based learning</a></li>
<li><a href="/wiki/Neuro-symbolic_AI" title="Neuro-symbolic AI">Neuro-symbolic AI</a></li>
<li><a href="/wiki/Neuromorphic_engineering" class="mw-redirect" title="Neuromorphic engineering">Neuromorphic engineering</a></li>
<li><a href="/wiki/Quantum_machine_learning" title="Quantum machine learning">Quantum machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Problems</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Generative_model" title="Generative model">Generative modeling</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></li>
<li><a href="/wiki/Density_estimation" title="Density estimation">Density estimation</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Data_cleaning" class="mw-redirect" title="Data cleaning">Data cleaning</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Semantic_analysis_(machine_learning)" title="Semantic analysis (machine learning)">Semantic analysis</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
<li><a href="/wiki/Ontology_learning" title="Ontology learning">Ontology learning</a></li>
<li><a href="/wiki/Multimodal_learning" title="Multimodal learning">Multimodal learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><div style="display: inline-block; line-height: 1.2em; padding: .1em 0;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><span class="nobold"><span style="font-size: 85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Apprenticeship_learning" title="Apprenticeship learning">Apprenticeship learning</a></li>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/CURE_algorithm" title="CURE algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Fuzzy_clustering" title="Fuzzy clustering">Fuzzy</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
<li><a href="/wiki/Sparse_dictionary_learning" title="Sparse dictionary learning">SDL</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Random_sample_consensus" title="Random sample consensus">RANSAC</a></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
<li><a href="/wiki/Isolation_forest" title="Isolation forest">Isolation forest</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">Neural networks</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">Feedforward neural network</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li>
<li><a href="/wiki/Reservoir_computing" title="Reservoir computing">reservoir computing</a></li></ul></li>
<li><a href="/wiki/Boltzmann_machine" title="Boltzmann machine">Boltzmann machine</a>
<ul><li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted</a></li></ul></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Diffusion_model" title="Diffusion model">Diffusion model</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li>
<li><a href="/wiki/LeNet" title="LeNet">LeNet</a></li>
<li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li></ul></li>
<li><a href="/wiki/Neural_field" title="Neural field">Neural field</a>
<ul><li><a href="/wiki/Neural_radiance_field" title="Neural radiance field">Neural radiance field</a></li>
<li><a href="/wiki/Physics-informed_neural_networks" title="Physics-informed neural networks">Physics-informed neural networks</a></li></ul></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" class="mw-redirect" title="Transformer (deep learning architecture)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision</a></li></ul></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Spiking_neural_network" title="Spiking neural network">Spiking neural network</a></li>
<li><a href="/wiki/Memtransistor" title="Memtransistor">Memtransistor</a></li>
<li><a href="/wiki/Electrochemical_RAM" title="Electrochemical RAM">Electrochemical RAM</a> (ECRAM)</li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/Policy_gradient_method" title="Policy gradient method">Policy gradient</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li>
<li><a href="/wiki/Multi-agent_reinforcement_learning" title="Multi-agent reinforcement learning">Multi-agent</a>
<ul><li><a href="/wiki/Self-play_(reinforcement_learning_technique)" class="mw-redirect" title="Self-play (reinforcement learning technique)">Self-play</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Learning with humans</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Active_learning_(machine_learning)" title="Active learning (machine learning)">Active learning</a></li>
<li><a href="/wiki/Crowdsourcing" title="Crowdsourcing">Crowdsourcing</a></li>
<li><a href="/wiki/Human-in-the-loop" title="Human-in-the-loop">Human-in-the-loop</a></li>
<li><a href="/wiki/Mechanistic_interpretability" title="Mechanistic interpretability">Mechanistic interpretability</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Model diagnostics</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Coefficient_of_determination" title="Coefficient of determination">Coefficient of determination</a></li>
<li><a href="/wiki/Confusion_matrix" title="Confusion matrix">Confusion matrix</a></li>
<li><a href="/wiki/Learning_curve_(machine_learning)" title="Learning curve (machine learning)">Learning curve</a></li>
<li><a href="/wiki/Receiver_operating_characteristic" title="Receiver operating characteristic">ROC curve</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Mathematical foundations</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Kernel_machines" class="mw-redirect" title="Kernel machines">Kernel machines</a></li>
<li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li>
<li><a href="/wiki/Topological_deep_learning" title="Topological deep learning">Topological deep learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Journals and conferences</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/AAAI_Conference_on_Artificial_Intelligence" title="AAAI Conference on Artificial Intelligence">AAAI</a></li>
<li><a href="/wiki/ECML_PKDD" title="ECML PKDD">ECML PKDD</a></li>
<li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/International_Conference_on_Learning_Representations" title="International Conference on Learning Representations">ICLR</a></li>
<li><a href="/wiki/International_Joint_Conference_on_Artificial_Intelligence" title="International Joint Conference on Artificial Intelligence">IJCAI</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Related articles</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li>
<li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a>
<ul><li><a href="/wiki/List_of_datasets_in_computer_vision_and_image_processing" title="List of datasets in computer vision and image processing">List of datasets in computer vision and image processing</a></li></ul></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-navbar"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333133064" /><style data-mw-deduplicate="TemplateStyles:r1239400231">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning" title="Template:Machine learning"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning" title="Template talk:Machine learning"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Machine_learning" title="Special:EditPage/Template:Machine learning"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Attention_mechanism_overview.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/6/62/Attention_mechanism_overview.svg/250px-Attention_mechanism_overview.svg.png" decoding="async" width="250" height="186" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/62/Attention_mechanism_overview.svg/500px-Attention_mechanism_overview.svg.png 1.5x" data-file-width="251" data-file-height="187" /></a><figcaption>Attention mechanism, overview</figcaption></figure>
<p>In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b>attention</b> is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>, importance is represented by "soft" weights assigned to each word in a sentence. More generally, attention encodes <a href="/wiki/Vector_(mathematics_and_physics)" title="Vector (mathematics and physics)">vectors</a> called <a href="/wiki/Lexical_token" class="mw-redirect" title="Lexical token">token</a> <a href="/wiki/Word_embedding" title="Word embedding">embeddings</a> across a fixed-width <a href="/wiki/Context_window" title="Context window">sequence</a> that can range from tens to millions of tokens in size.
</p><p>Unlike "hard" weights, which are computed during the backwards training pass, "soft" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> (RNN) language translation system, but a more recent design, namely the <a href="/wiki/Transformer_(machine_learning_model)" class="mw-redirect" title="Transformer (machine learning model)">transformer</a>, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.
</p><p>Inspired by ideas about <a href="/wiki/Attention" title="Attention">attention in humans</a>, the attention mechanism was developed to address the weaknesses of using information from the <a href="/wiki/Hidden_layer" title="Hidden layer">hidden layers</a> of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">attenuated</a>. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.
</p>
<meta property="mw:PageProp/toc" />
<div class="mw-heading mw-heading2"><h2 id="History">History</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=1" title="Edit section: History"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1320445320">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}</style><div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Timeline_of_machine_learning" title="Timeline of machine learning">Timeline of machine learning</a></div>
<table class="wikitable">
<tbody><tr>
<td>1950s–1960s
</td>
<td>Psychology and biology of attention. <a href="/wiki/Cocktail_party_effect" title="Cocktail party effect">Cocktail party effect</a><sup id="cite&#95;ref-Cherry&#95;1953&#95;1-0" class="reference"><a href="#cite_note-Cherry_1953-1"><span class="cite-bracket">&#91;</span>1<span class="cite-bracket">&#93;</span></a></sup> — focusing on content by filtering out background noise. <a href="/wiki/Broadbent%27s_filter_model_of_attention" title="Broadbent&#39;s filter model of attention">Filter model of attention</a>,<sup id="cite&#95;ref-Broadbent&#95;2-0" class="reference"><a href="#cite_note-Broadbent-2"><span class="cite-bracket">&#91;</span>2<span class="cite-bracket">&#93;</span></a></sup> <a href="/wiki/Iconic_memory#Sperling&#39;s_partial_report_procedure" title="Iconic memory">partial report paradigm</a>, and <a href="/wiki/Saccade" title="Saccade">saccade control</a>.<sup id="cite&#95;ref-Kowler1995&#95;3-0" class="reference"><a href="#cite_note-Kowler1995-3"><span class="cite-bracket">&#91;</span>3<span class="cite-bracket">&#93;</span></a></sup>
</td></tr>
<tr>
<td>1980s
</td>
<td>Sigma-pi units,<sup id="cite&#95;ref-PDP&#95;4-0" class="reference"><a href="#cite_note-PDP-4"><span class="cite-bracket">&#91;</span>4<span class="cite-bracket">&#93;</span></a></sup> higher-order neural networks.
</td></tr>
<tr>
<td>1990s
</td>
<td><i>Fast weight controllers</i> and dynamic links between neurons, anticipating key-value mechanisms in attention.<sup id="cite&#95;ref-transform1992&#95;5-0" class="reference"><a href="#cite_note-transform1992-5"><span class="cite-bracket">&#91;</span>5<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-malsburg1981&#95;6-0" class="reference"><a href="#cite_note-malsburg1981-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-feldman1982&#95;7-0" class="reference"><a href="#cite_note-feldman1982-7"><span class="cite-bracket">&#91;</span>7<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-hinton1987&#95;8-0" class="reference"><a href="#cite_note-hinton1987-8"><span class="cite-bracket">&#91;</span>8<span class="cite-bracket">&#93;</span></a></sup>
</td></tr>
<tr>
<td>1998
</td>
<td>The <a href="/wiki/Bilateral_filter" title="Bilateral filter">bilateral filter</a> was introduced in image processing. It uses pairwise affinity matrices to propagate relevance across elements.<sup id="cite&#95;ref-tomasi1998&#95;9-0" class="reference"><a href="#cite_note-tomasi1998-9"><span class="cite-bracket">&#91;</span>9<span class="cite-bracket">&#93;</span></a></sup>
</td></tr>
<tr>
<td>2005
</td>
<td>Non-local means extended affinity-based filtering in image denoising, using Gaussian similarity kernels as fixed attention-like weights.<sup id="cite&#95;ref-buades20052&#95;10-0" class="reference"><a href="#cite_note-buades20052-10"><span class="cite-bracket">&#91;</span>10<span class="cite-bracket">&#93;</span></a></sup>
</td></tr>
<tr>
<td>2014
</td>
<td><a href="/wiki/Seq2seq" title="Seq2seq">seq2seq</a> with RNN + Attention.<sup id="cite&#95;ref-bahdanau&#95;11-0" class="reference"><a href="#cite_note-bahdanau-11"><span class="cite-bracket">&#91;</span>11<span class="cite-bracket">&#93;</span></a></sup> Attention was introduced to enhance RNN encoder-decoder translation, particularly for long sentences. See Overview section.
<p>Attentional Neural Networks introduced a learned feature selection mechanism using top-down cognitive modulation, showing how attention weights can highlight relevant inputs.<sup id="cite&#95;ref-wang2014&#95;12-0" class="reference"><a href="#cite_note-wang2014-12"><span class="cite-bracket">&#91;</span>12<span class="cite-bracket">&#93;</span></a></sup>
</p>
</td></tr>
<tr>
<td>2015
</td>
<td>Attention was extended to vision for image captioning tasks.<sup id="cite&#95;ref-xu2015&#95;13-0" class="reference"><a href="#cite_note-xu2015-13"><span class="cite-bracket">&#91;</span>13<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-vinyals2015&#95;14-0" class="reference"><a href="#cite_note-vinyals2015-14"><span class="cite-bracket">&#91;</span>14<span class="cite-bracket">&#93;</span></a></sup>
</td></tr>
<tr>
<td>2016
</td>
<td>Self-attention was integrated into RNN-based models to capture intra-sequence dependencies.<sup id="cite&#95;ref-cheng2016&#95;15-0" class="reference"><a href="#cite_note-cheng2016-15"><span class="cite-bracket">&#91;</span>15<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-paulus2017&#95;16-0" class="reference"><a href="#cite_note-paulus2017-16"><span class="cite-bracket">&#91;</span>16<span class="cite-bracket">&#93;</span></a></sup>
<p>Self-attention was explored in decomposable attention models for natural language inference<sup id="cite&#95;ref-parikh2016&#95;17-0" class="reference"><a href="#cite_note-parikh2016-17"><span class="cite-bracket">&#91;</span>17<span class="cite-bracket">&#93;</span></a></sup> and structured self-attentive sentence embeddings.<sup id="cite&#95;ref-lin2017&#95;18-0" class="reference"><a href="#cite_note-lin2017-18"><span class="cite-bracket">&#91;</span>18<span class="cite-bracket">&#93;</span></a></sup>
</p>
</td></tr>
<tr>
<td>2017
</td>
<td>The <a href="/wiki/Transformer_(machine_learning_model)" class="mw-redirect" title="Transformer (machine learning model)">Transformer</a> architecture introduced in the research paper <a href="/wiki/Attention_Is_All_You_Need" title="Attention Is All You Need">Attention is All You Need</a><sup id="cite&#95;ref-allyouneed&#95;19-0" class="reference"><a href="#cite_note-allyouneed-19"><span class="cite-bracket">&#91;</span>19<span class="cite-bracket">&#93;</span></a></sup> formalized scaled dot-product self-attention:
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle A={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}\right)V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>A</mi>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>softmax</mtext>
        </mrow>
        <mrow>
          <mo>(</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mfrac>
              <mrow>
                <mi>Q</mi>
                <msup>
                  <mi>K</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>T</mi>
                  </mrow>
                </msup>
              </mrow>
              <msqrt>
                <msub>
                  <mi>d</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>k</mi>
                  </mrow>
                </msub>
              </msqrt>
            </mfrac>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle A={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}\right)V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/832efdc6eb8eb84cbd581143634ec573b28eb421" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:25.263ex; height:7.509ex;" alt="{\displaystyle A={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}\right)V}"></span></dd></dl>
<p>Relation networks<sup id="cite&#95;ref-santoro2017&#95;20-0" class="reference"><a href="#cite_note-santoro2017-20"><span class="cite-bracket">&#91;</span>20<span class="cite-bracket">&#93;</span></a></sup> and set Transformers<sup id="cite&#95;ref-lee2019&#95;21-0" class="reference"><a href="#cite_note-lee2019-21"><span class="cite-bracket">&#91;</span>21<span class="cite-bracket">&#93;</span></a></sup> applied attention to unordered sets and relational reasoning, generalizing pairwise interaction models.
</p>
</td></tr>
<tr>
<td>2018
</td>
<td>Non-local neural networks<sup id="cite&#95;ref-wang2018&#95;22-0" class="reference"><a href="#cite_note-wang2018-22"><span class="cite-bracket">&#91;</span>22<span class="cite-bracket">&#93;</span></a></sup> extended attention to computer vision by capturing long-range dependencies in space and time. Graph attention networks<sup id="cite&#95;ref-velickovic2018&#95;23-0" class="reference"><a href="#cite_note-velickovic2018-23"><span class="cite-bracket">&#91;</span>23<span class="cite-bracket">&#93;</span></a></sup> applied attention mechanisms to graph-structured data.
</td></tr>
<tr>
<td>2019–2020
</td>
<td>Efficient Transformers, including Reformer,<sup id="cite&#95;ref-reformer2020&#95;24-0" class="reference"><a href="#cite_note-reformer2020-24"><span class="cite-bracket">&#91;</span>24<span class="cite-bracket">&#93;</span></a></sup> Linformer,<sup id="cite&#95;ref-linformer2020&#95;25-0" class="reference"><a href="#cite_note-linformer2020-25"><span class="cite-bracket">&#91;</span>25<span class="cite-bracket">&#93;</span></a></sup> and Performer,<sup id="cite&#95;ref-performer2020&#95;26-0" class="reference"><a href="#cite_note-performer2020-26"><span class="cite-bracket">&#91;</span>26<span class="cite-bracket">&#93;</span></a></sup> introduced scalable approximations of attention for long sequences.
</td></tr>
<tr>
<td>2019+
</td>
<td>Hopfield networks were reinterpreted as associative memory-based attention systems,<sup id="cite&#95;ref-ramsauer2021&#95;27-0" class="reference"><a href="#cite_note-ramsauer2021-27"><span class="cite-bracket">&#91;</span>27<span class="cite-bracket">&#93;</span></a></sup> and <a href="/wiki/Vision_transformer" title="Vision transformer">vision transformers</a> (ViTs) achieved competitive results in image classification.<sup id="cite&#95;ref-dosovitskiy2021&#95;28-0" class="reference"><a href="#cite_note-dosovitskiy2021-28"><span class="cite-bracket">&#91;</span>28<span class="cite-bracket">&#93;</span></a></sup>
<p>Transformers were adopted across scientific domains, including <a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a> for protein folding,<sup id="cite&#95;ref-alphafold&#95;29-0" class="reference"><a href="#cite_note-alphafold-29"><span class="cite-bracket">&#91;</span>29<span class="cite-bracket">&#93;</span></a></sup> CLIP for vision-language pretraining,<sup id="cite&#95;ref-clip&#95;30-0" class="reference"><a href="#cite_note-clip-30"><span class="cite-bracket">&#91;</span>30<span class="cite-bracket">&#93;</span></a></sup> and attention-based dense segmentation models like CCNet<sup id="cite&#95;ref-ccnet&#95;31-0" class="reference"><a href="#cite_note-ccnet-31"><span class="cite-bracket">&#91;</span>31<span class="cite-bracket">&#93;</span></a></sup> and DANet.<sup id="cite&#95;ref-danet&#95;32-0" class="reference"><a href="#cite_note-danet-32"><span class="cite-bracket">&#91;</span>32<span class="cite-bracket">&#93;</span></a></sup>
</p>
</td></tr></tbody></table>
<p>Additional surveys of the attention mechanism in deep learning are provided by Niu et al.<sup id="cite&#95;ref-:0&#95;33-0" class="reference"><a href="#cite_note-:0-33"><span class="cite-bracket">&#91;</span>33<span class="cite-bracket">&#93;</span></a></sup> and Soydaner.<sup id="cite&#95;ref-:1&#95;34-0" class="reference"><a href="#cite_note-:1-34"><span class="cite-bracket">&#91;</span>34<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The major breakthrough came with self-attention, where each element in the input sequence attends to all others, enabling the model to capture global dependencies. This idea was central to the <a href="/wiki/Transformer_architecture" class="mw-redirect" title="Transformer architecture">Transformer architecture</a>, which replaced recurrence with attention mechanisms. As a result, Transformers became the foundation for models like <a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a>, <a href="/wiki/T5_(language_model)" title="T5 (language model)">T5</a> and <a href="/wiki/Generative_pre-trained_transformer" title="Generative pre-trained transformer">generative pre-trained transformers</a> (GPT).<sup id="cite&#95;ref-allyouneed&#95;19-1" class="reference"><a href="#cite_note-allyouneed-19"><span class="cite-bracket">&#91;</span>19<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Overview">Overview</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=2" title="Edit section: Overview"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1305433154">.mw-parser-output .ambox{border:1px solid #a2a9b1;border-left:10px solid #36c;background-color:#fbfbfb;box-sizing:border-box}.mw-parser-output .ambox+link+.ambox,.mw-parser-output .ambox+link+style+.ambox,.mw-parser-output .ambox+link+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+style+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+link+.ambox{margin-top:-1px}html body.mediawiki .mw-parser-output .ambox.mbox-small-left{margin:4px 1em 4px 0;overflow:hidden;width:238px;border-collapse:collapse;font-size:88%;line-height:1.25em}.mw-parser-output .ambox-speedy{border-left:10px solid #b32424;background-color:#fee7e6}.mw-parser-output .ambox-delete{border-left:10px solid #b32424}.mw-parser-output .ambox-content{border-left:10px solid #f28500}.mw-parser-output .ambox-style{border-left:10px solid #fc3}.mw-parser-output .ambox-move{border-left:10px solid #9932cc}.mw-parser-output .ambox-protection{border-left:10px solid #a2a9b1}.mw-parser-output .ambox .mbox-text{border:none;padding:0.25em 0.5em;width:100%}.mw-parser-output .ambox .mbox-image{border:none;padding:2px 0 2px 0.5em;text-align:center}.mw-parser-output .ambox .mbox-imageright{border:none;padding:2px 0.5em 2px 0;text-align:center}.mw-parser-output .ambox .mbox-empty-cell{border:none;padding:0;width:1px}.mw-parser-output .ambox .mbox-image-div{width:52px}@media(min-width:720px){.mw-parser-output .ambox{margin:0 10%}}@media print{body.ns-0 .mw-parser-output .ambox{display:none!important}}</style><table class="box-Original&#95;research plainlinks metadata ambox ambox-content ambox-Original&#95;research" role="presentation"><tbody><tr><td class="mbox-image"><div class="mbox-image-div"><span typeof="mw:File"><span><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/40px-Ambox_important.svg.png" decoding="async" width="40" height="40" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/120px-Ambox_important.svg.png 2x" data-file-width="40" data-file-height="40" /></span></span></div></td><td class="mbox-text"><div class="mbox-text-span">This article <b>may contain <a href="/wiki/Wikipedia:No_original_research" title="Wikipedia:No original research">original research</a></b>.<span class="hide-when-compact"> Please <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Attention_(machine_learning)&amp;action=edit">improve it</a> by <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">verifying</a> the claims made and adding <a href="/wiki/Wikipedia:Citing_sources#Inline_citations" title="Wikipedia:Citing sources">inline citations</a>. Statements consisting only of original research should be removed.</span>  <span class="date-container"><i>(<span class="date">June 2025</span>)</i></span><span class="hide-when-compact"><i> (<small><a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this message</a></small>)</i></span></div></td></tr></tbody></table>
<p>The modern era of machine attention was revitalized by grafting an attention mechanism (Fig 1.  orange) to an Encoder-Decoder.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (June 2025)">citation needed</span></a></i>&#93;</sup>
</p>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<td><figure class="mw-default-size" typeof="mw:File/Thumb"><span><video id="mwe_player_0" poster="//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Attention-animated.webm/250px--Attention-animated.webm.jpg" controls="" preload="none" data-mw-tmh="" class="mw-file-element" width="250" height="141" data-durationhint="75" data-mwtitle="Attention-animated.webm" data-mwprovider="wikimediacommons" resource="/wiki/File:Attention-animated.webm"><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/b/b4/Attention-animated.webm/Attention-animated.webm.480p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="480p.vp9.webm" data-width="854" data-height="480" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/b/b4/Attention-animated.webm/Attention-animated.webm.1080p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="1080p.vp9.webm" data-width="1920" data-height="1080" /><source src="//upload.wikimedia.org/wikipedia/commons/b/b4/Attention-animated.webm" type="video/webm; codecs=&quot;vp9&quot;" data-width="1920" data-height="1080" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/b/b4/Attention-animated.webm/Attention-animated.webm.144p.mjpeg.mov" type="video/quicktime" data-transcodekey="144p.mjpeg.mov" data-width="256" data-height="144" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/b/b4/Attention-animated.webm/Attention-animated.webm.240p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="240p.vp9.webm" data-width="426" data-height="240" /><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/b/b4/Attention-animated.webm/Attention-animated.webm.360p.webm" type="video/webm; codecs=&quot;vp8, vorbis&quot;" data-transcodekey="360p.webm" data-width="640" data-height="360" /></video></span><figcaption>Animated sequence of language translation</figcaption></figure>
</td>
<td><style data-mw-deduplicate="TemplateStyles:r1062633282">@media all and (max-width:720px){.mw-parser-output .content .thumb>div:not(.thumbinner){display:flex;justify-content:center;flex-wrap:wrap;align-content:flex-start;flex-direction:column}}body.skin-vector .mw-parser-output div.thumb>div:not(.thumbinner){font-size:94%;text-align:center;overflow:hidden;min-width:100px}body.skin-minerva .mw-parser-output div.thumb>div:not(.thumbinner){margin:0 auto;max-width:100%!important}</style>
<div class="thumb tnone">
<div style="display:table;">
<span typeof="mw:File/Frameless"><a href="/wiki/File:Attention-1-sn.png" class="mw-file-description" title="Fig 1. Encoder-decoder with attention.[35] Numerical subscripts (100, 300, 500, 9k, 10k) indicate vector sizes while lettered subscripts i and i − 1 indicate time steps. Pinkish regions in H matrix and w vector are zero values. See Legend for details."><img alt="Fig 1. Encoder-decoder with attention.[35] Numerical subscripts (100, 300, 500, 9k, 10k) indicate vector sizes while lettered subscripts i and i − 1 indicate time steps. Pinkish regions in H matrix and w vector are zero values. See Legend for details." src="//upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Attention-1-sn.png/500px-Attention-1-sn.png" decoding="async" width="500" height="357" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Attention-1-sn.png/960px-Attention-1-sn.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Attention-1-sn.png/1280px-Attention-1-sn.png 2x" data-file-width="1835" data-file-height="1311" /></a></span><div class="thumbcaption" style="display:table-caption;caption-side:bottom;box-sizing:border-box;">Fig 1.  Encoder-decoder with attention.<sup id="cite&#95;ref-bdritz2017&#95;35-0" class="reference"><a href="#cite_note-bdritz2017-35"><span class="cite-bracket">&#91;</span>35<span class="cite-bracket">&#93;</span></a></sup> Numerical subscripts (100, 300, 500, 9k, 10k) indicate vector sizes while lettered subscripts i and i − 1 indicate time steps.  Pinkish regions in H matrix and w vector are zero values.  See Legend for details. </div>
</div>
</div>
<table class="wikitable mw-collapsible mw-collapsed">
<caption>Legend
</caption>
<tbody><tr>
<th>Label
</th>
<th>Description
</th></tr>
<tr>
<td>100
</td>
<td>Max. sentence length
</td></tr>
<tr>
<td>300
</td>
<td><a href="/wiki/Word_embedding" title="Word embedding">Embedding</a> size (word dimension)
</td></tr>
<tr>
<td>500
</td>
<td>Length of hidden vector
</td></tr>
<tr>
<td>9k, 10k
</td>
<td>Dictionary size of input &amp; output languages respectively.
</td></tr>
<tr>
<td><u>x</u>, <u>Y</u>
</td>
<td>9k and 10k <a href="/wiki/1-hot" class="mw-redirect" title="1-hot">1-hot</a> dictionary vectors.  <u>x</u> → x implemented as a <a href="/wiki/Lookup_table" title="Lookup table">lookup table</a> rather than vector multiplication.  <u>Y</u> is the 1-hot maximizer of the linear Decoder layer D; that is, it takes the argmax of D's linear layer output.
</td></tr>
<tr>
<td>x
</td>
<td>300-long word embedding vector.  The vectors are usually pre-calculated from other projects such as <a href="/wiki/GloVe" title="GloVe">GloVe</a> or <a href="/wiki/Word2Vec" class="mw-redirect" title="Word2Vec">Word2Vec</a>.
</td></tr>
<tr>
<td>h
</td>
<td>500-long encoder hidden vector.  At each point in time, this vector summarizes all the preceding words before it.  The final h can be viewed as a "sentence" vector, or a <a href="/wiki/Thought_vector" title="Thought vector">thought vector</a> as Hinton calls it.
</td></tr>
<tr>
<td>s
</td>
<td>500-long decoder hidden state vector.
</td></tr>
<tr>
<td>E
</td>
<td>500 neuron <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> encoder.  500 outputs.  Input count is 800–300 from source embedding + 500 from recurrent connections.  The encoder feeds directly into the decoder only to initialize it, but not thereafter; hence, that direct connection is shown very faintly.
</td></tr>
<tr>
<td>D
</td>
<td>2-layer decoder.  The recurrent layer has 500 neurons and the fully-connected linear layer has 10k neurons (the size of the target vocabulary).<sup id="cite&#95;ref-pytorch&#95;s2s&#95;36-0" class="reference"><a href="#cite_note-pytorch_s2s-36"><span class="cite-bracket">&#91;</span>36<span class="cite-bracket">&#93;</span></a></sup>   The linear layer alone has 5 million (500 × 10k) weights – ~10 times more weights than the recurrent layer.
</td></tr>
<tr>
<td>score
</td>
<td>100-long alignment score
</td></tr>
<tr>
<td>w
</td>
<td>100-long vector attention weight.  These are "soft" weights which changes during the forward pass, in contrast to "hard" neuronal weights that change during the learning phase.
</td></tr>
<tr>
<td>A
</td>
<td>Attention module – this can be a dot product of recurrent states, or the query-key-value fully-connected layers.  The output is a 100-long vector w.
</td></tr>
<tr>
<td>H
</td>
<td>500×100. 100 hidden vectors h concatenated into a matrix
</td></tr>
<tr>
<td>c
</td>
<td>500-long context vector = H * w.  c is a linear combination of h vectors weighted by w.
</td></tr></tbody></table>
</td></tr></tbody></table>
<p>Figure 2 shows the internal step-by-step operation of the attention block (A) in Fig 1.
</p>
<div class="center"><div class="thumb tnone" style=""><div class="thumbinner" style="width: 702px;"><div class="thumbimage noresize" style="width: 700px;">
<span typeof="mw:File"><a href="/wiki/File:Attention-qkv.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/8/81/Attention-qkv.png/960px-Attention-qkv.png" decoding="async" width="700" height="304" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/81/Attention-qkv.png/1280px-Attention-qkv.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/81/Attention-qkv.png/1920px-Attention-qkv.png 2x" data-file-width="2634" data-file-height="1145" /></a></span> </div><div class="thumbcaption">Figure 2.  The diagram shows the attention forward pass calculating correlations of the word "that" with other words in "See that girl run."  Given the right weights from training, the network should be able to identify "girl" as a highly correlated word.  Some things to note:
<ul><li>This example focuses on the attention of a single word "that".  In practice, the attention of each word is calculated in parallel to speed up calculations.  Simply changing the lowercase "x" vector to the uppercase "X" matrix will yield the formula for this.</li>
<li>Softmax scaling <span class="texhtml">qW<sub>k</sub><sup>T</sup></span> / <span class="nowrap">&#8730;<span style="border-top:1px solid; padding:0 0.1em;">100</span></span> prevents a high variance in <span class="texhtml">qW<sub>k</sub><sup>T</sup></span> that would allow a single word to excessively dominate the softmax resulting in attention to only one word, as a discrete hard max would do.</li>
<li><u>Notation</u>: the commonly written row-wise <span class="texhtml">softmax</span> formula above assumes that vectors are rows, which runs contrary to the standard math notation of column vectors. More correctly, we should take the transpose of the context vector and use the column-wise <span class="texhtml">softmax</span>, resulting in the more correct form</li></ul>
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{aligned}(XW_{v})^{T}*{[(W_{k}X^{T})*{({\underline {x}}W_{q})^{T}}]_{sm}}\end{aligned}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
            <mtr>
              <mtd>
                <mo stretchy="false">(</mo>
                <mi>X</mi>
                <msub>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>v</mi>
                  </mrow>
                </msub>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>T</mi>
                  </mrow>
                </msup>
                <mo>&#x2217;<!-- ∗ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">[</mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>W</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>k</mi>
                    </mrow>
                  </msub>
                  <msup>
                    <mi>X</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>T</mi>
                    </mrow>
                  </msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2217;<!-- ∗ --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mo stretchy="false">(</mo>
                    <mrow class="MJX-TeXAtom-ORD">
                      <munder>
                        <mi>x</mi>
                        <mo>&#x005F;<!-- _ --></mo>
                      </munder>
                    </mrow>
                    <msub>
                      <mi>W</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>q</mi>
                      </mrow>
                    </msub>
                    <msup>
                      <mo stretchy="false">)</mo>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>T</mi>
                      </mrow>
                    </msup>
                  </mrow>
                  <msub>
                    <mo stretchy="false">]</mo>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>s</mi>
                      <mi>m</mi>
                    </mrow>
                  </msub>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}(XW_{v})^{T}*{[(W_{k}X^{T})*{({\underline {x}}W_{q})^{T}}]_{sm}}\end{aligned}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1adff6d5e9decb390b437b6bd3d0b35ad2791406" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.79ex; margin-bottom: -0.715ex; width:33.473ex; height:3.676ex;" alt="{\displaystyle {\begin{aligned}(XW_{v})^{T}*{[(W_{k}X^{T})*{({\underline {x}}W_{q})^{T}}]_{sm}}\end{aligned}}}"></span>.</div></div></div></div>
<div class="mw-heading mw-heading3"><h3 id="Interpreting_attention_weights">Interpreting attention weights</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=3" title="Edit section: Interpreting attention weights"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>In translating between languages, alignment is the process of matching words from the source sentence to words of the translated sentence. Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix. The off-diagonal dominance shows that the attention mechanism is more nuanced.
</p><p>Consider an example of translating <i>I love you</i> to French. On the first pass through the decoder, 94% of the attention weight is on the first English word <i>I</i>, so the network offers the word <i>je</i>. On the second pass of the decoder, 88% of the attention weight is on the third English word <i>you</i>, so it offers <i>t'</i>. On the last pass, 95% of the attention weight is on the second English word <i>love</i>, so it offers <i>aime</i>.
</p><p>In the <i>I love you</i> example, the second word <i>love</i> is aligned with the third word <i>aime</i>. Stacking soft row vectors together for <i>je</i>, <i>t'</i>, and <i>aime</i> yields an <a href="/wiki/Statistical_machine_translation#Word_alignment" title="Statistical machine translation">alignment matrix</a>:
</p>
<table class="wikitable">
<tbody><tr>
<th>
</th>
<th>I
</th>
<th>love
</th>
<th>you
</th></tr>
<tr>
<th>je
</th>
<td style="background-color:black; color:white;">0.94
</td>
<td>0.02
</td>
<td>0.04
</td></tr>
<tr>
<th>t'
</th>
<td>0.11
</td>
<td>0.01
</td>
<td style="background-color:grey ; color:white;">0.88
</td></tr>
<tr>
<th>aime
</th>
<td>0.03
</td>
<td style="background-color:black; color:white;">0.95
</td>
<td>0.02
</td></tr></tbody></table>
<p>Sometimes, alignment can be multiple-to-multiple. For example, the English phrase <i>look it up</i> corresponds to <i>cherchez-le</i>. Thus, "soft" attention weights work better than "hard" attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than "the best one", as there may not be a best hidden vector.
</p>
<div class="mw-heading mw-heading2"><h2 id="Variants">Variants</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=4" title="Edit section: Variants"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Self-attention_in_CNN,_RNN,_and_self-attention.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d7/Self-attention_in_CNN%2C_RNN%2C_and_self-attention.svg/250px-Self-attention_in_CNN%2C_RNN%2C_and_self-attention.svg.png" decoding="async" width="250" height="167" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d7/Self-attention_in_CNN%2C_RNN%2C_and_self-attention.svg/500px-Self-attention_in_CNN%2C_RNN%2C_and_self-attention.svg.png 1.5x" data-file-width="503" data-file-height="335" /></a><figcaption>Comparison of the data flow in CNN, RNN, and self-attention</figcaption></figure>
<p>Many variants of attention implement soft weights, such as
</p>
<ul><li>fast weight programmers, or fast weight controllers (1992).<sup id="cite&#95;ref-transform1992&#95;5-1" class="reference"><a href="#cite_note-transform1992-5"><span class="cite-bracket">&#91;</span>5<span class="cite-bracket">&#93;</span></a></sup> A "slow" <a href="/wiki/Neural_network" title="Neural network">neural network</a> outputs the "fast" weights of another neural network through <a href="/wiki/Outer_product" title="Outer product">outer products</a>. The slow network learns by gradient descent. It was later renamed as "linearized self-attention".<sup id="cite&#95;ref-schlag2021&#95;37-0" class="reference"><a href="#cite_note-schlag2021-37"><span class="cite-bracket">&#91;</span>37<span class="cite-bracket">&#93;</span></a></sup></li>
<li>Bahdanau-style attention,<sup id="cite&#95;ref-bahdanau&#95;11-1" class="reference"><a href="#cite_note-bahdanau-11"><span class="cite-bracket">&#91;</span>11<span class="cite-bracket">&#93;</span></a></sup> also referred to as <i>additive attention</i>,</li>
<li>Luong-style attention,<sup id="cite&#95;ref-xy-dot&#95;38-0" class="reference"><a href="#cite_note-xy-dot-38"><span class="cite-bracket">&#91;</span>38<span class="cite-bracket">&#93;</span></a></sup> which is known as <i>multiplicative attention</i>,</li>
<li>Early attention mechanisms similar to modern self-attention were proposed using recurrent neural networks. However, the highly parallelizable self-attention was introduced in 2017 and successfully used in the Transformer model,</li>
<li><i>positional attention</i> and <i>factorized positional attention</i>.<sup id="cite&#95;ref-luo&#95;39-0" class="reference"><a href="#cite_note-luo-39"><span class="cite-bracket">&#91;</span>39<span class="cite-bracket">&#93;</span></a></sup></li></ul>
<p>For <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural networks</a>, attention mechanisms can be distinguished by the dimension on which they operate, namely: spatial attention,<sup id="cite&#95;ref-xzhu1&#95;40-0" class="reference"><a href="#cite_note-xzhu1-40"><span class="cite-bracket">&#91;</span>40<span class="cite-bracket">&#93;</span></a></sup> channel attention,<sup id="cite&#95;ref-jhu1&#95;41-0" class="reference"><a href="#cite_note-jhu1-41"><span class="cite-bracket">&#91;</span>41<span class="cite-bracket">&#93;</span></a></sup> or combinations.<sup id="cite&#95;ref-psanghyun1&#95;42-0" class="reference"><a href="#cite_note-psanghyun1-42"><span class="cite-bracket">&#91;</span>42<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-mgeorgescu&#95;43-0" class="reference"><a href="#cite_note-mgeorgescu-43"><span class="cite-bracket">&#91;</span>43<span class="cite-bracket">&#93;</span></a></sup>
</p><p>These variants recombine the encoder-side inputs to redistribute those effects to each target output. Often, a correlation-style matrix of dot products provides the re-weighting coefficients.  In the figures below, W is the matrix of context attention weights, similar to the formula in Overview section above.
</p>
<table class="wikitable">

<tbody><tr>
<th>1. encoder-decoder dot product
</th>
<th>2. encoder-decoder QKV
</th>
<th>3. encoder-only dot product
</th>
<th>4. encoder-only QKV
</th>
<th>5. Pytorch tutorial
</th></tr>
<tr>
<td><figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Attn-xy-dot.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Attn-xy-dot.png/250px-Attn-xy-dot.png" decoding="async" width="250" height="277" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/a/a5/Attn-xy-dot.png 1.5x" data-file-width="301" data-file-height="334" /></a><figcaption>Both encoder &amp; decoder are needed to calculate attention.<sup id="cite&#95;ref-xy-dot&#95;38-1" class="reference"><a href="#cite_note-xy-dot-38"><span class="cite-bracket">&#91;</span>38<span class="cite-bracket">&#93;</span></a></sup></figcaption></figure>
</td>
<td><figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Attn-xy-qkv.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Attn-xy-qkv.png/250px-Attn-xy-qkv.png" decoding="async" width="250" height="263" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Attn-xy-qkv.png/500px-Attn-xy-qkv.png 1.5x" data-file-width="704" data-file-height="741" /></a><figcaption>Both encoder &amp; decoder are needed to calculate attention.<sup id="cite&#95;ref-xy-qkv&#95;44-0" class="reference"><a href="#cite_note-xy-qkv-44"><span class="cite-bracket">&#91;</span>44<span class="cite-bracket">&#93;</span></a></sup></figcaption></figure>
</td>
<td><figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Attn-xx-dot.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/6/61/Attn-xx-dot.png/250px-Attn-xx-dot.png" decoding="async" width="250" height="240" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/6/61/Attn-xx-dot.png 1.5x" data-file-width="302" data-file-height="290" /></a><figcaption>Decoder is <i>not</i> used to calculate attention. With only 1 input into corr, W is an auto-correlation of dot products. w<sub><b>ij</b></sub> = x<sub><b>i</b></sub> x<sub><b>j</b></sub>.<sup id="cite&#95;ref-xx-dot&#95;45-0" class="reference"><a href="#cite_note-xx-dot-45"><span class="cite-bracket">&#91;</span>45<span class="cite-bracket">&#93;</span></a></sup></figcaption></figure>
</td>
<td><figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Attn-xx-qkv.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Attn-xx-qkv.png/250px-Attn-xx-qkv.png" decoding="async" width="250" height="236" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Attn-xx-qkv.png/500px-Attn-xx-qkv.png 1.5x" data-file-width="685" data-file-height="647" /></a><figcaption>Decoder is <i>not</i> used to calculate attention.<sup id="cite&#95;ref-xx-qkv&#95;46-0" class="reference"><a href="#cite_note-xx-qkv-46"><span class="cite-bracket">&#91;</span>46<span class="cite-bracket">&#93;</span></a></sup></figcaption></figure>
</td>
<td><figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Attn-pytorch-tutorial.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/7/72/Attn-pytorch-tutorial.png/250px-Attn-pytorch-tutorial.png" decoding="async" width="250" height="267" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/7/72/Attn-pytorch-tutorial.png 1.5x" data-file-width="342" data-file-height="365" /></a><figcaption>A fully-connected layer is used to calculate attention instead of dot product correlation.<sup id="cite&#95;ref-pytorch-tutorial&#95;47-0" class="reference"><a href="#cite_note-pytorch-tutorial-47"><span class="cite-bracket">&#91;</span>47<span class="cite-bracket">&#93;</span></a></sup></figcaption></figure>
</td></tr></tbody></table>
<table class="wikitable mw-collapsible mw-collapsed">
<caption>Legend
</caption>
<tbody><tr>
<th>Label</th>
<th>Description
</th></tr>
<tr>
<td>Variables X, H, S, T</td>
<td>Upper case variables represent the entire sentence, and not just the current word. For example, H is a matrix of the encoder hidden state—one word per column.
</td></tr>
<tr>
<td>S, T</td>
<td>S, decoder hidden state; T, target word embedding. In the <a href="/wiki/Pytorch" class="mw-redirect" title="Pytorch">Pytorch</a> Tutorial variant training phase, T alternates between 2 sources depending on the level of <a href="/wiki/Teacher_forcing" title="Teacher forcing">teacher forcing</a> used. T could be the embedding of the network's output word; i.e. embedding(argmax(FC output)). Alternatively with teacher forcing, T could be the embedding of the known correct word which can occur with a constant forcing probability, say 1/2.
</td></tr>
<tr>
<td>X, H</td>
<td>H, encoder hidden state; X, input word embeddings.
</td></tr>
<tr>
<td>W</td>
<td>Attention coefficients
</td></tr>
<tr>
<td>Qw, Kw, Vw, FC</td>
<td>Weight matrices for query, key, value respectively. FC is a fully-connected weight matrix.
</td></tr>
<tr>
<td>⊕, ⊗</td>
<td>⊕, vector concatenation; ⊗, matrix multiplication.
</td></tr>
<tr>
<td>corr</td>
<td>Column-wise softmax(matrix of all combinations of dot products). The dot products are <b>x<sub>i</sub> * x<sub>j</sub></b> in variant #3, <b>h<sub>i</sub>* s</b><sub>j</sub> in variant 1, and column&#160;<sub><b>i</b></sub> ( Kw * H ) * column&#160;<sub><b>j</b></sub> ( Qw * S ) in variant 2, and column&#160;<sub><b>i</b></sub> ( Kw * X ) * column&#160;<sub><b>j</b></sub> ( Qw * X ) in variant 4. Variant 5 uses a fully-connected layer to determine the coefficients. If the variant is QKV, then the dot products are normalized by the <span class="texhtml"><span class="nowrap">&#8730;<span style="border-top:1px solid; padding:0 0.1em;">d</span></span></span> where <span class="texhtml">d</span> is the height of the QKV matrices.
</td></tr></tbody></table>
<div class="mw-heading mw-heading2"><h2 id="Optimizations">Optimizations</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=5" title="Edit section: Optimizations"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<div class="mw-heading mw-heading3"><h3 id="Flash_attention">Flash attention</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=6" title="Edit section: Flash attention"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The size of the attention matrix is proportional to the square of the number of input tokens. Therefore, when the input is long, calculating the attention matrix requires a lot of <a href="/wiki/GPU" class="mw-redirect" title="GPU">GPU</a> memory. Flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy. It achieves this by partitioning the attention computation into smaller blocks that fit into the GPU's faster on-chip memory, reducing the need to store large intermediate matrices and thus lowering memory usage while increasing computational efficiency.<sup id="cite&#95;ref-48" class="reference"><a href="#cite_note-48"><span class="cite-bracket">&#91;</span>48<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="FlexAttention">FlexAttention</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=7" title="Edit section: FlexAttention"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>FlexAttention<sup id="cite&#95;ref-49" class="reference"><a href="#cite_note-49"><span class="cite-bracket">&#91;</span>49<span class="cite-bracket">&#93;</span></a></sup> is an attention kernel developed by <a href="/wiki/Meta_Platforms" title="Meta Platforms">Meta</a> that allows users to modify attention scores prior to <a href="/wiki/Softmax" class="mw-redirect" title="Softmax">softmax</a> and dynamically chooses the optimal attention algorithm.
</p>
<div class="mw-heading mw-heading2"><h2 id="Applications">Applications</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=8" title="Edit section: Applications"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Attention is widely used in natural language processing, computer vision, and speech recognition. In NLP, it improves context understanding in tasks like question answering and summarization. In vision, visual attention helps models focus on relevant image regions, enhancing object detection and image captioning.
</p>
<div class="mw-heading mw-heading3"><h3 id="Attention_maps_as_explanations_for_vision_transformers">Attention maps as explanations for vision transformers</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=9" title="Edit section: Attention maps as explanations for vision transformers"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1320445320" /><div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Saliency_map" title="Saliency map">Saliency map</a> and <a href="/wiki/Mechanistic_interpretability" title="Mechanistic interpretability">Mechanistic interpretability</a></div>
<p>From the original paper on <a href="/wiki/Vision_transformer" title="Vision transformer">vision transformers</a> (ViT), visualizing attention scores as a heat map (called <a href="/wiki/Saliency_map" title="Saliency map">saliency maps</a> or attention maps) has become an important and routine way to inspect the decision making process of ViT models.<sup id="cite&#95;ref-50" class="reference"><a href="#cite_note-50"><span class="cite-bracket">&#91;</span>50<span class="cite-bracket">&#93;</span></a></sup> One can compute the attention maps with respect to any attention head at any layer, while the deeper layers tend to show more semantically meaningful visualization. Attention rollout is a recursive algorithm to combine attention scores across all layers, by computing the dot product of successive attention maps.<sup id="cite&#95;ref-51" class="reference"><a href="#cite_note-51"><span class="cite-bracket">&#91;</span>51<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Because vision transformers are typically trained in a <a href="/wiki/Self-supervised_learning" title="Self-supervised learning">self-supervised</a> manner, attention maps are generally not class-sensitive. When a classification head is attached to the ViT backbone, class-discriminative attention maps (CDAM) combines attention maps and gradients with respect to the class <code>[CLS]</code> token.<sup id="cite&#95;ref-52" class="reference"><a href="#cite_note-52"><span class="cite-bracket">&#91;</span>52<span class="cite-bracket">&#93;</span></a></sup> Some class-sensitive <a href="/wiki/Interpretability_(machine_learning)" class="mw-redirect" title="Interpretability (machine learning)">interpretability</a> methods originally developed for <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural networks</a> can be also applied to ViT, such as GradCAM, which <a href="/wiki/Backpropagation" title="Backpropagation">back-propagates</a> the gradients to the outputs of the final attention layer.<sup id="cite&#95;ref-53" class="reference"><a href="#cite_note-53"><span class="cite-bracket">&#91;</span>53<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Using attention as basis of explanation for the transformers in language and vision is not without debate. While some pioneering papers analyzed and framed attention scores as explanations,<sup id="cite&#95;ref-54" class="reference"><a href="#cite_note-54"><span class="cite-bracket">&#91;</span>54<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-55" class="reference"><a href="#cite_note-55"><span class="cite-bracket">&#91;</span>55<span class="cite-bracket">&#93;</span></a></sup> higher attention scores do not always correlate with greater impact on model performances.<sup id="cite&#95;ref-56" class="reference"><a href="#cite_note-56"><span class="cite-bracket">&#91;</span>56<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Mathematical_representation">Mathematical representation</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=10" title="Edit section: Mathematical representation"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<div class="mw-heading mw-heading3"><h3 id="Standard_scaled_dot-product_attention">Standard scaled dot-product attention</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=11" title="Edit section: Standard scaled dot-product attention"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>For matrices: <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Q\in \mathbb {R} ^{m\times d_{k}},K\in \mathbb {R} ^{n\times d_{k}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>Q</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
            <mo>&#x00D7;<!-- × --></mo>
            <msub>
              <mi>d</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>k</mi>
              </mrow>
            </msub>
          </mrow>
        </msup>
        <mo>,</mo>
        <mi>K</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>&#x00D7;<!-- × --></mo>
            <msub>
              <mi>d</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>k</mi>
              </mrow>
            </msub>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Q\in \mathbb {R} ^{m\times d_{k}},K\in \mathbb {R} ^{n\times d_{k}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32465b005ab325fcae5d7dc472b578d060b8ff99" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:22.855ex; height:3.009ex;" alt="{\displaystyle Q\in \mathbb {R} ^{m\times d_{k}},K\in \mathbb {R} ^{n\times d_{k}}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V\in \mathbb {R} ^{n\times d_{v}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>V</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>&#x00D7;<!-- × --></mo>
            <msub>
              <mi>d</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>v</mi>
              </mrow>
            </msub>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V\in \mathbb {R} ^{n\times d_{v}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/202925a11ae6c1678f0975f8902dfc398a8626e3" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:10.469ex; height:2.676ex;" alt="{\displaystyle V\in \mathbb {R} ^{n\times d_{v}}}"></span>, the scaled dot-product, or QKV attention, is defined as:
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}\right)V\in \mathbb {R} ^{m\times d_{v}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>Attention</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <mo>,</mo>
        <mi>K</mi>
        <mo>,</mo>
        <mi>V</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>softmax</mtext>
        </mrow>
        <mrow>
          <mo>(</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mfrac>
              <mrow>
                <mi>Q</mi>
                <msup>
                  <mi>K</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>T</mi>
                  </mrow>
                </msup>
              </mrow>
              <msqrt>
                <msub>
                  <mi>d</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>k</mi>
                  </mrow>
                </msub>
              </msqrt>
            </mfrac>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mi>V</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
            <mo>&#x00D7;<!-- × --></mo>
            <msub>
              <mi>d</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>v</mi>
              </mrow>
            </msub>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}\right)V\in \mathbb {R} ^{m\times d_{v}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/db3106d8746fa638cda2e3440cd2438c7e682bde" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:52.111ex; height:7.509ex;" alt="{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}\right)V\in \mathbb {R} ^{m\times d_{v}}}"></span>
where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {}^{T}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">

          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {}^{T}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8f5df5b5ecb120f5e7f4837d839c7c3181a08346" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.171ex; width:1.389ex; height:2.509ex;" alt="{\displaystyle {}^{T}}"></span> denotes <a href="/wiki/Matrix_transpose" class="mw-redirect" title="Matrix transpose">transpose</a> and the <a href="/wiki/Softmax_function" title="Softmax function">softmax function</a> is applied independently to every row of its argument. The matrix <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Q}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>Q</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Q}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8752c7023b4b3286800fe3238271bbca681219ed" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.838ex; height:2.509ex;" alt="{\displaystyle Q}"></span> contains <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle m}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>m</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle m}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0a07d98bb302f3856cbabc47b2b9016692e3f7bc" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.04ex; height:1.676ex;" alt="{\displaystyle m}"></span> queries, while matrices <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K,V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
        <mo>,</mo>
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K,V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c1059d12adb442403ba8c33fd0379064bb9754bb" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:4.887ex; height:2.509ex;" alt="{\displaystyle K,V}"></span> jointly contain an <i>unordered</i> set of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;" alt="{\displaystyle n}"></span> key-value pairs. Value vectors in matrix <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.787ex; height:2.176ex;" alt="{\displaystyle V}"></span> are weighted using the weights resulting from the softmax operation, so that the rows of the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle m}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>m</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle m}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0a07d98bb302f3856cbabc47b2b9016692e3f7bc" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.04ex; height:1.676ex;" alt="{\displaystyle m}"></span>-by-<span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle d_{v}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>d</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>v</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle d_{v}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4034edfd613443b0938cb4da1899ab1ce102da2a" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.239ex; height:2.509ex;" alt="{\displaystyle d_{v}}"></span> output matrix are confined to the <a href="/wiki/Convex_hull" title="Convex hull">convex hull</a> of the points in <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbb {R} ^{d_{v}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>d</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>v</mi>
              </mrow>
            </msub>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbb {R} ^{d_{v}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f17b1fab835e9a966e7a1f9c75875f98d3de3970" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:3.577ex; height:2.676ex;" alt="{\displaystyle \mathbb {R} ^{d_{v}}}"></span> given by the rows of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.787ex; height:2.176ex;" alt="{\displaystyle V}"></span>.
</p><p>To understand the permutation invariance and permutation equivariance properties of QKV attention,<sup id="cite&#95;ref-SetTransformer&#95;57-0" class="reference"><a href="#cite_note-SetTransformer-57"><span class="cite-bracket">&#91;</span>57<span class="cite-bracket">&#93;</span></a></sup> let <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle A\in \mathbb {R} ^{m\times m}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>A</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
            <mo>&#x00D7;<!-- × --></mo>
            <mi>m</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle A\in \mathbb {R} ^{m\times m}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/900e5c7a9590bf32dbf6130f6f7579df9027d80c" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:10.658ex; height:2.343ex;" alt="{\displaystyle A\in \mathbb {R} ^{m\times m}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle B\in \mathbb {R} ^{n\times n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>B</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>&#x00D7;<!-- × --></mo>
            <mi>n</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle B\in \mathbb {R} ^{n\times n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3a6da48b3b39c68ecfdbddb3367c0bb9c6c5dab8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:9.766ex; height:2.343ex;" alt="{\displaystyle B\in \mathbb {R} ^{n\times n}}"></span> be <a href="/wiki/Permutation_matrix" title="Permutation matrix">permutation matrices</a>; and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle D\in \mathbb {R} ^{m\times n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>D</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
            <mo>&#x00D7;<!-- × --></mo>
            <mi>n</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D\in \mathbb {R} ^{m\times n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4078865760ea47300ba9eb9958cbc326e836c6da" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:10.383ex; height:2.343ex;" alt="{\displaystyle D\in \mathbb {R} ^{m\times n}}"></span> an arbitrary matrix. The softmax function is permutation equivariant in the sense that:
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{softmax}}(ADB)=A\,{\text{softmax}}(D)B}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>softmax</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>A</mi>
        <mi>D</mi>
        <mi>B</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>A</mi>
        <mspace width="thinmathspace" />
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>softmax</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>D</mi>
        <mo stretchy="false">)</mo>
        <mi>B</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{softmax}}(ADB)=A\,{\text{softmax}}(D)B}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/03aacfdd979d855aff47c20abf94d6d4f5271a20" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:34.009ex; height:2.843ex;" alt="{\displaystyle {\text{softmax}}(ADB)=A\,{\text{softmax}}(D)B}"></span>
By noting that the transpose of a permutation matrix is also its inverse, it follows that:
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{Attention}}(AQ,BK,BV)=A\,{\text{Attention}}(Q,K,V)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>Attention</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>A</mi>
        <mi>Q</mi>
        <mo>,</mo>
        <mi>B</mi>
        <mi>K</mi>
        <mo>,</mo>
        <mi>B</mi>
        <mi>V</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>A</mi>
        <mspace width="thinmathspace" />
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>Attention</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <mo>,</mo>
        <mi>K</mi>
        <mo>,</mo>
        <mi>V</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{Attention}}(AQ,BK,BV)=A\,{\text{Attention}}(Q,K,V)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8cdee6b028b70be2e05e00a58783c9f9c646e24e" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:49.405ex; height:2.843ex;" alt="{\displaystyle {\text{Attention}}(AQ,BK,BV)=A\,{\text{Attention}}(Q,K,V)}"></span>
which shows that QKV attention is <a href="/wiki/Equivariance" class="mw-redirect" title="Equivariance">equivariant</a> with respect to re-ordering the queries (rows of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Q}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>Q</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Q}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8752c7023b4b3286800fe3238271bbca681219ed" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.838ex; height:2.509ex;" alt="{\displaystyle Q}"></span>); and <a href="/wiki/Invariant_(mathematics)" title="Invariant (mathematics)">invariant</a> to re-ordering of the key-value pairs in <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K,V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
        <mo>,</mo>
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K,V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c1059d12adb442403ba8c33fd0379064bb9754bb" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:4.887ex; height:2.509ex;" alt="{\displaystyle K,V}"></span>. These properties are inherited when applying linear transforms to the inputs and outputs of QKV attention blocks. For example, a simple self-attention function defined as:
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle X\mapsto {\text{Attention}}(XT_{q},XT_{k},XT_{v})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>X</mi>
        <mo stretchy="false">&#x21A6;<!-- ↦ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>Attention</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <msub>
          <mi>T</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>q</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mi>X</mi>
        <msub>
          <mi>T</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mi>X</mi>
        <msub>
          <mi>T</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>v</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle X\mapsto {\text{Attention}}(XT_{q},XT_{k},XT_{v})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d70ba131ae5fab0bf4ed87a19c9e32fdb306d1e7" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:32.475ex; height:3.009ex;" alt="{\displaystyle X\mapsto {\text{Attention}}(XT_{q},XT_{k},XT_{v})}"></span>  
is permutation equivariant with respect to re-ordering the rows of the input matrix <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle X}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>X</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle X}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;" alt="{\displaystyle X}"></span> in a non-trivial way, because every row of the output is a function of all the rows of the input. Similar properties hold for <i>multi-head attention</i>, which is defined below.
</p>
<div class="mw-heading mw-heading3"><h3 id="Masked_attention">Masked attention</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=12" title="Edit section: Masked attention"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>When QKV attention is used as a building block for an autoregressive decoder, and when at training time all input and output matrices have <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;" alt="{\displaystyle n}"></span> rows, a masked attention variant is used:
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}+M\right)V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>Attention</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <mo>,</mo>
        <mi>K</mi>
        <mo>,</mo>
        <mi>V</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>softmax</mtext>
        </mrow>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mi>Q</mi>
                  <msup>
                    <mi>K</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>T</mi>
                    </mrow>
                  </msup>
                </mrow>
                <msqrt>
                  <msub>
                    <mi>d</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>k</mi>
                    </mrow>
                  </msub>
                </msqrt>
              </mfrac>
            </mrow>
            <mo>+</mo>
            <mi>M</mi>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}+M\right)V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f2344362e74c257757b96397c5e7d2acb2f27f07" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:48.255ex; height:7.509ex;" alt="{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}+M\right)V}"></span>
where the mask, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle M\in \mathbb {R} ^{n\times n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>M</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>&#x00D7;<!-- × --></mo>
            <mi>n</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle M\in \mathbb {R} ^{n\times n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ba045fb18350a8830da60df84aa1bc0f699a2dd8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:10.444ex; height:2.343ex;" alt="{\displaystyle M\in \mathbb {R} ^{n\times n}}"></span> is a <a href="/wiki/Triangular_matrix" title="Triangular matrix">strictly upper triangular matrix</a>, with zeros on and below the diagonal and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle -\infty }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo>&#x2212;<!-- − --></mo>
        <mi mathvariant="normal">&#x221E;<!-- ∞ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle -\infty }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ca2608c4b5fd3bffc73585f8c67e379b4e99b6f1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.505ex; width:4.132ex; height:2.176ex;" alt="{\displaystyle -\infty }"></span> in every element above the diagonal. The softmax output, also in <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbb {R} ^{n\times n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>&#x00D7;<!-- × --></mo>
            <mi>n</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbb {R} ^{n\times n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7095e87521f2c247d021fa7101072f11beba0a70" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:5.161ex; height:2.343ex;" alt="{\displaystyle \mathbb {R} ^{n\times n}}"></span> is then <i>lower triangular</i>, with zeros in all elements above the diagonal. The masking ensures that for all <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle 1\leq i&lt;j\leq n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mn>1</mn>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mi>i</mi>
        <mo>&lt;</mo>
        <mi>j</mi>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 1\leq i&lt;j\leq n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/05aa5c14ef41693716e76cecdd716851f4f5d304" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:13.613ex; height:2.509ex;" alt="{\displaystyle 1\leq i&lt;j\leq n}"></span>, row <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle i}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;" alt="{\displaystyle i}"></span> of the attention output is independent of row <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span> of any of the three input matrices. The permutation invariance and equivariance properties of standard QKV attention do not hold for the masked variant.
</p>
<div class="mw-heading mw-heading3"><h3 id="Multi-head_attention">Multi-head attention</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=13" title="Edit section: Multi-head attention"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Encoder_cross-attention,_multiheaded_version.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Encoder_cross-attention%2C_multiheaded_version.png/250px-Encoder_cross-attention%2C_multiheaded_version.png" decoding="async" width="250" height="202" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Encoder_cross-attention%2C_multiheaded_version.png/500px-Encoder_cross-attention%2C_multiheaded_version.png 1.5x" data-file-width="1426" data-file-height="1150" /></a><figcaption>Decoder multiheaded cross-attention</figcaption></figure>
<p>Multi-head attention
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{MultiHead}}(Q,K,V)={\text{Concat}}({\text{head}}_{1},...,{\text{head}}_{h})W^{O}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>MultiHead</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <mo>,</mo>
        <mi>K</mi>
        <mo>,</mo>
        <mi>V</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>Concat</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>head</mtext>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>,</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>head</mtext>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>h</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>O</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{MultiHead}}(Q,K,V)={\text{Concat}}({\text{head}}_{1},...,{\text{head}}_{h})W^{O}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/008a9557a33d71fa3d16abe664eb4fc257d5a6db" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:53.518ex; height:3.176ex;" alt="{\displaystyle {\text{MultiHead}}(Q,K,V)={\text{Concat}}({\text{head}}_{1},...,{\text{head}}_{h})W^{O}}"></span>
where each head is computed with QKV attention as:
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{head}}_{i}={\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>head</mtext>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>Attention</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <msubsup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>Q</mi>
          </mrow>
        </msubsup>
        <mo>,</mo>
        <mi>K</mi>
        <msubsup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>K</mi>
          </mrow>
        </msubsup>
        <mo>,</mo>
        <mi>V</mi>
        <msubsup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>V</mi>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{head}}_{i}={\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fba64142fc62f2da5f4ed116d6cc19772a89bba" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:40.375ex; height:3.509ex;" alt="{\displaystyle {\text{head}}_{i}={\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})}"></span>
and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>Q</mi>
          </mrow>
        </msubsup>
        <mo>,</mo>
        <msubsup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>K</mi>
          </mrow>
        </msubsup>
        <mo>,</mo>
        <msubsup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>V</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3087f1739ddc134719d701163d3a75af985498b1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:14.313ex; height:3.509ex;" alt="{\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}"></span>, and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W^{O}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>O</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W^{O}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f0f376fd1863b3a195a6bf34b56ff43ec5d695f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:3.994ex; height:2.676ex;" alt="{\displaystyle W^{O}}"></span> are parameter matrices.
</p><p>The permutation properties of (standard, unmasked) QKV attention apply here also. For permutation matrices, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle A,B}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>A</mi>
        <mo>,</mo>
        <mi>B</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle A,B}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/96c3298ea9aa77c226be56a7d8515baaa517b90b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:4.541ex; height:2.509ex;" alt="{\displaystyle A,B}"></span>:
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{MultiHead}}(AQ,BK,BV)=A\,{\text{MultiHead}}(Q,K,V)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>MultiHead</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>A</mi>
        <mi>Q</mi>
        <mo>,</mo>
        <mi>B</mi>
        <mi>K</mi>
        <mo>,</mo>
        <mi>B</mi>
        <mi>V</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>A</mi>
        <mspace width="thinmathspace" />
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>MultiHead</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <mo>,</mo>
        <mi>K</mi>
        <mo>,</mo>
        <mi>V</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{MultiHead}}(AQ,BK,BV)=A\,{\text{MultiHead}}(Q,K,V)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3b87f0747c0651491cc17e9b81eb7c3256dc782" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:51.342ex; height:2.843ex;" alt="{\displaystyle {\text{MultiHead}}(AQ,BK,BV)=A\,{\text{MultiHead}}(Q,K,V)}"></span>
from which we also see that multi-head self-attention:
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle X\mapsto {\text{MultiHead}}(XT_{q},XT_{k},XT_{v})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>X</mi>
        <mo stretchy="false">&#x21A6;<!-- ↦ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>MultiHead</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <msub>
          <mi>T</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>q</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mi>X</mi>
        <msub>
          <mi>T</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mi>X</mi>
        <msub>
          <mi>T</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>v</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle X\mapsto {\text{MultiHead}}(XT_{q},XT_{k},XT_{v})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0630d9f93e5736f659a56b908fa8234198c3672c" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:33.443ex; height:3.009ex;" alt="{\displaystyle X\mapsto {\text{MultiHead}}(XT_{q},XT_{k},XT_{v})}"></span>  
is equivariant with respect to re-ordering of the rows of input matrix <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle X}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>X</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle X}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;" alt="{\displaystyle X}"></span>.
</p>
<div class="mw-heading mw-heading3"><h3 id="Bahdanau_(additive)_attention"><span id="Bahdanau_.28additive.29_attention"></span>Bahdanau (additive) attention</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=14" title="Edit section: Bahdanau (additive) attention"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p><span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(\tanh(W_{Q}Q+W_{K}K))V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>Attention</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <mo>,</mo>
        <mi>K</mi>
        <mo>,</mo>
        <mi>V</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>softmax</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>tanh</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>Q</mi>
          </mrow>
        </msub>
        <mi>Q</mi>
        <mo>+</mo>
        <msub>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>K</mi>
          </mrow>
        </msub>
        <mi>K</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(\tanh(W_{Q}Q+W_{K}K))V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/40d34445e83f991acd397ab8979f2f194f5c530e" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:54.987ex; height:3.009ex;" alt="{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(\tanh(W_{Q}Q+W_{K}K))V}"></span>
where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W_{Q}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>Q</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W_{Q}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/27233f4471dd458034969c094d1ac13bff1e38d1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.726ex; height:2.843ex;" alt="{\displaystyle W_{Q}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W_{K}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>K</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W_{K}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1476608b04371ce36cdf6625d8b9ba8a96c615b9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:3.887ex; height:2.509ex;" alt="{\displaystyle W_{K}}"></span> are learnable weight matrices.<sup id="cite&#95;ref-bahdanau&#95;11-2" class="reference"><a href="#cite_note-bahdanau-11"><span class="cite-bracket">&#91;</span>11<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Luong_attention_(general)"><span id="Luong_attention_.28general.29"></span>Luong attention (general)</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=15" title="Edit section: Luong attention (general)"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p><span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(QWK^{T})V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>Attention</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <mo>,</mo>
        <mi>K</mi>
        <mo>,</mo>
        <mi>V</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>softmax</mtext>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Q</mi>
        <mi>W</mi>
        <msup>
          <mi>K</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(QWK^{T})V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5e929a24c593127608cadcb84997c67fdb434741" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:41.925ex; height:3.176ex;" alt="{\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}(QWK^{T})V}"></span>
where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>W</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54a9c4c547f4d6111f81946cad242b18298d70b7" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.435ex; height:2.176ex;" alt="{\displaystyle W}"></span> is a learnable weight matrix.<sup id="cite&#95;ref-xy-dot&#95;38-2" class="reference"><a href="#cite_note-xy-dot-38"><span class="cite-bracket">&#91;</span>38<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Self-attention">Self-attention</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=16" title="Edit section: Self-attention"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Self-attention is essentially the same as cross-attention, except that query, key, and value vectors all come from the same model. Both encoder and decoder can use self-attention, but with subtle differences.
</p><p>For encoder self-attention, we can start with a simple encoder without self-attention, such as an "embedding layer", which simply converts each input word into a vector by a fixed <a href="/wiki/Lookup_table" title="Lookup table">lookup table</a>. This gives a sequence of hidden vectors <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle h_{0},h_{1},\dots }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>h</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>h</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle h_{0},h_{1},\dots }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1c31f089081e2b1d4e3022a7c2e090cb393f74b5" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:9.578ex; height:2.509ex;" alt="{\displaystyle h_{0},h_{1},\dots }"></span>. These can then be applied to a dot-product attention mechanism, to obtain<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{aligned}h_{0}'&amp;=\mathrm {Attention} (h_{0}W^{Q},HW^{K},HW^{V})\\h_{1}'&amp;=\mathrm {Attention} (h_{1}W^{Q},HW^{K},HW^{V})\\&amp;\;\,\vdots \end{aligned}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
            <mtr>
              <mtd>
                <msubsup>
                  <mi>h</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>0</mn>
                  </mrow>
                  <mo>&#x2032;</mo>
                </msubsup>
              </mtd>
              <mtd>
                <mi></mi>
                <mo>=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="normal">A</mi>
                  <mi mathvariant="normal">t</mi>
                  <mi mathvariant="normal">t</mi>
                  <mi mathvariant="normal">e</mi>
                  <mi mathvariant="normal">n</mi>
                  <mi mathvariant="normal">t</mi>
                  <mi mathvariant="normal">i</mi>
                  <mi mathvariant="normal">o</mi>
                  <mi mathvariant="normal">n</mi>
                </mrow>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>h</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>0</mn>
                  </mrow>
                </msub>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>Q</mi>
                  </mrow>
                </msup>
                <mo>,</mo>
                <mi>H</mi>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>K</mi>
                  </mrow>
                </msup>
                <mo>,</mo>
                <mi>H</mi>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>V</mi>
                  </mrow>
                </msup>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msubsup>
                  <mi>h</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                  <mo>&#x2032;</mo>
                </msubsup>
              </mtd>
              <mtd>
                <mi></mi>
                <mo>=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="normal">A</mi>
                  <mi mathvariant="normal">t</mi>
                  <mi mathvariant="normal">t</mi>
                  <mi mathvariant="normal">e</mi>
                  <mi mathvariant="normal">n</mi>
                  <mi mathvariant="normal">t</mi>
                  <mi mathvariant="normal">i</mi>
                  <mi mathvariant="normal">o</mi>
                  <mi mathvariant="normal">n</mi>
                </mrow>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>h</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msub>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>Q</mi>
                  </mrow>
                </msup>
                <mo>,</mo>
                <mi>H</mi>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>K</mi>
                  </mrow>
                </msup>
                <mo>,</mo>
                <mi>H</mi>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>V</mi>
                  </mrow>
                </msup>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd />
              <mtd>
                <mi></mi>
                <mspace width="thickmathspace" />
                <mspace width="thinmathspace" />
                <mo>&#x22EE;<!-- ⋮ --></mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}h_{0}'&amp;=\mathrm {Attention} (h_{0}W^{Q},HW^{K},HW^{V})\\h_{1}'&amp;=\mathrm {Attention} (h_{1}W^{Q},HW^{K},HW^{V})\\&amp;\;\,\vdots \end{aligned}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2c8313b988269a80d9ff4a821dd650a069eb5ed7" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -5.005ex; width:38.769ex; height:11.176ex;" alt="{\displaystyle {\begin{aligned}h_{0}&#039;&amp;=\mathrm {Attention} (h_{0}W^{Q},HW^{K},HW^{V})\\h_{1}&#039;&amp;=\mathrm {Attention} (h_{1}W^{Q},HW^{K},HW^{V})\\&amp;\;\,\vdots \end{aligned}}}"></span>or more succinctly, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle H'=\mathrm {Attention} (HW^{Q},HW^{K},HW^{V})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>H</mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">A</mi>
          <mi mathvariant="normal">t</mi>
          <mi mathvariant="normal">t</mi>
          <mi mathvariant="normal">e</mi>
          <mi mathvariant="normal">n</mi>
          <mi mathvariant="normal">t</mi>
          <mi mathvariant="normal">i</mi>
          <mi mathvariant="normal">o</mi>
          <mi mathvariant="normal">n</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>H</mi>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>Q</mi>
          </mrow>
        </msup>
        <mo>,</mo>
        <mi>H</mi>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>K</mi>
          </mrow>
        </msup>
        <mo>,</mo>
        <mi>H</mi>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>V</mi>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle H'=\mathrm {Attention} (HW^{Q},HW^{K},HW^{V})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f4c3a1f071c91152b75f73e4dce95c0ceb90394c" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:38.083ex; height:3.176ex;" alt="{\displaystyle H&#039;=\mathrm {Attention} (HW^{Q},HW^{K},HW^{V})}"></span>. This can be applied repeatedly, to obtain a multilayered encoder. This is the "encoder self-attention", sometimes called the "all-to-all attention", as the vector at every position can attend to every other.
</p>
<div class="mw-heading mw-heading3"><h3 id="Masking">Masking</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=17" title="Edit section: Masking"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Decoder_self-attention_with_causal_masking,_detailed_diagram.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/7/72/Decoder_self-attention_with_causal_masking%2C_detailed_diagram.png/250px-Decoder_self-attention_with_causal_masking%2C_detailed_diagram.png" decoding="async" width="250" height="175" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/72/Decoder_self-attention_with_causal_masking%2C_detailed_diagram.png/500px-Decoder_self-attention_with_causal_masking%2C_detailed_diagram.png 1.5x" data-file-width="1426" data-file-height="1000" /></a><figcaption>Decoder self-attention with causal masking, detailed diagram</figcaption></figure><p>For decoder self-attention, all-to-all attention is inappropriate, because during the autoregressive decoding process, the decoder cannot attend to future outputs that has yet to be decoded. This can be solved by forcing the attention weights <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}=0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}=0}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/22675f237ad99282d4a6cd7e9742aeeeb1e049fb" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:7.402ex; height:2.843ex;" alt="{\displaystyle w_{ij}=0}"></span> for all <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle i&lt;j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
        <mo>&lt;</mo>
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i&lt;j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e60ff2d1b23e30fb2979e8c1536da03493f943cf" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:4.859ex; height:2.509ex;" alt="{\displaystyle i&lt;j}"></span>, called "causal masking". This attention mechanism is the "causally masked self-attention".
</p><div class="mw-heading mw-heading2"><h2 id="See_also">See also</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=18" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">seq2seq</a></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" class="mw-redirect" title="Transformer (deep learning architecture)">Transformer (deep learning architecture)</a></li>
<li><a href="/wiki/Attention" title="Attention">Attention</a></li>
<li><a href="/wiki/Dynamic_neural_network" class="mw-redirect" title="Dynamic neural network">Dynamic neural network</a></li></ul>
<div class="mw-heading mw-heading2"><h2 id="References">References</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=19" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite&#95;note-Cherry&#95;1953-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-Cherry_1953_1-0">^</a></b></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1333433106">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#bf3c2c)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#bf3c2c)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}</style><cite id="CITEREFCherry1953" class="citation journal cs1">Cherry, E. Colin (1953). "Some Experiments on the Recognition of Speech, with One and with Two Ears". <i>The Journal of the Acoustical Society of America</i>. <b>25</b> (5): <span class="nowrap">975–</span>979. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/1953ASAJ...25..975C">1953ASAJ...25..975C</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1121%2F1.1907229">10.1121/1.1907229</a>. <a href="/wiki/Hdl_(identifier)" class="mw-redirect" title="Hdl (identifier)">hdl</a>:<a rel="nofollow" class="external text" href="https://hdl.handle.net/11858%2F00-001M-0000-002A-F750-3">11858/00-001M-0000-002A-F750-3</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+the+Acoustical+Society+of+America&amp;rft.atitle=Some+Experiments+on+the+Recognition+of+Speech%2C+with+One+and+with+Two+Ears&amp;rft.volume=25&amp;rft.issue=5&amp;rft.pages=975-979&amp;rft.date=1953&amp;rft&#95;id=info%3Ahdl%2F11858%2F00-001M-0000-002A-F750-3&amp;rft&#95;id=info%3Adoi%2F10.1121%2F1.1907229&amp;rft&#95;id=info%3Abibcode%2F1953ASAJ...25..975C&amp;rft.aulast=Cherry&amp;rft.aufirst=E.+Colin&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Broadbent-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-Broadbent_2-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBroadbent1958" class="citation book cs1">Broadbent, Donald E. (1958). <i>Perception and Communication</i>. Pergamon Press.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Perception+and+Communication&amp;rft.pub=Pergamon+Press&amp;rft.date=1958&amp;rft.aulast=Broadbent&amp;rft.aufirst=Donald+E.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Kowler1995-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-Kowler1995_3-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFKowler1995" class="citation journal cs1">Kowler, Eileen (1995). "The control of saccadic eye movements". <i>Reviews of Oculomotor Research</i>. <b>5</b>: <span class="nowrap">1–</span>70.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Reviews+of+Oculomotor+Research&amp;rft.atitle=The+control+of+saccadic+eye+movements&amp;rft.volume=5&amp;rft.pages=1-70&amp;rft.date=1995&amp;rft.aulast=Kowler&amp;rft.aufirst=Eileen&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-PDP-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-PDP_4-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRumelhartHintonMcclelland1987" class="citation book cs1">Rumelhart, David E.; Hinton, G. E.; Mcclelland, James L. (1987-07-29). <a rel="nofollow" class="external text" href="https://stanford.edu/~jlmcc/papers/PDP/Chapter2.pdf">"A General Framework for Parallel Distributed Processing"</a> <span class="cs1-format">(PDF)</span>. In Rumelhart, David E.; Hinton, G. E.; PDP Research Group (eds.). <i>Parallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations</i>. Cambridge, Massachusetts: MIT Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-262-68053-0" title="Special:BookSources/978-0-262-68053-0"><bdi>978-0-262-68053-0</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+General+Framework+for+Parallel+Distributed+Processing&amp;rft.btitle=Parallel+Distributed+Processing%2C+Volume+1%3A+Explorations+in+the+Microstructure+of+Cognition%3A+Foundations&amp;rft.place=Cambridge%2C+Massachusetts&amp;rft.pub=MIT+Press&amp;rft.date=1987-07-29&amp;rft.isbn=978-0-262-68053-0&amp;rft.aulast=Rumelhart&amp;rft.aufirst=David+E.&amp;rft.au=Hinton%2C+G.+E.&amp;rft.au=Mcclelland%2C+James+L.&amp;rft&#95;id=https%3A%2F%2Fstanford.edu%2F~jlmcc%2Fpapers%2FPDP%2FChapter2.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-transform1992-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-transform1992_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-transform1992_5-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSchmidhuber1992" class="citation journal cs1"><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Schmidhuber, Jürgen</a> (1992). "Learning to control fast-weight memories: an alternative to recurrent nets". <i>Neural Computation</i>. <b>4</b> (1): <span class="nowrap">131–</span>139. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1162%2Fneco.1992.4.1.131">10.1162/neco.1992.4.1.131</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:16683347">16683347</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Learning+to+control+fast-weight+memories%3A+an+alternative+to+recurrent+nets.&amp;rft.volume=4&amp;rft.issue=1&amp;rft.pages=131-139&amp;rft.date=1992&amp;rft&#95;id=info%3Adoi%2F10.1162%2Fneco.1992.4.1.131&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A16683347%23id-name%3DS2CID&amp;rft.aulast=Schmidhuber&amp;rft.aufirst=J%C3%BCrgen&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-malsburg1981-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-malsburg1981_6-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFvon&#95;der&#95;Malsburg1981" class="citation journal cs1">von der Malsburg, Christoph (1981). "The correlation theory of brain function". <i>Internal Report 81–2, Max-Planck-Institute for Biophysical Chemistry</i>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Internal+Report+81%E2%80%932%2C+Max-Planck-Institute+for+Biophysical+Chemistry&amp;rft.atitle=The+correlation+theory+of+brain+function&amp;rft.date=1981&amp;rft.aulast=von+der+Malsburg&amp;rft.aufirst=Christoph&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-feldman1982-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-feldman1982_7-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFFeldman1982" class="citation journal cs1">Feldman, Jerome A. (1982). "Dynamic connections in neural networks". <i>Biological Cybernetics</i>. <b>46</b> (1): <span class="nowrap">27–</span>39. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2FBF00335349">10.1007/BF00335349</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/6307398">6307398</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biological+Cybernetics&amp;rft.atitle=Dynamic+connections+in+neural+networks&amp;rft.volume=46&amp;rft.issue=1&amp;rft.pages=27-39&amp;rft.date=1982&amp;rft&#95;id=info%3Adoi%2F10.1007%2FBF00335349&amp;rft&#95;id=info%3Apmid%2F6307398&amp;rft.aulast=Feldman&amp;rft.aufirst=Jerome+A.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-hinton1987-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-hinton1987_8-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFHinton1989" class="citation journal cs1">Hinton, Geoffrey E. (1989). "Connectionist learning procedures". <i>Artificial Intelligence</i>. <b>40</b> (<span class="nowrap">1–</span>3): <span class="nowrap">185–</span>234. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2F0004-3702%2889%2990049-0">10.1016/0004-3702(89)90049-0</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Artificial+Intelligence&amp;rft.atitle=Connectionist+learning+procedures&amp;rft.volume=40&amp;rft.issue=%3Cspan+class%3D%22nowrap%22%3E1%E2%80%93%3C%2Fspan%3E3&amp;rft.pages=185-234&amp;rft.date=1989&amp;rft&#95;id=info%3Adoi%2F10.1016%2F0004-3702%2889%2990049-0&amp;rft.aulast=Hinton&amp;rft.aufirst=Geoffrey+E.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-tomasi1998-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-tomasi1998_9-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFTomasi1998" class="citation conference cs1">Tomasi, Carlo (1998). <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/document/710815"><i>Bilateral filtering for gray and color images</i></a>. ICCV.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Bilateral+filtering+for+gray+and+color+images&amp;rft.date=1998&amp;rft.aulast=Tomasi&amp;rft.aufirst=Carlo&amp;rft&#95;id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F710815&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-buades20052-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-buades20052_10-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBuades2005" class="citation conference cs1">Buades, Antoni (2005). <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/document/1467423"><i>A non-local algorithm for image denoising</i></a>. CVPR.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=A+non-local+algorithm+for+image+denoising&amp;rft.date=2005&amp;rft.aulast=Buades&amp;rft.aufirst=Antoni&amp;rft&#95;id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F1467423&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-bahdanau-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-bahdanau_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bahdanau_11-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-bahdanau_11-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBahdanauChoBengio2014" class="citation arxiv cs1">Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). "Neural Machine Translation by Jointly Learning to Align and Translate". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1409.0473">1409.0473</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Neural+Machine+Translation+by+Jointly+Learning+to+Align+and+Translate&amp;rft.date=2014&amp;rft&#95;id=info%3Aarxiv%2F1409.0473&amp;rft.aulast=Bahdanau&amp;rft.aufirst=Dzmitry&amp;rft.au=Cho%2C+Kyunghyun&amp;rft.au=Bengio%2C+Yoshua&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-wang2014-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-wang2014_12-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFWang2014" class="citation conference cs1">Wang, Qian (2014). <i>Attentional Neural Network: Feature Selection Using Cognitive Feedback</i>. NeurIPS.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Attentional+Neural+Network%3A+Feature+Selection+Using+Cognitive+Feedback&amp;rft.date=2014&amp;rft.aulast=Wang&amp;rft.aufirst=Qian&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-xu2015-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-xu2015_13-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFXuBaKiros2015" class="citation conference cs1">Xu, Kelvin; Ba, Jimmy; Kiros, Ryan (2015). <i>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1502.03044">1502.03044</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Show%2C+Attend+and+Tell%3A+Neural+Image+Caption+Generation+with+Visual+Attention&amp;rft.date=2015&amp;rft&#95;id=info%3Aarxiv%2F1502.03044&amp;rft.aulast=Xu&amp;rft.aufirst=Kelvin&amp;rft.au=Ba%2C+Jimmy&amp;rft.au=Kiros%2C+Ryan&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-vinyals2015-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-vinyals2015_14-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFVinyalsToshevBengioErhan2015" class="citation conference cs1">Vinyals, Oriol; Toshev, Alexander; Bengio, Samy; Erhan, Dumitru (2015). "Show and Tell: A Neural Image Caption Generator". <i>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>. pp.&#160;<span class="nowrap">3156–</span>3164. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FCVPR.2015.7298935">10.1109/CVPR.2015.7298935</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4673-6964-0" title="Special:BookSources/978-1-4673-6964-0"><bdi>978-1-4673-6964-0</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Show+and+Tell%3A+A+Neural+Image+Caption+Generator&amp;rft.btitle=2015+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+%28CVPR%29&amp;rft.pages=3156-3164&amp;rft.date=2015&amp;rft&#95;id=info%3Adoi%2F10.1109%2FCVPR.2015.7298935&amp;rft.isbn=978-1-4673-6964-0&amp;rft.aulast=Vinyals&amp;rft.aufirst=Oriol&amp;rft.au=Toshev%2C+Alexander&amp;rft.au=Bengio%2C+Samy&amp;rft.au=Erhan%2C+Dumitru&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-cheng2016-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-cheng2016_15-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFCheng2016" class="citation arxiv cs1">Cheng, Jianpeng (2016). "Long Short-Term Memory-Networks for Machine Reading". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1601.06733">1601.06733</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Long+Short-Term+Memory-Networks+for+Machine+Reading&amp;rft.date=2016&amp;rft&#95;id=info%3Aarxiv%2F1601.06733&amp;rft.aulast=Cheng&amp;rft.aufirst=Jianpeng&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-paulus2017-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-paulus2017_16-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFPaulus2017" class="citation arxiv cs1">Paulus, Romain (2017). "A Deep Reinforced Model for Abstractive Summarization". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1705.04304">1705.04304</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=A+Deep+Reinforced+Model+for+Abstractive+Summarization&amp;rft.date=2017&amp;rft&#95;id=info%3Aarxiv%2F1705.04304&amp;rft.aulast=Paulus&amp;rft.aufirst=Romain&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-parikh2016-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-parikh2016_17-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFParikh2016" class="citation conference cs1">Parikh, Anees (2016). <i>Decomposable Attention Model for Natural Language Inference</i>. EMNLP. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1606.01933">1606.01933</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Decomposable+Attention+Model+for+Natural+Language+Inference&amp;rft.date=2016&amp;rft&#95;id=info%3Aarxiv%2F1606.01933&amp;rft.aulast=Parikh&amp;rft.aufirst=Anees&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-lin2017-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-lin2017_18-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLin2017" class="citation conference cs1">Lin, Zichao (2017). <i>A Structured Self-Attentive Sentence Embedding</i>. ICLR. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1703.03130">1703.03130</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=A+Structured+Self-Attentive+Sentence+Embedding&amp;rft.date=2017&amp;rft&#95;id=info%3Aarxiv%2F1703.03130&amp;rft.aulast=Lin&amp;rft.aufirst=Zichao&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-allyouneed-19"><span class="mw-cite-backlink">^ <a href="#cite_ref-allyouneed_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-allyouneed_19-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFVaswaniShazeerParmarUszkoreit2017" class="citation arxiv cs1">Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia (2017). "Attention is All You Need". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1706.03762">1706.03762</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Attention+is+All+You+Need&amp;rft.date=2017&amp;rft&#95;id=info%3Aarxiv%2F1706.03762&amp;rft.aulast=Vaswani&amp;rft.aufirst=Ashish&amp;rft.au=Shazeer%2C+Noam&amp;rft.au=Parmar%2C+Niki&amp;rft.au=Uszkoreit%2C+Jakob&amp;rft.au=Jones%2C+Llion&amp;rft.au=Gomez%2C+Aidan+N.&amp;rft.au=Kaiser%2C+Lukasz&amp;rft.au=Polosukhin%2C+Illia&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-santoro2017-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-santoro2017_20-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSantoro2017" class="citation conference cs1">Santoro, Adam (2017). <i>Relation Networks for Relational Reasoning</i>. ICLR. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1706.01427">1706.01427</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Relation+Networks+for+Relational+Reasoning&amp;rft.date=2017&amp;rft&#95;id=info%3Aarxiv%2F1706.01427&amp;rft.aulast=Santoro&amp;rft.aufirst=Adam&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-lee2019-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-lee2019_21-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLee2019" class="citation conference cs1">Lee, Juho (2019). <i>Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks</i>. ICML. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1810.00825">1810.00825</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Set+Transformer%3A+A+Framework+for+Attention-based+Permutation-Invariant+Neural+Networks&amp;rft.date=2019&amp;rft&#95;id=info%3Aarxiv%2F1810.00825&amp;rft.aulast=Lee&amp;rft.aufirst=Juho&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-wang2018-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-wang2018_22-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFWang2018" class="citation conference cs1">Wang, Xiaolong (2018). <i>Non-Local Neural Networks</i>. CVPR.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Non-Local+Neural+Networks&amp;rft.date=2018&amp;rft.aulast=Wang&amp;rft.aufirst=Xiaolong&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-velickovic2018-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-velickovic2018_23-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFVeličković2018" class="citation conference cs1">Veličković, Petar (2018). <i>Graph Attention Networks</i>. ICLR.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Graph+Attention+Networks&amp;rft.date=2018&amp;rft.aulast=Veli%C4%8Dkovi%C4%87&amp;rft.aufirst=Petar&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-reformer2020-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-reformer2020_24-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFKitaev2020" class="citation conference cs1">Kitaev, Nikita (2020). <i>Reformer: The Efficient Transformer</i>. ICLR. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2001.04451">2001.04451</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Reformer%3A+The+Efficient+Transformer&amp;rft.date=2020&amp;rft&#95;id=info%3Aarxiv%2F2001.04451&amp;rft.aulast=Kitaev&amp;rft.aufirst=Nikita&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-linformer2020-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-linformer2020_25-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFWang2020" class="citation conference cs1">Wang, Salah (2020). <i>Linformer: Self-Attention with Linear Complexity</i>. ICLR. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2006.04768">2006.04768</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Linformer%3A+Self-Attention+with+Linear+Complexity&amp;rft.date=2020&amp;rft&#95;id=info%3Aarxiv%2F2006.04768&amp;rft.aulast=Wang&amp;rft.aufirst=Salah&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-performer2020-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-performer2020_26-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFChoromanski2020" class="citation conference cs1">Choromanski, Krzysztof (2020). <i>Rethinking Attention with Performers</i>. ICLR. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2009.14794">2009.14794</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Rethinking+Attention+with+Performers&amp;rft.date=2020&amp;rft&#95;id=info%3Aarxiv%2F2009.14794&amp;rft.aulast=Choromanski&amp;rft.aufirst=Krzysztof&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-ramsauer2021-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-ramsauer2021_27-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRamsauer2021" class="citation conference cs1">Ramsauer, Johannes (2021). <i>Hopfield Networks is All You Need</i>. NeurIPS. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2008.02217">2008.02217</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Hopfield+Networks+is+All+You+Need&amp;rft.date=2021&amp;rft&#95;id=info%3Aarxiv%2F2008.02217&amp;rft.aulast=Ramsauer&amp;rft.aufirst=Johannes&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-dosovitskiy2021-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-dosovitskiy2021_28-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFDosovitskiy2021" class="citation conference cs1">Dosovitskiy, Aleksander (2021). <i>An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale</i>. ICLR. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2010.11929">2010.11929</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=An+Image+is+Worth+16%C3%9716+Words%3A+Transformers+for+Image+Recognition+at+Scale&amp;rft.date=2021&amp;rft&#95;id=info%3Aarxiv%2F2010.11929&amp;rft.aulast=Dosovitskiy&amp;rft.aufirst=Aleksander&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-alphafold-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-alphafold_29-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFJumper2021" class="citation journal cs1">Jumper, John (2021). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8371605">"Highly accurate protein structure prediction with AlphaFold"</a>. <i>Nature</i>. <b>596</b> (7873): <span class="nowrap">583–</span>589. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2021Natur.596..583J">2021Natur.596..583J</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fs41586-021-03819-2">10.1038/s41586-021-03819-2</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8371605">8371605</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/34265844">34265844</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Highly+accurate+protein+structure+prediction+with+AlphaFold&amp;rft.volume=596&amp;rft.issue=7873&amp;rft.pages=583-589&amp;rft.date=2021&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC8371605%23id-name%3DPMC&amp;rft&#95;id=info%3Apmid%2F34265844&amp;rft&#95;id=info%3Adoi%2F10.1038%2Fs41586-021-03819-2&amp;rft&#95;id=info%3Abibcode%2F2021Natur.596..583J&amp;rft.aulast=Jumper&amp;rft.aufirst=John&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC8371605&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-clip-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-clip_30-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRadford2021" class="citation conference cs1">Radford, Alec (2021). <i>Learning Transferable Visual Models from Natural Language Supervision</i>. ICML.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Learning+Transferable+Visual+Models+from+Natural+Language+Supervision&amp;rft.date=2021&amp;rft.aulast=Radford&amp;rft.aufirst=Alec&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-ccnet-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-ccnet_31-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFHuang2019" class="citation conference cs1">Huang, Xiangyu (2019). <i>CCNet: Criss-Cross Attention for Semantic Segmentation</i>. ICCV. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1811.11721">1811.11721</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=CCNet%3A+Criss-Cross+Attention+for+Semantic+Segmentation&amp;rft.date=2019&amp;rft&#95;id=info%3Aarxiv%2F1811.11721&amp;rft.aulast=Huang&amp;rft.aufirst=Xiangyu&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-danet-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-danet_32-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFFu2019" class="citation conference cs1">Fu, Jing (2019). <i>Dual Attention Network for Scene Segmentation</i>. CVPR. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1809.02983">1809.02983</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Dual+Attention+Network+for+Scene+Segmentation&amp;rft.date=2019&amp;rft&#95;id=info%3Aarxiv%2F1809.02983&amp;rft.aulast=Fu&amp;rft.aufirst=Jing&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:0-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-:0_33-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFNiuZhongYu2021" class="citation journal cs1">Niu, Zhaoyang; Zhong, Guoqiang; Yu, Hui (2021-09-10). <span class="id-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://www.sciencedirect.com/science/article/pii/S092523122100477X">"A review on the attention mechanism of deep learning"</a></span>. <i>Neurocomputing</i>. <b>452</b>: <span class="nowrap">48–</span>62. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neucom.2021.03.091">10.1016/j.neucom.2021.03.091</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/0925-2312">0925-2312</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=A+review+on+the+attention+mechanism+of+deep+learning&amp;rft.volume=452&amp;rft.pages=48-62&amp;rft.date=2021-09-10&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.neucom.2021.03.091&amp;rft.issn=0925-2312&amp;rft.aulast=Niu&amp;rft.aufirst=Zhaoyang&amp;rft.au=Zhong%2C+Guoqiang&amp;rft.au=Yu%2C+Hui&amp;rft&#95;id=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS092523122100477X&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:1-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-:1_34-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSoydaner2022" class="citation journal cs1">Soydaner, Derya (August 2022). <a rel="nofollow" class="external text" href="https://link.springer.com/10.1007/s00521-022-07366-3">"Attention mechanism in neural networks: where it comes and where it goes"</a>. <i>Neural Computing and Applications</i>. <b>34</b> (16): <span class="nowrap">13371–</span>13385. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2204.13154">2204.13154</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00521-022-07366-3">10.1007/s00521-022-07366-3</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/0941-0643">0941-0643</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computing+and+Applications&amp;rft.atitle=Attention+mechanism+in+neural+networks%3A+where+it+comes+and+where+it+goes&amp;rft.volume=34&amp;rft.issue=16&amp;rft.pages=13371-13385&amp;rft.date=2022-08&amp;rft&#95;id=info%3Aarxiv%2F2204.13154&amp;rft.issn=0941-0643&amp;rft&#95;id=info%3Adoi%2F10.1007%2Fs00521-022-07366-3&amp;rft.aulast=Soydaner&amp;rft.aufirst=Derya&amp;rft&#95;id=https%3A%2F%2Flink.springer.com%2F10.1007%2Fs00521-022-07366-3&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-bdritz2017-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-bdritz2017_35-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBritzGoldieLuongLe2017" class="citation arxiv cs1">Britz, Denny; Goldie, Anna; Luong, Minh-Thanh; Le, Quoc (2017-03-21). "Massive Exploration of Neural Machine Translation Architectures". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1703.03906">1703.03906</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Massive+Exploration+of+Neural+Machine+Translation+Architectures&amp;rft.date=2017-03-21&amp;rft&#95;id=info%3Aarxiv%2F1703.03906&amp;rft.aulast=Britz&amp;rft.aufirst=Denny&amp;rft.au=Goldie%2C+Anna&amp;rft.au=Luong%2C+Minh-Thanh&amp;rft.au=Le%2C+Quoc&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-pytorch&#95;s2s-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-pytorch_s2s_36-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">"Pytorch.org seq2seq tutorial"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">December 2,</span> 2021</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Pytorch.org+seq2seq+tutorial&amp;rft&#95;id=https%3A%2F%2Fpytorch.org%2Ftutorials%2Fintermediate%2Fseq2seq&#95;translation&#95;tutorial.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-schlag2021-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-schlag2021_37-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSchlagIrieSchmidhuber2021" class="citation conference cs1"><a href="/wiki/Juergen_Schmidhuber" class="mw-redirect" title="Juergen Schmidhuber">Schlag, Imanol</a>; Irie, Kazuki; Schmidhuber, Jürgen (2021). "Linear Transformers Are Secretly Fast Weight Programmers". <i>ICML 2021</i>. Springer. pp.&#160;<span class="nowrap">9355–</span>9366.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Linear+Transformers+Are+Secretly+Fast+Weight+Programmers&amp;rft.btitle=ICML+2021&amp;rft.pages=9355-9366&amp;rft.pub=Springer&amp;rft.date=2021&amp;rft.aulast=Schlag&amp;rft.aufirst=Imanol&amp;rft.au=Irie%2C+Kazuki&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-xy-dot-38"><span class="mw-cite-backlink">^ <a href="#cite_ref-xy-dot_38-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-xy-dot_38-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-xy-dot_38-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLuong2015" class="citation arxiv cs1">Luong, Minh-Thang (2015-09-20). "Effective Approaches to Attention-Based Neural Machine Translation". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1508.04025v5">1508.04025v5</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Effective+Approaches+to+Attention-Based+Neural+Machine+Translation&amp;rft.date=2015-09-20&amp;rft&#95;id=info%3Aarxiv%2F1508.04025v5&amp;rft.aulast=Luong&amp;rft.aufirst=Minh-Thang&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-luo-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-luo_39-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLuoZhangXu2024" class="citation web cs1">Luo, Fan; Zhang, Juan; Xu, Shenghui (3 July 2024). <a rel="nofollow" class="external text" href="https://www.catalyzex.com/paper/learning-positional-attention-for-sequential">"Learning Positional Attention for Sequential Recommendation"</a>. <i>catalyzex.com</i>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=catalyzex.com&amp;rft.atitle=Learning+Positional+Attention+for+Sequential+Recommendation&amp;rft.date=2024-07-03&amp;rft.aulast=Luo&amp;rft.aufirst=Fan&amp;rft.au=Zhang%2C+Juan&amp;rft.au=Xu%2C+Shenghui&amp;rft&#95;id=https%3A%2F%2Fwww.catalyzex.com%2Fpaper%2Flearning-positional-attention-for-sequential&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-xzhu1-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-xzhu1_40-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFZhuChengZhangLin2019" class="citation book cs1">Zhu, Xizhou; Cheng, Dazhi; Zhang, Zheng; Lin, Stephen; Dai, Jifeng (2019). "An Empirical Study of Spatial Attention Mechanisms in Deep Networks". <i>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</i>. pp.&#160;<span class="nowrap">6687–</span>6696. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1904.05873">1904.05873</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FICCV.2019.00679">10.1109/ICCV.2019.00679</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-7281-4803-8" title="Special:BookSources/978-1-7281-4803-8"><bdi>978-1-7281-4803-8</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:118673006">118673006</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=An+Empirical+Study+of+Spatial+Attention+Mechanisms+in+Deep+Networks&amp;rft.btitle=2019+IEEE%2FCVF+International+Conference+on+Computer+Vision+%28ICCV%29&amp;rft.pages=6687-6696&amp;rft.date=2019&amp;rft&#95;id=info%3Aarxiv%2F1904.05873&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A118673006%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1109%2FICCV.2019.00679&amp;rft.isbn=978-1-7281-4803-8&amp;rft.aulast=Zhu&amp;rft.aufirst=Xizhou&amp;rft.au=Cheng%2C+Dazhi&amp;rft.au=Zhang%2C+Zheng&amp;rft.au=Lin%2C+Stephen&amp;rft.au=Dai%2C+Jifeng&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-jhu1-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-jhu1_41-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFHuShenSun2018" class="citation book cs1">Hu, Jie; Shen, Li; Sun, Gang (2018). "Squeeze-and-Excitation Networks". <i>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>. pp.&#160;<span class="nowrap">7132–</span>7141. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1709.01507">1709.01507</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FCVPR.2018.00745">10.1109/CVPR.2018.00745</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5386-6420-9" title="Special:BookSources/978-1-5386-6420-9"><bdi>978-1-5386-6420-9</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:206597034">206597034</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Squeeze-and-Excitation+Networks&amp;rft.btitle=2018+IEEE%2FCVF+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;rft.pages=7132-7141&amp;rft.date=2018&amp;rft&#95;id=info%3Aarxiv%2F1709.01507&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A206597034%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1109%2FCVPR.2018.00745&amp;rft.isbn=978-1-5386-6420-9&amp;rft.aulast=Hu&amp;rft.aufirst=Jie&amp;rft.au=Shen%2C+Li&amp;rft.au=Sun%2C+Gang&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-psanghyun1-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-psanghyun1_42-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFWooParkLeeKweon2018" class="citation arxiv cs1">Woo, Sanghyun; Park, Jongchan; Lee, Joon-Young; Kweon, In So (2018-07-18). "CBAM: Convolutional Block Attention Module". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1807.06521">1807.06521</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=CBAM%3A+Convolutional+Block+Attention+Module&amp;rft.date=2018-07-18&amp;rft&#95;id=info%3Aarxiv%2F1807.06521&amp;rft.aulast=Woo&amp;rft.aufirst=Sanghyun&amp;rft.au=Park%2C+Jongchan&amp;rft.au=Lee%2C+Joon-Young&amp;rft.au=Kweon%2C+In+So&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-mgeorgescu-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-mgeorgescu_43-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFGeorgescuIonescuMironSavencu2022" class="citation arxiv cs1">Georgescu, Mariana-Iuliana; Ionescu, Radu Tudor; Miron, Andreea-Iuliana; Savencu, Olivian; Ristea, Nicolae-Catalin; Verga, Nicolae; Khan, Fahad Shahbaz (2022-10-12). "Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2204.04218">2204.04218</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/eess.IV">eess.IV</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Multimodal+Multi-Head+Convolutional+Attention+with+Various+Kernel+Sizes+for+Medical+Image+Super-Resolution&amp;rft.date=2022-10-12&amp;rft&#95;id=info%3Aarxiv%2F2204.04218&amp;rft.aulast=Georgescu&amp;rft.aufirst=Mariana-Iuliana&amp;rft.au=Ionescu%2C+Radu+Tudor&amp;rft.au=Miron%2C+Andreea-Iuliana&amp;rft.au=Savencu%2C+Olivian&amp;rft.au=Ristea%2C+Nicolae-Catalin&amp;rft.au=Verga%2C+Nicolae&amp;rft.au=Khan%2C+Fahad+Shahbaz&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-xy-qkv-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-xy-qkv_44-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite class="citation audio-visual cs1">Neil Rhodes (2021). <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=rA28vBqN4RM"><i>CS 152 NN—27: Attention: Keys, Queries, &amp; Values</i></a>.  Event occurs at 06:30<span class="reference-accessdate">. Retrieved <span class="nowrap">2021-12-22</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=CS+152+NN%E2%80%9427%3A+Attention%3A+Keys%2C+Queries%2C+%26+Values&amp;rft.date=2021&amp;rft&#95;id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DrA28vBqN4RM&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-xx-dot-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-xx-dot_45-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite class="citation audio-visual cs1">Alfredo Canziani &amp; Yann Lecun (2021). <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=f01J0Dri-6k"><i>NYU Deep Learning course, Spring 2020</i></a>.  Event occurs at 05:30<span class="reference-accessdate">. Retrieved <span class="nowrap">2021-12-22</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=NYU+Deep+Learning+course%2C+Spring+2020&amp;rft.date=2021&amp;rft&#95;id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Df01J0Dri-6k&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-xx-qkv-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-xx-qkv_46-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite class="citation audio-visual cs1">Alfredo Canziani &amp; Yann Lecun (2021). <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=f01J0Dri-6k"><i>NYU Deep Learning course, Spring 2020</i></a>.  Event occurs at 20:15<span class="reference-accessdate">. Retrieved <span class="nowrap">2021-12-22</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=NYU+Deep+Learning+course%2C+Spring+2020&amp;rft.date=2021&amp;rft&#95;id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Df01J0Dri-6k&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-pytorch-tutorial-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-pytorch-tutorial_47-0">^</a></b></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRobertson" class="citation web cs1">Robertson, Sean. <a rel="nofollow" class="external text" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">"NLP From Scratch: Translation With a Sequence To Sequence Network and Attention"</a>. <i>pytorch.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2021-12-22</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=pytorch.org&amp;rft.atitle=NLP+From+Scratch%3A+Translation+With+a+Sequence+To+Sequence+Network+and+Attention&amp;rft.aulast=Robertson&amp;rft.aufirst=Sean&amp;rft&#95;id=https%3A%2F%2Fpytorch.org%2Ftutorials%2Fintermediate%2Fseq2seq&#95;translation&#95;tutorial.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFMittal2024" class="citation web cs1">Mittal, Aayush (2024-07-17). <a rel="nofollow" class="external text" href="https://www.unite.ai/flash-attention-revolutionizing-transformer-efficiency/">"Flash Attention: Revolutionizing Transformer Efficiency"</a>. <i>Unite.AI</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-11-16</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Unite.AI&amp;rft.atitle=Flash+Attention%3A+Revolutionizing+Transformer+Efficiency&amp;rft.date=2024-07-17&amp;rft.aulast=Mittal&amp;rft.aufirst=Aayush&amp;rft&#95;id=https%3A%2F%2Fwww.unite.ai%2Fflash-attention-revolutionizing-transformer-efficiency%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://pytorch.org/blog/flexattention/">"FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention – PyTorch"</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=FlexAttention%3A+The+Flexibility+of+PyTorch+with+the+Performance+of+FlashAttention+%E2%80%93+PyTorch&amp;rft&#95;id=https%3A%2F%2Fpytorch.org%2Fblog%2Fflexattention%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-50">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFDosovitskiyBeyerKolesnikovWeissenborn2021" class="citation cs2">Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg (2021-06-03), <i>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2010.11929">2010.11929</a></span></cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=An+Image+is+Worth+16x16+Words%3A+Transformers+for+Image+Recognition+at+Scale&amp;rft.date=2021-06-03&amp;rft&#95;id=info%3Aarxiv%2F2010.11929&amp;rft.aulast=Dosovitskiy&amp;rft.aufirst=Alexey&amp;rft.au=Beyer%2C+Lucas&amp;rft.au=Kolesnikov%2C+Alexander&amp;rft.au=Weissenborn%2C+Dirk&amp;rft.au=Zhai%2C+Xiaohua&amp;rft.au=Unterthiner%2C+Thomas&amp;rft.au=Dehghani%2C+Mostafa&amp;rft.au=Minderer%2C+Matthias&amp;rft.au=Heigold%2C+Georg&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFAbnarZuidema2020" class="citation cs2">Abnar, Samira; Zuidema, Willem (2020-05-31), <i>Quantifying Attention Flow in Transformers</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2005.00928">2005.00928</a></span></cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Quantifying+Attention+Flow+in+Transformers&amp;rft.date=2020-05-31&amp;rft&#95;id=info%3Aarxiv%2F2005.00928&amp;rft.aulast=Abnar&amp;rft.aufirst=Samira&amp;rft.au=Zuidema%2C+Willem&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-52">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBrockiBindaChung2024" class="citation cs2">Brocki, Lennart; Binda, Jakub; Chung, Neo Christopher (2024-10-25), <i>Class-Discriminative Attention Maps for Vision Transformers</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2312.02364">2312.02364</a></span></cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Class-Discriminative+Attention+Maps+for+Vision+Transformers&amp;rft.date=2024-10-25&amp;rft&#95;id=info%3Aarxiv%2F2312.02364&amp;rft.aulast=Brocki&amp;rft.aufirst=Lennart&amp;rft.au=Binda%2C+Jakub&amp;rft.au=Chung%2C+Neo+Christopher&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-53">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFGildenblat2025" class="citation cs2">Gildenblat, Jacob (2025-07-21), <a rel="nofollow" class="external text" href="https://github.com/jacobgil/pytorch-grad-cam"><i>jacobgil/pytorch-grad-cam</i></a><span class="reference-accessdate">, retrieved <span class="nowrap">2025-07-21</span></span></cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=jacobgil%2Fpytorch-grad-cam&amp;rft.date=2025-07-21&amp;rft.aulast=Gildenblat&amp;rft.aufirst=Jacob&amp;rft&#95;id=https%3A%2F%2Fgithub.com%2Fjacobgil%2Fpytorch-grad-cam&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFMullenbachWiegreffeDukeSun2018" class="citation cs2">Mullenbach, James; Wiegreffe, Sarah; Duke, Jon; Sun, Jimeng; Eisenstein, Jacob (2018-04-16), <i>Explainable Prediction of Medical Codes from Clinical Text</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1802.05695">1802.05695</a></span></cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Explainable+Prediction+of+Medical+Codes+from+Clinical+Text&amp;rft.date=2018-04-16&amp;rft&#95;id=info%3Aarxiv%2F1802.05695&amp;rft.aulast=Mullenbach&amp;rft.aufirst=James&amp;rft.au=Wiegreffe%2C+Sarah&amp;rft.au=Duke%2C+Jon&amp;rft.au=Sun%2C+Jimeng&amp;rft.au=Eisenstein%2C+Jacob&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBahdanauChoBengio2016" class="citation cs2">Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2016-05-19), <i>Neural Machine Translation by Jointly Learning to Align and Translate</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1409.0473">1409.0473</a></span></cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Neural+Machine+Translation+by+Jointly+Learning+to+Align+and+Translate&amp;rft.date=2016-05-19&amp;rft&#95;id=info%3Aarxiv%2F1409.0473&amp;rft.aulast=Bahdanau&amp;rft.aufirst=Dzmitry&amp;rft.au=Cho%2C+Kyunghyun&amp;rft.au=Bengio%2C+Yoshua&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSerranoSmith2019" class="citation cs2">Serrano, Sofia; Smith, Noah A. (2019-06-09), <i>Is Attention Interpretable?</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1906.03731">1906.03731</a></span></cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Is+Attention+Interpretable%3F&amp;rft.date=2019-06-09&amp;rft&#95;id=info%3Aarxiv%2F1906.03731&amp;rft.aulast=Serrano&amp;rft.aufirst=Sofia&amp;rft.au=Smith%2C+Noah+A.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-SetTransformer-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-SetTransformer_57-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLeeLeeKimKosiorek2018" class="citation arxiv cs1">Lee, Juho; Lee, Yoonho; Kim, Jungtaek; Kosiorek, Adam R; Choi, Seungjin; Teh, Yee Whye (2018). "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1810.00825">1810.00825</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Set+Transformer%3A+A+Framework+for+Attention-based+Permutation-Invariant+Neural+Networks&amp;rft.date=2018&amp;rft&#95;id=info%3Aarxiv%2F1810.00825&amp;rft.aulast=Lee&amp;rft.aufirst=Juho&amp;rft.au=Lee%2C+Yoonho&amp;rft.au=Kim%2C+Jungtaek&amp;rft.au=Kosiorek%2C+Adam+R&amp;rft.au=Choi%2C+Seungjin&amp;rft.au=Teh%2C+Yee+Whye&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></span>
</li>
</ol></div>
<div class="mw-heading mw-heading2"><h2 id="External_links">External links</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Attention_(machine_learning)&amp;action=edit&amp;section=20" title="Edit section: External links"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFOlahCarter2016" class="citation journal cs1">Olah, Chris; Carter, Shan (September 8, 2016). <span class="id-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://distill.pub/2016/augmented-rnns/">"Attention and Augmented Recurrent Neural Networks"</a></span>. <i>Distill</i>. <b>1</b> (9). Distill Working Group. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.23915%2Fdistill.00001">10.23915/distill.00001</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Distill&amp;rft.atitle=Attention+and+Augmented+Recurrent+Neural+Networks&amp;rft.volume=1&amp;rft.issue=9&amp;rft.date=2016-09-08&amp;rft&#95;id=info%3Adoi%2F10.23915%2Fdistill.00001&amp;rft.aulast=Olah&amp;rft.aufirst=Chris&amp;rft.au=Carter%2C+Shan&amp;rft&#95;id=https%3A%2F%2Fdistill.pub%2F2016%2Faugmented-rnns%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AAttention+%28machine+learning%29" class="Z3988"></span></li>
<li><a href="/wiki/Dan_Jurafsky" title="Dan Jurafsky">Dan Jurafsky</a> and James H. Martin (2022). <a rel="nofollow" class="external text" href="https://web.stanford.edu/~jurafsky/slp3/"><i>Speech and Language Processing</i> (3rd ed. draft, January 2022)</a> — Chapter 10.4 (Attention) and Chapter 9.7 (Self-Attention Networks: Transformers)</li>
<li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a> (2020). <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=AIiwuClvH6k">Attention and Memory in Deep Learning</a> — video lecture from <a href="/wiki/DeepMind" class="mw-redirect" title="DeepMind">DeepMind</a> / <a href="/wiki/University_College_London" title="University College London">UCL</a></li></ul>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333133064" /><style data-mw-deduplicate="TemplateStyles:r1314944253">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd;color:inherit}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf;color:inherit}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf;color:inherit}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff;color:inherit}.mw-parser-output .navbox-even{background-color:#f7f7f7;color:inherit}.mw-parser-output .navbox-odd{background-color:transparent;color:inherit}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}</style></div><div role="navigation" class="navbox" aria-labelledby="Artificial&#95;intelligence&#95;(AI)8549" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333133064" /><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1239400231" /><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Artificial_intelligence_navbox" title="Template:Artificial intelligence navbox"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Artificial_intelligence_navbox" title="Template talk:Artificial intelligence navbox"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Artificial_intelligence_navbox" title="Special:EditPage/Template:Artificial intelligence navbox"><abbr title="Edit this template">e</abbr></a></li></ul></div><div id="Artificial&#95;intelligence&#95;(AI)8549" style="font-size:114%;margin:0 4em"><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a> (AI)</div></th></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><a href="/wiki/History_of_artificial_intelligence" title="History of artificial intelligence">History</a>
<ul><li><a href="/wiki/Timeline_of_artificial_intelligence" title="Timeline of artificial intelligence">timeline</a></li></ul></li>
<li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_companies" title="List of artificial intelligence companies">Companies</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">Projects</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Parameter" title="Parameter">Parameter</a>
<ul><li><a href="/wiki/Hyperparameter_(machine_learning)" title="Hyperparameter (machine learning)">Hyperparameter</a></li></ul></li>
<li><a href="/wiki/Loss_functions_for_classification" title="Loss functions for classification">Loss functions</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a>
<ul><li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Double_descent" title="Double descent">Double descent</a></li>
<li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li></ul></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a>
<ul><li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD</a></li>
<li><a href="/wiki/Quasi-Newton_method" title="Quasi-Newton method">Quasi-Newton method</a></li>
<li><a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">Conjugate gradient method</a></li></ul></li>
<li><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a></li>
<li><a class="mw-selflink selflink">Attention</a></li>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Normalization_(machine_learning)" title="Normalization (machine learning)">Normalization</a>
<ul><li><a href="/wiki/Batch_normalization" title="Batch normalization">Batchnorm</a></li></ul></li>
<li><a href="/wiki/Activation_function" title="Activation function">Activation</a>
<ul><li><a href="/wiki/Softmax_function" title="Softmax function">Softmax</a></li>
<li><a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a></li>
<li><a href="/wiki/Rectifier_(neural_networks)" class="mw-redirect" title="Rectifier (neural networks)">Rectifier</a></li></ul></li>
<li><a href="/wiki/Gating_mechanism" title="Gating mechanism">Gating</a></li>
<li><a href="/wiki/Weight_initialization" title="Weight initialization">Weight initialization</a></li>
<li><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">Regularization</a></li>
<li><a href="/wiki/Training,_validation,_and_test_data_sets" title="Training, validation, and test data sets">Datasets</a>
<ul><li><a href="/wiki/Data_augmentation" title="Data augmentation">Augmentation</a></li></ul></li>
<li><a href="/wiki/Prompt_engineering" title="Prompt engineering">Prompt engineering</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a>
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Imitation_learning" title="Imitation learning">Imitation</a></li>
<li><a href="/wiki/Policy_gradient_method" title="Policy gradient method">Policy gradient</a></li></ul></li>
<li><a href="/wiki/Diffusion_process" title="Diffusion process">Diffusion</a></li>
<li><a href="/wiki/Latent_diffusion_model" title="Latent diffusion model">Latent diffusion model</a></li>
<li><a href="/wiki/Autoregressive_model" title="Autoregressive model">Autoregression</a></li>
<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversary</a></li>
<li><a href="/wiki/Retrieval-augmented_generation" title="Retrieval-augmented generation">RAG</a></li>
<li><a href="/wiki/Uncanny_valley" title="Uncanny valley">Uncanny valley</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reflection_(artificial_intelligence)" class="mw-redirect" title="Reflection (artificial intelligence)">Reflection</a></li>
<li><a href="/wiki/Recursive_self-improvement" title="Recursive self-improvement">Recursive self-improvement</a></li>
<li><a href="/wiki/Hallucination_(artificial_intelligence)" title="Hallucination (artificial intelligence)">Hallucination</a></li>
<li><a href="/wiki/Word_embedding" title="Word embedding">Word embedding</a></li>
<li><a href="/wiki/Vibe_coding" title="Vibe coding">Vibe coding</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Applications_of_artificial_intelligence" title="Applications of artificial intelligence">Applications</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a>
<ul><li><a href="/wiki/Prompt_engineering#In-context_learning" title="Prompt engineering">In-context learning</a></li></ul></li>
<li><a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">Artificial neural network</a>
<ul><li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul></li>
<li><a href="/wiki/Language_model" title="Language model">Language model</a>
<ul><li><a href="/wiki/Large_language_model" title="Large language model">Large</a></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>
<li><a href="/wiki/Reasoning_model" title="Reasoning model">Reasoning</a></li></ul></li>
<li><a href="/wiki/Model_Context_Protocol" title="Model Context Protocol">Model Context Protocol</a></li>
<li><a href="/wiki/Intelligent_agent" title="Intelligent agent">Intelligent agent</a></li>
<li><a href="/wiki/Artificial_human_companion" title="Artificial human companion">Artificial human companion</a></li>
<li><a href="/wiki/Humanity%27s_Last_Exam" title="Humanity&#39;s Last Exam">Humanity's Last Exam</a></li>
<li><a href="/wiki/Lethal_autonomous_weapon" title="Lethal autonomous weapon">Lethal autonomous weapons (LAWs)</a></li>
<li><a href="/wiki/Generative_artificial_intelligence" title="Generative artificial intelligence">Generative artificial intelligence (GenAI)</a></li>
<li>(Hypothetical: <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Artificial general intelligence (AGI)</a>)</li>
<li>(Hypothetical: <a href="/wiki/Artificial_superintelligence" class="mw-redirect" title="Artificial superintelligence">Artificial superintelligence (ASI)</a>)</li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Implementations</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Audio–visual</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>
<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/Deep_learning_speech_synthesis" title="Deep learning speech synthesis">Speech synthesis</a>
<ul><li><a href="/wiki/15.ai" title="15.ai">15.ai</a></li>
<li><a href="/wiki/ElevenLabs" title="ElevenLabs">ElevenLabs</a></li></ul></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a>
<ul><li><a href="/wiki/Whisper_(speech_recognition_system)" title="Whisper (speech recognition system)">Whisper</a></li></ul></li>
<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition</a></li>
<li><a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a></li>
<li><a href="/wiki/Text-to-image_model" title="Text-to-image model">Text-to-image models</a>
<ul><li><a href="/wiki/Aurora_(text-to-image_model)" class="mw-redirect" title="Aurora (text-to-image model)">Aurora</a></li>
<li><a href="/wiki/DALL-E" title="DALL-E">DALL-E</a></li>
<li><a href="/wiki/Adobe_Firefly" title="Adobe Firefly">Firefly</a></li>
<li><a href="/wiki/Flux_(text-to-image_model)" title="Flux (text-to-image model)">Flux</a></li>
<li><a href="/wiki/GPT_Image" title="GPT Image">GPT Image</a></li>
<li><a href="/wiki/Ideogram_(text-to-image_model)" title="Ideogram (text-to-image model)">Ideogram</a></li>
<li><a href="/wiki/Imagen_(text-to-image_model)" title="Imagen (text-to-image model)">Imagen</a></li>
<li><a href="/wiki/Midjourney" title="Midjourney">Midjourney</a></li>
<li><a href="/wiki/Recraft" title="Recraft">Recraft</a></li>
<li><a href="/wiki/Stable_Diffusion" title="Stable Diffusion">Stable Diffusion</a></li></ul></li>
<li><a href="/wiki/Text-to-video_model" title="Text-to-video model">Text-to-video models</a>
<ul><li><a href="/wiki/Dream_Machine_(text-to-video_model)" title="Dream Machine (text-to-video model)">Dream Machine</a></li>
<li><a href="/wiki/Runway_(company)#Services_and_technologies" title="Runway (company)">Runway Gen</a></li>
<li><a href="/wiki/MiniMax_(company)#Hailuo_AI" title="MiniMax (company)">Hailuo AI</a></li>
<li><a href="/wiki/Kling_AI" title="Kling AI">Kling</a></li>
<li><a href="/wiki/Sora_(text-to-video_model)" title="Sora (text-to-video model)">Sora</a></li>
<li><a href="/wiki/Seedance_2.0" title="Seedance 2.0">Seedance</a></li>
<li><a href="/wiki/Veo_(text-to-video_model)" title="Veo (text-to-video model)">Veo</a></li></ul></li>
<li><a href="/wiki/Music_and_artificial_intelligence" title="Music and artificial intelligence">Music generation</a>
<ul><li><a href="/wiki/Riffusion" title="Riffusion">Riffusion</a></li>
<li><a href="/wiki/Suno_AI" class="mw-redirect" title="Suno AI">Suno AI</a></li>
<li><a href="/wiki/Udio" title="Udio">Udio</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Text</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/GloVe" title="GloVe">GloVe</a></li>
<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/T5_(language_model)" title="T5 (language model)">T5</a></li>
<li><a href="/wiki/Llama_(language_model)" title="Llama (language model)">Llama</a></li>
<li><a href="/wiki/Chinchilla_(language_model)" title="Chinchilla (language model)">Chinchilla AI</a></li>
<li><a href="/wiki/PaLM" title="PaLM">PaLM</a></li>
<li><a href="/wiki/Generative_pre-trained_transformer" title="Generative pre-trained transformer">GPT</a>
<ul><li><a href="/wiki/GPT-1" title="GPT-1">1</a></li>
<li><a href="/wiki/GPT-2" title="GPT-2">2</a></li>
<li><a href="/wiki/GPT-3" title="GPT-3">3</a></li>
<li><a href="/wiki/GPT-J" title="GPT-J">J</a></li>
<li><a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a></li>
<li><a href="/wiki/GPT-4" title="GPT-4">4</a></li>
<li><a href="/wiki/GPT-4o" title="GPT-4o">4o</a></li>
<li><a href="/wiki/OpenAI_o1" title="OpenAI o1">o1</a></li>
<li><a href="/wiki/OpenAI_o3" title="OpenAI o3">o3</a></li>
<li><a href="/wiki/GPT-4.5" title="GPT-4.5">4.5</a></li>
<li><a href="/wiki/GPT-4.1" title="GPT-4.1">4.1</a></li>
<li><a href="/wiki/OpenAI_o4-mini" title="OpenAI o4-mini">o4-mini</a></li>
<li><a href="/wiki/GPT-5" title="GPT-5">5</a></li>
<li><a href="/wiki/GPT-5.1" title="GPT-5.1">5.1</a></li>
<li><a href="/wiki/GPT-5.2" title="GPT-5.2">5.2</a></li></ul></li>
<li><a href="/wiki/Claude_(language_model)" title="Claude (language model)">Claude</a></li>
<li><a href="/wiki/Gemini_(chatbot)" class="mw-redirect" title="Gemini (chatbot)">Gemini</a>
<ul><li><a href="/wiki/Gemini_(language_model)" title="Gemini (language model)">Gemini (language model)</a></li>
<li><a href="/wiki/Gemma_(language_model)" title="Gemma (language model)">Gemma</a></li></ul></li>
<li><a href="/wiki/Grok_(chatbot)" title="Grok (chatbot)">Grok</a></li>
<li><a href="/wiki/LaMDA" title="LaMDA">LaMDA</a></li>
<li><a href="/wiki/BLOOM_(language_model)" title="BLOOM (language model)">BLOOM</a></li>
<li><a href="/wiki/DBRX" title="DBRX">DBRX</a></li>
<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>
<li><a href="/wiki/IBM_Watson" title="IBM Watson">IBM Watson</a></li>
<li><a href="/wiki/IBM_Watsonx" title="IBM Watsonx">IBM Watsonx</a></li>
<li><a href="/wiki/IBM_Granite" title="IBM Granite">Granite</a></li>
<li><a href="/wiki/Huawei_PanGu" title="Huawei PanGu">PanGu-Σ</a></li>
<li><a href="/wiki/DeepSeek_(chatbot)" title="DeepSeek (chatbot)">DeepSeek</a></li>
<li><a href="/wiki/Qwen" title="Qwen">Qwen</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Decisional</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>
<li><a href="/wiki/AlphaZero" title="AlphaZero">AlphaZero</a></li>
<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li>
<li><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving car</a></li>
<li><a href="/wiki/MuZero" title="MuZero">MuZero</a></li>
<li><a href="/wiki/Action_selection" title="Action selection">Action selection</a>
<ul><li><a href="/wiki/AutoGPT" title="AutoGPT">AutoGPT</a></li></ul></li>
<li><a href="/wiki/Robot_control" title="Robot control">Robot control</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a></li>
<li><a href="/wiki/Warren_Sturgis_McCulloch" title="Warren Sturgis McCulloch">Warren Sturgis McCulloch</a></li>
<li><a href="/wiki/Walter_Pitts" title="Walter Pitts">Walter Pitts</a></li>
<li><a href="/wiki/John_von_Neumann" title="John von Neumann">John von Neumann</a></li>
<li><a href="/wiki/Christopher_D._Manning" title="Christopher D. Manning">Christopher D. Manning</a></li>
<li><a href="/wiki/Claude_Shannon" title="Claude Shannon">Claude Shannon</a></li>
<li><a href="/wiki/Shun%27ichi_Amari" title="Shun&#39;ichi Amari">Shun'ichi Amari</a></li>
<li><a href="/wiki/Kunihiko_Fukushima" title="Kunihiko Fukushima">Kunihiko Fukushima</a></li>
<li><a href="/wiki/Takeo_Kanade" title="Takeo Kanade">Takeo Kanade</a></li>
<li><a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a></li>
<li><a href="/wiki/John_McCarthy_(computer_scientist)" title="John McCarthy (computer scientist)">John McCarthy</a></li>
<li><a href="/wiki/Nathaniel_Rochester_(computer_scientist)" title="Nathaniel Rochester (computer scientist)">Nathaniel Rochester</a></li>
<li><a href="/wiki/Allen_Newell" title="Allen Newell">Allen Newell</a></li>
<li><a href="/wiki/Cliff_Shaw" title="Cliff Shaw">Cliff Shaw</a></li>
<li><a href="/wiki/Herbert_A._Simon" title="Herbert A. Simon">Herbert A. Simon</a></li>
<li><a href="/wiki/Oliver_Selfridge" title="Oliver Selfridge">Oliver Selfridge</a></li>
<li><a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a></li>
<li><a href="/wiki/Bernard_Widrow" title="Bernard Widrow">Bernard Widrow</a></li>
<li><a href="/wiki/Joseph_Weizenbaum" title="Joseph Weizenbaum">Joseph Weizenbaum</a></li>
<li><a href="/wiki/Seymour_Papert" title="Seymour Papert">Seymour Papert</a></li>
<li><a href="/wiki/Seppo_Linnainmaa" title="Seppo Linnainmaa">Seppo Linnainmaa</a></li>
<li><a href="/wiki/Paul_Werbos" title="Paul Werbos">Paul Werbos</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/John_Hopfield" title="John Hopfield">John Hopfield</a></li>
<li><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a></li>
<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>
<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Lotfi_A._Zadeh" title="Lotfi A. Zadeh">Lotfi A. Zadeh</a></li>
<li><a href="/wiki/Stephen_Grossberg" title="Stephen Grossberg">Stephen Grossberg</a></li>
<li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>
<li><a href="/wiki/James_Goodnight" title="James Goodnight">James Goodnight</a></li>
<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>
<li><a href="/wiki/Fei-Fei_Li" title="Fei-Fei Li">Fei-Fei Li</a></li>
<li><a href="/wiki/Alex_Krizhevsky" title="Alex Krizhevsky">Alex Krizhevsky</a></li>
<li><a href="/wiki/Ilya_Sutskever" title="Ilya Sutskever">Ilya Sutskever</a></li>
<li><a href="/wiki/Oriol_Vinyals" title="Oriol Vinyals">Oriol Vinyals</a></li>
<li><a href="/wiki/Quoc_V._Le" title="Quoc V. Le">Quoc V. Le</a></li>
<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>
<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>
<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li>
<li><a href="/wiki/Andrej_Karpathy" title="Andrej Karpathy">Andrej Karpathy</a></li>
<li><a href="/wiki/Ashish_Vaswani" title="Ashish Vaswani">Ashish Vaswani</a></li>
<li><a href="/wiki/Noam_Shazeer" title="Noam Shazeer">Noam Shazeer</a></li>
<li><a href="/wiki/Aidan_Gomez" title="Aidan Gomez">Aidan Gomez</a></li>
<li><a href="/wiki/John_Schulman" title="John Schulman">John Schulman</a></li>
<li><a href="/wiki/Mustafa_Suleyman" title="Mustafa Suleyman">Mustafa Suleyman</a></li>
<li><a href="/wiki/Jan_Leike" title="Jan Leike">Jan Leike</a></li>
<li><a href="/wiki/Daniel_Kokotajlo_(researcher)" title="Daniel Kokotajlo (researcher)">Daniel Kokotajlo</a></li>
<li><a href="/wiki/Fran%C3%A7ois_Chollet" title="François Chollet">François Chollet</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Architectures</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" class="mw-redirect" title="Transformer (deep learning architecture)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision transformer (ViT)</a></li></ul></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network (RNN)</a></li>
<li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory (LSTM)</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit (GRU)</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">Echo state network</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron (MLP)</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network (CNN)</a></li>
<li><a href="/wiki/Residual_neural_network" title="Residual neural network">Residual neural network (RNN)</a></li>
<li><a href="/wiki/Highway_network" title="Highway network">Highway network</a></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoder (VAE)</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">Generative adversarial network (GAN)</a></li>
<li><a href="/wiki/Graph_neural_network" title="Graph neural network">Graph neural network (GNN)</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Political</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AI_safety" title="AI safety">AI safety</a> (<a href="/wiki/AI_alignment" title="AI alignment">Alignment</a>)</li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of AI</a></li>
<li>EU <a href="/wiki/Artificial_Intelligence_Act" title="Artificial Intelligence Act">AI Act</a></li>
<li><a href="/wiki/Precautionary_principle" title="Precautionary principle">Precautionary principle</a></li>
<li><a href="/wiki/Regulation_of_artificial_intelligence" title="Regulation of artificial intelligence">Regulation of AI</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Social and economic</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AI_boom" title="AI boom">AI boom</a></li>
<li><a href="/wiki/AI_bubble" title="AI bubble">AI bubble</a></li>
<li><a href="/wiki/AI_literacy" title="AI literacy">AI literacy</a></li>
<li><a href="/wiki/AI_slop" title="AI slop">AI slop</a></li>
<li><a href="/wiki/AI_winter" title="AI winter">AI winter</a></li>
<li><a href="/wiki/AI_anthropomorphism" title="AI anthropomorphism">Anthropomorphism</a></li>
<li><a href="/wiki/Artificial_intelligence_in_architecture" title="Artificial intelligence in architecture">In architecture</a></li>
<li><a href="/wiki/Artificial_intelligence_in_education" title="Artificial intelligence in education">In education</a></li>
<li><a href="/wiki/Artificial_intelligence_in_healthcare" title="Artificial intelligence in healthcare">In healthcare</a>
<ul><li><a href="/wiki/Chatbot_psychosis" title="Chatbot psychosis">Chatbot psychosis</a></li>
<li><a href="/wiki/Artificial_intelligence_in_mental_health" title="Artificial intelligence in mental health">Mental health</a></li></ul></li>
<li><a href="/wiki/Artificial_intelligence_visual_art" title="Artificial intelligence visual art">In visual art</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><span class="noviewer" typeof="mw:File"><span title="Category"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/20px-Symbol_category_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/40px-Symbol_category_class.svg.png 1.5x" data-file-width="180" data-file-height="185" /></span></span> <a href="/wiki/Category:Artificial_intelligence" title="Category:Artificial intelligence">Category</a></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw‐api‐ext.eqiad.main‐7ddb8778f6‐bzpsd
Cached time: 20260224005300
Cache expiry: 83221
Reduced expiry: true
Complications: [vary‐revision‐sha1, prevent‐selective‐update, show‐toc]
CPU time usage: 0.759 seconds
Real time usage: 0.989 seconds
Preprocessor visited node count: 4584/1000000
Revision size: 40385/2097152 bytes
Post‐expand include size: 192135/2097152 bytes
Template argument size: 7374/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 225528/5000000 bytes
Lua time usage: 0.455/10.000 seconds
Lua memory usage: 6836842/52428800 bytes
Number of Wikibase entities loaded: 0/500
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  697.392      1 -total
 18.15%  126.586     10 Template:Cite_journal
 12.38%   86.333      1 Template:Machine_learning
 12.10%   84.366      1 Template:Sidebar_with_collapsible_lists
 11.16%   77.859     20 Template:Cite_conference
  9.97%   69.523      1 Template:Short_description
  7.21%   50.278      9 Template:Cite_arXiv
  7.18%   50.100      1 Template:Original_research
  6.53%   45.524      2 Template:Pagetype
  6.49%   45.262      1 Template:Ambox
-->

<!-- Saved in parser cache with key enwiki:pcache:66001552:|#|:idhash:canonical and timestamp 20260224005300 and revision id 1338022964. Rendering was triggered because: unknown
 -->
</div><noscript><img src="https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?useformat=desktop&amp;type=1x1&amp;usesul3=1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Attention_(machine_learning)&amp;oldid=1338022964">https://en.wikipedia.org/w/index.php?title=Attention_(machine_learning)&amp;oldid=1338022964</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw-interface=""><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Category</a>: <ul><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_matches_Wikidata" title="Category:Short description matches Wikidata">Short description matches Wikidata</a></li><li><a href="/wiki/Category:Articles_that_may_contain_original_research_from_June_2025" title="Category:Articles that may contain original research from June 2025">Articles that may contain original research from June 2025</a></li><li><a href="/wiki/Category:All_articles_that_may_contain_original_research" title="Category:All articles that may contain original research">All articles that may contain original research</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_June_2025" title="Category:Articles with unsourced statements from June 2025">Articles with unsourced statements from June 2025</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 12 February 2026, at 19:44<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a href="/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" title="Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License">Creative Commons Attribution-ShareAlike 4.0 License</a>;
additional terms may apply. By using this site, you agree to the <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use" class="extiw" title="foundation:Special:MyLanguage/Policy:Terms of Use">Terms of Use</a> and <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy" class="extiw" title="foundation:Special:MyLanguage/Policy:Privacy policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a rel="nofollow" class="external text" href="https://wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-legal-safety-contacts"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Legal:Wikimedia_Foundation_Legal_and_Safety_Contact_Information">Legal &amp; safety contacts</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
	<li id="footer-places-mobileview"><a href="//en.wikipedia.org/w/index.php?title=Attention_(machine_learning)&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://www.wikimedia.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/static/images/footer/wikimedia-button.svg" width="84" height="29"><img src="/static/images/footer/wikimedia.svg" width="25" height="25" alt="Wikimedia Foundation" lang="en" loading="lazy"></picture></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/w/resources/assets/poweredby_mediawiki.svg" width="88" height="31"><img src="/w/resources/assets/mediawiki_compact.svg" alt="Powered by MediaWiki" lang="en" width="25" height="25" loading="lazy"></picture></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class="vector-header-container vector-sticky-header-container no-font-mode-scale">
	<div id="vector-sticky-header" class="vector-sticky-header">
		<div class="vector-sticky-header-start">
			<div class="vector-sticky-header-icon-start vector-button-flush-left" aria-hidden="true">
				<button class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-sticky-header-search-toggle" tabindex="-1" data-event-name="ui.vector-sticky-search-form.icon"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
			</button>
		</div>
			
		<div role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box">
			<div class="vector-typeahead-search-container">
				<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail">
					<form action="/w/index.php" id="vector-sticky-search-form" class="cdx-search-input cdx-search-input--has-end-button">
						<div  class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
							<div class="cdx-text-input cdx-text-input--has-start-icon">
								<input
									class="cdx-text-input__input mw-searchInput" autocomplete="off"
									
									type="search" name="search" placeholder="Search Wikipedia">
								<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
							</div>
							<input type="hidden" name="title" value="Special:Search">
						</div>
						<button class="cdx-button cdx-search-input__end-button">Search</button>
					</form>
				</div>
			</div>
		</div>
		<div class="vector-sticky-header-context-bar">
				<nav aria-label="Contents" class="vector-toc-landmark">
						
					<div id="vector-sticky-header-toc" class="vector-dropdown mw-portlet mw-portlet-sticky-header-toc vector-sticky-header-toc vector-button-flush-left"  >
						<input type="checkbox" id="vector-sticky-header-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-sticky-header-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
						<label id="vector-sticky-header-toc-label" for="vector-sticky-header-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
						</label>
						<div class="vector-dropdown-content">
					
						<div id="vector-sticky-header-toc-unpinned-container" class="vector-unpinned-container">
						</div>
					
						</div>
					</div>
			</nav>
				<div class="vector-sticky-header-context-bar-primary" aria-hidden="true" ><span class="mw-page-title-main">Attention (machine learning)</span></div>
			</div>
		</div>
		<div class="vector-sticky-header-end" aria-hidden="true">
			<div class="vector-sticky-header-icons">
				<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-talk-sticky-header" tabindex="-1" data-event-name="talk-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbles mw-ui-icon-wikimedia-speechBubbles"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-subject-sticky-header" tabindex="-1" data-event-name="subject-sticky-header"><span class="vector-icon mw-ui-icon-article mw-ui-icon-wikimedia-article"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-history-sticky-header" tabindex="-1" data-event-name="history-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-history mw-ui-icon-wikimedia-wikimedia-history"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only mw-watchlink" id="ca-watchstar-sticky-header" tabindex="-1" data-event-name="watch-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-star mw-ui-icon-wikimedia-wikimedia-star"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-edit-sticky-header" tabindex="-1" data-event-name="wikitext-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-wikiText mw-ui-icon-wikimedia-wikimedia-wikiText"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-ve-edit-sticky-header" tabindex="-1" data-event-name="ve-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-edit mw-ui-icon-wikimedia-wikimedia-edit"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-viewsource-sticky-header" tabindex="-1" data-event-name="ve-edit-protected-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-editLock mw-ui-icon-wikimedia-wikimedia-editLock"></span>

<span></span>
			</a>
		</div>
			<div class="vector-sticky-header-buttons">
				<button class="cdx-button cdx-button--weight-quiet mw-interlanguage-selector" id="p-lang-btn-sticky-header" tabindex="-1" data-event-name="ui.dropdown-p-lang-btn-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-language mw-ui-icon-wikimedia-wikimedia-language"></span>

<span>14 languages</span>
			</button>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive" id="ca-addsection-sticky-header" tabindex="-1" data-event-name="addsection-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbleAdd-progressive mw-ui-icon-wikimedia-speechBubbleAdd-progressive"></span>

<span>Add topic</span>
			</a>
		</div>
			<div class="vector-sticky-header-icon-end">
				<div class="vector-user-links">
				</div>
			</div>
		</div>
	</div>
</div>
<div class="mw-portlet mw-portlet-dock-bottom emptyPortlet" id="p-dock-bottom">
	<ul>
		
	</ul>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw-web.eqiad.main-6f597bcf-68fxg","wgBackendResponseTime":191,"wgPageParseReport":{"limitreport":{"cputime":"0.759","walltime":"0.989","ppvisitednodes":{"value":4584,"limit":1000000},"revisionsize":{"value":40385,"limit":2097152},"postexpandincludesize":{"value":192135,"limit":2097152},"templateargumentsize":{"value":7374,"limit":2097152},"expansiondepth":{"value":16,"limit":100},"expensivefunctioncount":{"value":6,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":225528,"limit":5000000},"entityaccesscount":{"value":0,"limit":500},"timingprofile":["100.00%  697.392      1 -total"," 18.15%  126.586     10 Template:Cite_journal"," 12.38%   86.333      1 Template:Machine_learning"," 12.10%   84.366      1 Template:Sidebar_with_collapsible_lists"," 11.16%   77.859     20 Template:Cite_conference","  9.97%   69.523      1 Template:Short_description","  7.21%   50.278      9 Template:Cite_arXiv","  7.18%   50.100      1 Template:Original_research","  6.53%   45.524      2 Template:Pagetype","  6.49%   45.262      1 Template:Ambox"]},"scribunto":{"limitreport-timeusage":{"value":"0.455","limit":"10.000"},"limitreport-memusage":{"value":6836842,"limit":52428800}},"cachereport":{"origin":"mw-api-ext.eqiad.main-7ddb8778f6-bzpsd","timestamp":"20260224005300","ttl":83221,"transientcontent":true}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Attention (machine learning)","url":"https:\/\/en.wikipedia.org\/wiki\/Attention_(machine_learning)","sameAs":"http:\/\/www.wikidata.org\/entity\/Q103701642","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q103701642","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2020-12-03T09:15:34Z","dateModified":"2026-02-12T19:44:42Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/62\/Attention_mechanism_overview.svg","headline":"machine learning technique"}</script>
</body>
</html>