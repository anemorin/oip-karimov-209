<!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 skin-theme-clientpref-day vector-sticky-header-enabled wp25eastereggs-enable-clientpref-1 vector-toc-available" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>Backpropagation - Wikipedia</title>
<script>(function(){var className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 skin-theme-clientpref-day vector-sticky-header-enabled wp25eastereggs-enable-clientpref-1 vector-toc-available";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"677ec1e2-66c7-4113-97d8-67674ddb8dad","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Backpropagation","wgTitle":"Backpropagation","wgCurRevisionId":1328447875,"wgRevisionId":1328447875,"wgArticleId":1360091,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: ISBN date","CS1 Finnish-language sources (fi)","CS1: long volume value","Articles with short description","Short description matches Wikidata","Articles to be expanded from November 2019","All articles to be expanded","All articles with unsourced statements","Articles with unsourced statements from February 2022","Machine learning algorithms","Artificial neural networks"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Backpropagation","wgRelevantArticleId":1360091,"wgTempUserName":null,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgNoticeProject":"wikipedia","wgFlaggedRevsParams":{"tags":{"status":{"levels":1}}},"wgConfirmEditCaptchaNeededForGenericEdit":"hcaptcha","wgConfirmEditHCaptchaVisualEditorOnLoadIntegrationEnabled":false,"wgConfirmEditHCaptchaSiteKey":"5d0c670e-a5f4-4258-ad16-1f42792c9c62","wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":0,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":60000,"wgEditSubmitButtonLabelPublish":true,"wgVisualEditorPageIsDisambiguation":false,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":false,"wgVector2022LanguageInHeader":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q798503","wgCheckUserClientHintsHeadersJsApi":["brands","architecture","bitness","fullVersionList","mobile","model","platform","platformVersion"],"GEHomepageSuggestedEditsEnableTopics":true,"wgGESuggestedEditsTaskTypes":{"taskTypes":["copyedit","link-recommendation"],"unavailableTaskTypes":[]},"wgGETopicsMatchModeEnabled":false,"wgGELevelingUpEnabledForUser":false,"wgGEUseTestKitchenExtension":true,"wgMetricsPlatformUserExperiments":{"active_experiments":[],"overrides":[],"enrolled":[],"assigned":[],"subject_ids":[],"sampling_units":[],"coordinator":[]},"wgTestKitchenUserExperiments":{"active_experiments":[],"overrides":[],"enrolled":[],"assigned":[],"subject_ids":[],"sampling_units":[],"coordinator":[]}};
RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.wikimediamessages.styles":"ready","ext.cite.styles":"ready","ext.math.styles":"ready","skins.vector.search.codex.styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","jquery.makeCollapsible.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready","ext.wikimediaBadges":"ready","ext.wp25EasterEggs.styles":"ready"};RLPAGEMODULES=["ext.parsermigration.survey","ext.cite.ux-enhancements","ext.math.polyfills","mediawiki.page.media","ext.scribunto.logs","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.js","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips","ext.gadget.switcher","ext.urlShortener.toolbar","ext.centralauth.centralautologin","mmv.bootstrap","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.echo.centralauth","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.cx.uls.quick.actions","wikibase.client.vector-2022","wikibase.databox.fromWikidata","ext.checkUser.clientHints","ext.quicksurveys.init","ext.growthExperiments.SuggestedEditSession","ext.xLab","ext.testKitchen","ext.wp25EasterEggs"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cext.wikimediamessages.styles%7Cext.wp25EasterEggs.styles%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.46.0-wmf.16">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta name="viewport" content="width=1120">
<meta property="og:title" content="Backpropagation - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Backpropagation&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/rest.php/v1/search" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Backpropagation">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="auth.wikimedia.org">
</head>
<body class="skin--responsive skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Backpropagation rootpage-Backpropagation skin-vector-2022 action-view">
<div id="mw-aria-live-region" class="mw-aria-live-region" aria-live="polite"></div><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header no-font-mode-scale">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  title="Main menu" >
	<input type="checkbox" id="vector-main-menu-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-main-menu-dropdown" class="vector-dropdown-checkbox "  aria-label="Main menu"  >
	<label id="vector-main-menu-dropdown-label" for="vector-main-menu-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>

<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li>
		</ul>
		
	</div>
</div>

	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li><li id="n-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li>
		</ul>
		
	</div>
</div>

</div>

				</div>

	</div>
</div>

		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/enwiki-25.svg" alt="" aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container skin-invert">
		<img class="mw-logo-wordmark" alt="Wikipedia" src="/static/images/mobile/copyright/wikipedia-wordmark-en-25.svg" style="width: 8.75em; height: 1.375em;">
		<img class="mw-logo-tagline" alt="The Free Encyclopedia" src="/static/images/mobile/copyright/wikipedia-tagline-en-25.svg" width="140" height="11" style="width: 8.75em; height: 0.6875em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle" title="Search Wikipedia [f]" accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input mw-searchInput" autocomplete="off"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="none" spellcheck="false" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links vector-user-links-wide" aria-label="Personal tools">
	<div class="vector-user-links-main">
	
<div id="p-vector-user-menu-preferences" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-userpage" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	<nav class="vector-appearance-landmark" aria-label="Appearance">
		
<div id="vector-appearance-dropdown" class="vector-dropdown "  title="Change the appearance of the page&#039;s font size, width, and color" >
	<input type="checkbox" id="vector-appearance-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-appearance-dropdown" class="vector-dropdown-checkbox "  aria-label="Appearance"  >
	<label id="vector-appearance-dropdown-label" for="vector-appearance-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-appearance mw-ui-icon-wikimedia-appearance"></span>

<span class="vector-dropdown-label-text">Appearance</span>
	</label>
	<div class="vector-dropdown-content">


			<div id="vector-appearance-unpinned-container" class="vector-unpinned-container">
				
			</div>
		
	</div>
</div>

	</nav>
	
<div id="p-vector-user-menu-notifications" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="pt-sitesupport-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en" class=""><span>Donate</span></a>
</li>
<li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="/w/index.php?title=Special:CreateAccount&amp;returnto=Backpropagation" title="You are encouraged to create an account and log in; however, it is not mandatory" class=""><span>Create account</span></a>
</li>
<li id="pt-login-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="/w/index.php?title=Special:UserLogin&amp;returnto=Backpropagation" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o" class=""><span>Log in</span></a>
</li>

			
		</ul>
		
	</div>
</div>

	</div>
	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out user-links-collapsible-item"  title="Log in and more options" >
	<input type="checkbox" id="vector-user-links-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-user-links-dropdown" class="vector-dropdown-checkbox "  aria-label="Personal tools"  >
	<label id="vector-user-links-dropdown-label" for="vector-user-links-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>

<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-sitesupport" class="user-links-collapsible-item mw-list-item"><a href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en"><span>Donate</span></a></li><li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Backpropagation" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Backpropagation" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

	
	</div>
</div>

</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-sitenotice-container">
			<div id="siteNotice"><!-- CentralNotice --><div class="wp25eastereggs-sitenotice"><div class="wp25eastereggs-sitenotice-landmark"></div></div></div>
		</div>
		<div class="vector-column-start">
			<div class="vector-main-menu-container">
		<div id="mw-navigation">
			<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site">
				<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
				</div>
		</nav>
		</div>
	</div>
	<div class="vector-sticky-pinned-container">
				<nav id="mw-panel-toc" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark">
					<div id="vector-toc-pinned-container" class="vector-pinned-container">
					<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-Overview"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Overview">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">1</span>
				<span>Overview</span>
			</div>
		</a>
		
		<ul id="toc-Overview-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Matrix_multiplication"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Matrix_multiplication">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">2</span>
				<span>Matrix multiplication</span>
			</div>
		</a>
		
		<ul id="toc-Matrix_multiplication-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Adjoint_graph"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Adjoint_graph">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">3</span>
				<span>Adjoint graph</span>
			</div>
		</a>
		
		<ul id="toc-Adjoint_graph-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Intuition"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Intuition">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">4</span>
				<span>Intuition</span>
			</div>
		</a>
		
			<button aria-controls="toc-Intuition-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Intuition subsection</span>
			</button>
		
		<ul id="toc-Intuition-sublist" class="vector-toc-list">
			<li id="toc-Motivation"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Motivation">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.1</span>
					<span>Motivation</span>
				</div>
			</a>
			
			<ul id="toc-Motivation-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Learning_as_an_optimization_problem"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Learning_as_an_optimization_problem">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.2</span>
					<span>Learning as an optimization problem</span>
				</div>
			</a>
			
			<ul id="toc-Learning_as_an_optimization_problem-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Derivation"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Derivation">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">5</span>
				<span>Derivation</span>
			</div>
		</a>
		
			<button aria-controls="toc-Derivation-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Derivation subsection</span>
			</button>
		
		<ul id="toc-Derivation-sublist" class="vector-toc-list">
			<li id="toc-Finding_the_derivative_of_the_error"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Finding_the_derivative_of_the_error">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">5.1</span>
					<span>Finding the derivative of the error</span>
				</div>
			</a>
			
			<ul id="toc-Finding_the_derivative_of_the_error-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Second-order_gradient_descent"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Second-order_gradient_descent">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">6</span>
				<span>Second-order gradient descent</span>
			</div>
		</a>
		
		<ul id="toc-Second-order_gradient_descent-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Loss_function"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Loss_function">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">7</span>
				<span>Loss function</span>
			</div>
		</a>
		
			<button aria-controls="toc-Loss_function-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Loss function subsection</span>
			</button>
		
		<ul id="toc-Loss_function-sublist" class="vector-toc-list">
			<li id="toc-Assumptions"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Assumptions">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">7.1</span>
					<span>Assumptions</span>
				</div>
			</a>
			
			<ul id="toc-Assumptions-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Example_loss_function"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Example_loss_function">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">7.2</span>
					<span>Example loss function</span>
				</div>
			</a>
			
			<ul id="toc-Example_loss_function-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Limitations"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Limitations">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">8</span>
				<span>Limitations</span>
			</div>
		</a>
		
		<ul id="toc-Limitations-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-History"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#History">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">9</span>
				<span>History</span>
			</div>
		</a>
		
			<button aria-controls="toc-History-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle History subsection</span>
			</button>
		
		<ul id="toc-History-sublist" class="vector-toc-list">
			<li id="toc-Precursors"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Precursors">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">9.1</span>
					<span>Precursors</span>
				</div>
			</a>
			
			<ul id="toc-Precursors-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Modern_backpropagation"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Modern_backpropagation">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">9.2</span>
					<span>Modern backpropagation</span>
				</div>
			</a>
			
			<ul id="toc-Modern_backpropagation-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Early_successes"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Early_successes">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">9.3</span>
					<span>Early successes</span>
				</div>
			</a>
			
			<ul id="toc-Early_successes-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-After_backpropagation"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#After_backpropagation">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">9.4</span>
					<span>After backpropagation</span>
				</div>
			</a>
			
			<ul id="toc-After_backpropagation-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">10</span>
				<span>See also</span>
			</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Notes"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Notes">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">11</span>
				<span>Notes</span>
			</div>
		</a>
		
		<ul id="toc-Notes-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">12</span>
				<span>References</span>
			</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Further_reading"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Further_reading">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">13</span>
				<span>Further reading</span>
			</div>
		</a>
		
		<ul id="toc-Further_reading-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-External_links"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#External_links">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">14</span>
				<span>External links</span>
			</div>
		</a>
		
		<ul id="toc-External_links-sublist" class="vector-toc-list">
		</ul>
	</li>
</ul>
</div>

					</div>
		</nav>
			</div>
		</div>
		<div class="mw-content-container">
			<main id="content" class="mw-body">
				<header class="mw-body-header vector-page-titlebar no-font-mode-scale">
					<nav aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  title="Table of Contents" >
	<input type="checkbox" id="vector-page-titlebar-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-titlebar-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
	<label id="vector-page-titlebar-toc-label" for="vector-page-titlebar-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>

					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">Backpropagation</span></h1>
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox" id="p-lang-btn-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-lang-btn" class="vector-dropdown-checkbox mw-interlanguage-selector" aria-label="Go to an article in another language. Available in 24 languages"   >
	<label id="p-lang-btn-label" for="p-lang-btn-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-24" aria-hidden="true"  ><span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>

<span class="vector-dropdown-label-text">24 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list">
				
				<li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D8%A7%D9%86%D8%AA%D8%B4%D8%A7%D8%B1_%D8%AE%D9%84%D9%81%D9%8A" title="انتشار خلفي – Arabic" lang="ar" hreflang="ar" data-title="انتشار خلفي" data-language-autonym="العربية" data-language-local-name="Arabic" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Retropropagaci%C3%B3" title="Retropropagació – Catalan" lang="ca" hreflang="ca" data-title="Retropropagació" data-language-autonym="Català" data-language-local-name="Catalan" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-cs badge-Q70893996 mw-list-item" title=""><a href="https://cs.wikipedia.org/wiki/Algoritmus_zp%C4%9Btn%C3%A9ho_%C5%A1%C3%AD%C5%99en%C3%AD_chyby" title="Algoritmus zpětného šíření chyby – Czech" lang="cs" hreflang="cs" data-title="Algoritmus zpětného šíření chyby" data-language-autonym="Čeština" data-language-local-name="Czech" class="interlanguage-link-target"><span>Čeština</span></a></li><li class="interlanguage-link interwiki-de mw-list-item"><a href="https://de.wikipedia.org/wiki/Backpropagation" title="Backpropagation – German" lang="de" hreflang="de" data-title="Backpropagation" data-language-autonym="Deutsch" data-language-local-name="German" class="interlanguage-link-target"><span>Deutsch</span></a></li><li class="interlanguage-link interwiki-el mw-list-item"><a href="https://el.wikipedia.org/wiki/%CE%9F%CF%80%CE%B9%CF%83%CE%B8%CE%BF%CE%B4%CE%B9%CE%AC%CE%B4%CE%BF%CF%83%CE%B7" title="Οπισθοδιάδοση – Greek" lang="el" hreflang="el" data-title="Οπισθοδιάδοση" data-language-autonym="Ελληνικά" data-language-local-name="Greek" class="interlanguage-link-target"><span>Ελληνικά</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/Retropropagaci%C3%B3n" title="Retropropagación – Spanish" lang="es" hreflang="es" data-title="Retropropagación" data-language-autonym="Español" data-language-local-name="Spanish" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/%D9%BE%D8%B3%E2%80%8C%D8%A7%D9%86%D8%AA%D8%B4%D8%A7%D8%B1" title="پس‌انتشار – Persian" lang="fa" hreflang="fa" data-title="پس‌انتشار" data-language-autonym="فارسی" data-language-local-name="Persian" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/R%C3%A9tropropagation_du_gradient" title="Rétropropagation du gradient – French" lang="fr" hreflang="fr" data-title="Rétropropagation du gradient" data-language-autonym="Français" data-language-local-name="French" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-id mw-list-item"><a href="https://id.wikipedia.org/wiki/Algoritma_perambatan_mundur" title="Algoritma perambatan mundur – Indonesian" lang="id" hreflang="id" data-title="Algoritma perambatan mundur" data-language-autonym="Bahasa Indonesia" data-language-local-name="Indonesian" class="interlanguage-link-target"><span>Bahasa Indonesia</span></a></li><li class="interlanguage-link interwiki-it mw-list-item"><a href="https://it.wikipedia.org/wiki/Retropropagazione_dell%27errore" title="Retropropagazione dell&#039;errore – Italian" lang="it" hreflang="it" data-title="Retropropagazione dell&#039;errore" data-language-autonym="Italiano" data-language-local-name="Italian" class="interlanguage-link-target"><span>Italiano</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%83%E3%82%AF%E3%83%97%E3%83%AD%E3%83%91%E3%82%B2%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3" title="バックプロパゲーション – Japanese" lang="ja" hreflang="ja" data-title="バックプロパゲーション" data-language-autonym="日本語" data-language-local-name="Japanese" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-ka mw-list-item"><a href="https://ka.wikipedia.org/wiki/%E1%83%A3%E1%83%99%E1%83%A3%E1%83%9E%E1%83%A0%E1%83%9D%E1%83%9E%E1%83%90%E1%83%92%E1%83%90%E1%83%AA%E1%83%98%E1%83%A3%E1%83%9A%E1%83%98_%E1%83%9B%E1%83%9D%E1%83%93%E1%83%94%E1%83%9A%E1%83%98" title="უკუპროპაგაციული მოდელი – Georgian" lang="ka" hreflang="ka" data-title="უკუპროპაგაციული მოდელი" data-language-autonym="ქართული" data-language-local-name="Georgian" class="interlanguage-link-target"><span>ქართული</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/%EC%97%AD%EC%A0%84%ED%8C%8C" title="역전파 – Korean" lang="ko" hreflang="ko" data-title="역전파" data-language-autonym="한국어" data-language-local-name="Korean" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-pl mw-list-item"><a href="https://pl.wikipedia.org/wiki/Propagacja_wsteczna" title="Propagacja wsteczna – Polish" lang="pl" hreflang="pl" data-title="Propagacja wsteczna" data-language-autonym="Polski" data-language-local-name="Polish" class="interlanguage-link-target"><span>Polski</span></a></li><li class="interlanguage-link interwiki-ro mw-list-item"><a href="https://ro.wikipedia.org/wiki/Backpropagation" title="Backpropagation – Romanian" lang="ro" hreflang="ro" data-title="Backpropagation" data-language-autonym="Română" data-language-local-name="Romanian" class="interlanguage-link-target"><span>Română</span></a></li><li class="interlanguage-link interwiki-ru mw-list-item"><a href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B8" title="Метод обратного распространения ошибки – Russian" lang="ru" hreflang="ru" data-title="Метод обратного распространения ошибки" data-language-autonym="Русский" data-language-local-name="Russian" class="interlanguage-link-target"><span>Русский</span></a></li><li class="interlanguage-link interwiki-simple mw-list-item"><a href="https://simple.wikipedia.org/wiki/Backpropagation" title="Backpropagation – Simple English" lang="en-simple" hreflang="en-simple" data-title="Backpropagation" data-language-autonym="Simple English" data-language-local-name="Simple English" class="interlanguage-link-target"><span>Simple English</span></a></li><li class="interlanguage-link interwiki-sl mw-list-item"><a href="https://sl.wikipedia.org/wiki/Metoda_vzvratnega_raz%C5%A1irjanja" title="Metoda vzvratnega razširjanja – Slovenian" lang="sl" hreflang="sl" data-title="Metoda vzvratnega razširjanja" data-language-autonym="Slovenščina" data-language-local-name="Slovenian" class="interlanguage-link-target"><span>Slovenščina</span></a></li><li class="interlanguage-link interwiki-sr mw-list-item"><a href="https://sr.wikipedia.org/wiki/%D0%91%D0%B5%D0%BA%D0%BF%D1%80%D0%BE%D0%BF%D0%B0%D0%B3%D0%B0%D1%86%D0%B8%D1%98%D0%B0" title="Бекпропагација – Serbian" lang="sr" hreflang="sr" data-title="Бекпропагација" data-language-autonym="Српски / srpski" data-language-local-name="Serbian" class="interlanguage-link-target"><span>Српски / srpski</span></a></li><li class="interlanguage-link interwiki-th mw-list-item"><a href="https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%81%E0%B8%9E%E0%B8%A3%E0%B9%88%E0%B8%81%E0%B8%A3%E0%B8%B0%E0%B8%88%E0%B8%B2%E0%B8%A2%E0%B8%A2%E0%B9%89%E0%B8%AD%E0%B8%99%E0%B8%81%E0%B8%A5%E0%B8%B1%E0%B8%9A" title="การแพร่กระจายย้อนกลับ – Thai" lang="th" hreflang="th" data-title="การแพร่กระจายย้อนกลับ" data-language-autonym="ไทย" data-language-local-name="Thai" class="interlanguage-link-target"><span>ไทย</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B7%D0%B2%D0%BE%D1%80%D0%BE%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BF%D0%BE%D1%88%D0%B8%D1%80%D0%B5%D0%BD%D0%BD%D1%8F_%D0%BF%D0%BE%D0%BC%D0%B8%D0%BB%D0%BA%D0%B8" title="Метод зворотного поширення помилки – Ukrainian" lang="uk" hreflang="uk" data-title="Метод зворотного поширення помилки" data-language-autonym="Українська" data-language-local-name="Ukrainian" class="interlanguage-link-target"><span>Українська</span></a></li><li class="interlanguage-link interwiki-vi mw-list-item"><a href="https://vi.wikipedia.org/wiki/Truy%E1%BB%81n_ng%C6%B0%E1%BB%A3c" title="Truyền ngược – Vietnamese" lang="vi" hreflang="vi" data-title="Truyền ngược" data-language-autonym="Tiếng Việt" data-language-local-name="Vietnamese" class="interlanguage-link-target"><span>Tiếng Việt</span></a></li><li class="interlanguage-link interwiki-zh-yue mw-list-item"><a href="https://zh-yue.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E5%82%B3%E6%92%AD%E7%AE%97%E6%B3%95" title="反向傳播算法 – Cantonese" lang="yue" hreflang="yue" data-title="反向傳播算法" data-language-autonym="粵語" data-language-local-name="Cantonese" class="interlanguage-link-target"><span>粵語</span></a></li><li class="interlanguage-link interwiki-zh mw-list-item"><a href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95" title="反向传播算法 – Chinese" lang="zh" hreflang="zh" data-title="反向传播算法" data-language-autonym="中文" data-language-local-name="Chinese" class="interlanguage-link-target"><span>中文</span></a></li>
			</ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q798503#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
</header>
				<div class="vector-page-toolbar vector-feature-custom-font-size-clientpref--excluded">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Backpropagation" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="vector-tab-noicon mw-list-item"><a href="/wiki/Talk:Backpropagation" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

								
<div id="vector-variants-dropdown" class="vector-dropdown emptyPortlet"  >
	<input type="checkbox" id="vector-variants-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-variants-dropdown" class="vector-dropdown-checkbox " aria-label="Change language variant"   >
	<label id="vector-variants-dropdown-label" for="vector-variants-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">


					
<div id="p-variants" class="vector-menu mw-portlet mw-portlet-variants emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

				
	</div>
</div>

							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Backpropagation"><span>Read</span></a></li><li id="ca-edit" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Backpropagation&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Backpropagation&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="Page tools">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox" id="vector-page-tools-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-tools-dropdown" class="vector-dropdown-checkbox "  aria-label="Tools"  >
	<label id="vector-page-tools-dropdown-label" for="vector-page-tools-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/Backpropagation"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Backpropagation&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Backpropagation&amp;action=history"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Backpropagation" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Backpropagation" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Backpropagation&amp;oldid=1328447875" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Backpropagation&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Backpropagation&amp;id=1328447875&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-urlshortener" class="mw-list-item"><a href="/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FBackpropagation"><span>Get shortened URL</span></a></li><li id="t-urlshortener-qrcode" class="mw-list-item"><a href="/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FBackpropagation"><span>Download QR code</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Backpropagation&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Backpropagation&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-wikibase-otherprojects" class="vector-menu mw-portlet mw-portlet-wikibase-otherprojects"  >
	<div class="vector-menu-heading">
		In other projects
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li class="wb-otherproject-link wb-otherproject-commons mw-list-item"><a href="https://commons.wikimedia.org/wiki/Category:Backpropagation" hreflang="en"><span>Wikimedia Commons</span></a></li><li id="t-wikibase" class="wb-otherproject-link wb-otherproject-wikibase-dataitem mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q798503" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li>
		</ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>

							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end no-font-mode-scale">
					<div class="vector-sticky-pinned-container">
						<div class="wp25eastereggs-vector-sitenotice-landmark"></div>
						<nav class="vector-page-tools-landmark" aria-label="Page tools">
							<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
				
							</div>
		</nav>
						<nav class="vector-appearance-landmark" aria-label="Appearance">
							<div id="vector-appearance-pinned-container" class="vector-pinned-container">
				<div id="vector-appearance" class="vector-appearance vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-appearance-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="appearance-pinned"
	data-pinnable-element-id="vector-appearance"
	data-pinned-container-id="vector-appearance-pinned-container"
	data-unpinned-container-id="vector-appearance-unpinned-container"
>
	<div class="vector-pinnable-header-label">Appearance</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-appearance.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-appearance.unpin">hide</button>
</div>


</div>

							</div>
		</nav>
					</div>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content"><div class="mw-subjectpageheader">
</div><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Optimization algorithm for artificial neural networks</div>
<style data-mw-deduplicate="TemplateStyles:r1320445320">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}</style><div role="note" class="hatnote navigation-not-searchable">This article is about the computer algorithm. For the biological process, see <a href="/wiki/Neural_backpropagation" title="Neural backpropagation">Neural backpropagation</a>.</div>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1320445320" /><div role="note" class="hatnote navigation-not-searchable">Backpropagation can also refer to the way the result of a playout is propagated up the search tree in <a href="/wiki/Monte_Carlo_tree_search#Principle_of_operation" title="Monte Carlo tree search">Monte Carlo tree search</a>.</div>
<style data-mw-deduplicate="TemplateStyles:r1333133064">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:"\a0 · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1246091330">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}}</style><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886047488" /><table class="sidebar sidebar-collapse nomobile nowraplinks" role="navigation"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br />and <a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Paradigms</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" class="mw-redirect" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Meta-learning_(computer_science)" title="Meta-learning (computer science)">Meta-learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Batch_learning" class="mw-redirect" title="Batch learning">Batch learning</a></li>
<li><a href="/wiki/Curriculum_learning" title="Curriculum learning">Curriculum learning</a></li>
<li><a href="/wiki/Rule-based_machine_learning" title="Rule-based machine learning">Rule-based learning</a></li>
<li><a href="/wiki/Neuro-symbolic_AI" title="Neuro-symbolic AI">Neuro-symbolic AI</a></li>
<li><a href="/wiki/Neuromorphic_engineering" class="mw-redirect" title="Neuromorphic engineering">Neuromorphic engineering</a></li>
<li><a href="/wiki/Quantum_machine_learning" title="Quantum machine learning">Quantum machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Problems</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Generative_model" title="Generative model">Generative modeling</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></li>
<li><a href="/wiki/Density_estimation" title="Density estimation">Density estimation</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Data_cleaning" class="mw-redirect" title="Data cleaning">Data cleaning</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Semantic_analysis_(machine_learning)" title="Semantic analysis (machine learning)">Semantic analysis</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
<li><a href="/wiki/Ontology_learning" title="Ontology learning">Ontology learning</a></li>
<li><a href="/wiki/Multimodal_learning" title="Multimodal learning">Multimodal learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><div style="display: inline-block; line-height: 1.2em; padding: .1em 0;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><span class="nobold"><span style="font-size: 85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Apprenticeship_learning" title="Apprenticeship learning">Apprenticeship learning</a></li>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/CURE_algorithm" title="CURE algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Fuzzy_clustering" title="Fuzzy clustering">Fuzzy</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
<li><a href="/wiki/Sparse_dictionary_learning" title="Sparse dictionary learning">SDL</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Random_sample_consensus" title="Random sample consensus">RANSAC</a></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
<li><a href="/wiki/Isolation_forest" title="Isolation forest">Isolation forest</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">Neural networks</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">Feedforward neural network</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li>
<li><a href="/wiki/Reservoir_computing" title="Reservoir computing">reservoir computing</a></li></ul></li>
<li><a href="/wiki/Boltzmann_machine" title="Boltzmann machine">Boltzmann machine</a>
<ul><li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted</a></li></ul></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Diffusion_model" title="Diffusion model">Diffusion model</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li>
<li><a href="/wiki/LeNet" title="LeNet">LeNet</a></li>
<li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li></ul></li>
<li><a href="/wiki/Neural_field" title="Neural field">Neural field</a>
<ul><li><a href="/wiki/Neural_radiance_field" title="Neural radiance field">Neural radiance field</a></li>
<li><a href="/wiki/Physics-informed_neural_networks" title="Physics-informed neural networks">Physics-informed neural networks</a></li></ul></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" class="mw-redirect" title="Transformer (deep learning architecture)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision</a></li></ul></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Spiking_neural_network" title="Spiking neural network">Spiking neural network</a></li>
<li><a href="/wiki/Memtransistor" title="Memtransistor">Memtransistor</a></li>
<li><a href="/wiki/Electrochemical_RAM" title="Electrochemical RAM">Electrochemical RAM</a> (ECRAM)</li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/Policy_gradient_method" title="Policy gradient method">Policy gradient</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li>
<li><a href="/wiki/Multi-agent_reinforcement_learning" title="Multi-agent reinforcement learning">Multi-agent</a>
<ul><li><a href="/wiki/Self-play_(reinforcement_learning_technique)" class="mw-redirect" title="Self-play (reinforcement learning technique)">Self-play</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Learning with humans</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Active_learning_(machine_learning)" title="Active learning (machine learning)">Active learning</a></li>
<li><a href="/wiki/Crowdsourcing" title="Crowdsourcing">Crowdsourcing</a></li>
<li><a href="/wiki/Human-in-the-loop" title="Human-in-the-loop">Human-in-the-loop</a></li>
<li><a href="/wiki/Mechanistic_interpretability" title="Mechanistic interpretability">Mechanistic interpretability</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Model diagnostics</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Coefficient_of_determination" title="Coefficient of determination">Coefficient of determination</a></li>
<li><a href="/wiki/Confusion_matrix" title="Confusion matrix">Confusion matrix</a></li>
<li><a href="/wiki/Learning_curve_(machine_learning)" title="Learning curve (machine learning)">Learning curve</a></li>
<li><a href="/wiki/Receiver_operating_characteristic" title="Receiver operating characteristic">ROC curve</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Mathematical foundations</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Kernel_machines" class="mw-redirect" title="Kernel machines">Kernel machines</a></li>
<li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li>
<li><a href="/wiki/Topological_deep_learning" title="Topological deep learning">Topological deep learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Journals and conferences</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/AAAI_Conference_on_Artificial_Intelligence" title="AAAI Conference on Artificial Intelligence">AAAI</a></li>
<li><a href="/wiki/ECML_PKDD" title="ECML PKDD">ECML PKDD</a></li>
<li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/International_Conference_on_Learning_Representations" title="International Conference on Learning Representations">ICLR</a></li>
<li><a href="/wiki/International_Joint_Conference_on_Artificial_Intelligence" title="International Joint Conference on Artificial Intelligence">IJCAI</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;background-color:#e0e0e0;;color: var(--color-base)">Related articles</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li>
<li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a>
<ul><li><a href="/wiki/List_of_datasets_in_computer_vision_and_image_processing" title="List of datasets in computer vision and image processing">List of datasets in computer vision and image processing</a></li></ul></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-navbar"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333133064" /><style data-mw-deduplicate="TemplateStyles:r1239400231">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning" title="Template:Machine learning"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning" title="Template talk:Machine learning"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Machine_learning" title="Special:EditPage/Template:Machine learning"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p>In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b>backpropagation</b> is a <a href="/wiki/Gradient" title="Gradient">gradient</a> computation method commonly used for training a <a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">neural network</a> in computing parameter updates.
</p><p>It is an efficient application of the <a href="/wiki/Chain_rule" title="Chain rule">chain rule</a> to neural networks. Backpropagation computes the gradient of a <a href="/wiki/Loss_function" title="Loss function">loss function</a> with respect to the <a href="/wiki/Glossary_of_graph_theory_terms#weight" class="mw-redirect" title="Glossary of graph theory terms">weights</a> of the network for a single input–output example, and does so <a href="/wiki/Algorithmic_efficiency" title="Algorithmic efficiency">efficiently</a>, computing the gradient one layer at a time, <a href="/wiki/Iteration" title="Iteration">iterating</a> backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through <a href="/wiki/Dynamic_programming" title="Dynamic programming">dynamic programming</a>.<sup id="cite&#95;ref-kelley1960&#95;1-0" class="reference"><a href="#cite_note-kelley1960-1"><span class="cite-bracket">&#91;</span>1<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-bryson1961&#95;2-0" class="reference"><a href="#cite_note-bryson1961-2"><span class="cite-bracket">&#91;</span>2<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-FOOTNOTEGoodfellowBengioCourville2016&#91;httpswwwdeeplearningbookorgcontentsmlphtmlpf33&#95;214&#93;&#95;3-0" class="reference"><a href="#cite_note-FOOTNOTEGoodfellowBengioCourville2016[httpswwwdeeplearningbookorgcontentsmlphtmlpf33_214]-3"><span class="cite-bracket">&#91;</span>3<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Strictly speaking, the term <i>backpropagation</i> refers only to an algorithm for efficiently computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm. This includes changing model parameters in the negative direction of the gradient, such as by <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">stochastic gradient descent</a>, or as an intermediate step in a more complicated optimizer, such as <a href="/wiki/Stochastic_gradient_descent#Adam" title="Stochastic gradient descent">Adaptive Moment Estimation</a>.<sup id="cite&#95;ref-4" class="reference"><a href="#cite_note-4"><span class="cite-bracket">&#91;</span>4<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Backpropagation had multiple discoveries and partial discoveries, with a tangled history and terminology. See the <a href="#History">history</a> section for details. Some other names for the technique include "reverse mode of <a href="/wiki/Automatic_differentiation" title="Automatic differentiation">automatic differentiation</a>" or "<a href="/wiki/Reverse_accumulation" class="mw-redirect" title="Reverse accumulation">reverse accumulation</a>".<sup id="cite&#95;ref-DL-reverse-mode&#95;5-0" class="reference"><a href="#cite_note-DL-reverse-mode-5"><span class="cite-bracket">&#91;</span>5<span class="cite-bracket">&#93;</span></a></sup>
</p>
<meta property="mw:PageProp/toc" />
<div class="mw-heading mw-heading2"><h2 id="Overview">Overview</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=1" title="Edit section: Overview"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Backpropagation computes the gradient in <a href="/wiki/Parameter_space" title="Parameter space">weight space</a> of a feedforward neural network, with respect to a <a href="/wiki/Loss_function" title="Loss function">loss function</a>. Denote:
</p>
<ul><li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="{\displaystyle x}"></span>: input (vector of features)</li>
<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>y</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;" alt="{\displaystyle y}"></span>: target output
<dl><dd>For classification, output will be a vector of class probabilities (e.g., <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (0.1,0.7,0.2)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mn>0.1</mn>
        <mo>,</mo>
        <mn>0.7</mn>
        <mo>,</mo>
        <mn>0.2</mn>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (0.1,0.7,0.2)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fbb2f54392fb76b6dea26425e24447bf2c48cc94" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:12.792ex; height:2.843ex;" alt="{\displaystyle (0.1,0.7,0.2)}"></span>, and target output is a specific class, encoded by the <a href="/wiki/One-hot" title="One-hot">one-hot</a>/<a href="/wiki/Dummy_variable_(statistics)" title="Dummy variable (statistics)">dummy variable</a> (e.g., <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (0,1,0)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>0</mn>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (0,1,0)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3e28bfe8a44d06939011e3ba2159e894c0f22f23" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:7.365ex; height:2.843ex;" alt="{\displaystyle (0,1,0)}"></span>).</dd></dl></li>
<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fc55753007cd3c18576f7933f6f089196732029" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.766ex; height:2.176ex;" alt="{\displaystyle C}"></span>: <a href="/wiki/Loss_function" title="Loss function">loss function</a> or "cost function"<sup id="cite&#95;ref-6" class="reference"><a href="#cite_note-6"><span class="cite-bracket">&#91;</span>a<span class="cite-bracket">&#93;</span></a></sup>
<dl><dd>For classification, this is usually <a href="/wiki/Cross-entropy" title="Cross-entropy">cross-entropy</a> (XC, <a href="/wiki/Log_loss" class="mw-redirect" title="Log loss">log loss</a>), while for regression it is usually <a href="/wiki/Squared_error_loss" class="mw-redirect" title="Squared error loss">squared error loss</a> (SEL).</dd></dl></li>
<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle L}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>L</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/103168b86f781fe6e9a4a87b8ea1cebe0ad4ede8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.583ex; height:2.176ex;" alt="{\displaystyle L}"></span>: the number of layers</li>
<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W^{l}=(w_{jk}^{l})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mi>k</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W^{l}=(w_{jk}^{l})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6ef80d37cec97e026476841a0e20f9b35c402bd0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.338ex; width:11.568ex; height:3.676ex;" alt="{\displaystyle W^{l}=(w_{jk}^{l})}"></span>: the weights between layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l-1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l-1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4e0d448948a353d0b2469b88ca918f34e32c8752" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.505ex; width:4.696ex; height:2.343ex;" alt="{\displaystyle l-1}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>, where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{jk}^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mi>k</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{jk}^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8f47c5a00989f34da7a6cde720321223af36a019" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.338ex; width:3.43ex; height:3.509ex;" alt="{\displaystyle w_{jk}^{l}}"></span> is the weight between the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.211ex; height:2.176ex;" alt="{\displaystyle k}"></span>-th node in layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l-1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l-1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4e0d448948a353d0b2469b88ca918f34e32c8752" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.505ex; width:4.696ex; height:2.343ex;" alt="{\displaystyle l-1}"></span> and the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span>-th node in layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span><sup id="cite&#95;ref-7" class="reference"><a href="#cite_note-7"><span class="cite-bracket">&#91;</span>b<span class="cite-bracket">&#93;</span></a></sup></li>
<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle f^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ee0e6c373d37a6a029890630310cf35ba6acd174" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.043ex; height:3.009ex;" alt="{\displaystyle f^{l}}"></span>: <a href="/wiki/Activation_function" title="Activation function">activation functions</a> at layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>
<dl><dd>For classification the last layer is usually the <a href="/wiki/Logistic_function" title="Logistic function">logistic function</a> for binary classification, and <a href="/wiki/Softmax_function" title="Softmax function">softmax</a> (softargmax) for multi-class classification, while for the hidden layers this was traditionally a <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a> (logistic function or others) on each node (coordinate), but today is more varied, with <a href="/wiki/Rectifier_(neural_networks)" class="mw-redirect" title="Rectifier (neural networks)">rectifier</a> (<a href="/wiki/Ramp_function" title="Ramp function">ramp</a>, <a href="/wiki/ReLU" class="mw-redirect" title="ReLU">ReLU</a>) being common.</dd></dl></li>
<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle a_{j}^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle a_{j}^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fbb8152a0d8674023edebc1a8760c3ef478831d8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.338ex; width:2.14ex; height:3.509ex;" alt="{\displaystyle a_{j}^{l}}"></span>: activation of the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span>-th node in layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>.</li></ul>
<p>In the derivation of backpropagation, other intermediate quantities are used by introducing them as needed below. Bias terms are not treated specially since they correspond to a weight with a fixed input of 1. For backpropagation the specific loss function and activation functions do not matter as long as they and their derivatives can be evaluated efficiently. Traditional activation functions include sigmoid, <a href="/wiki/Tanh" class="mw-redirect" title="Tanh">tanh</a>, and <a href="/wiki/Rectifier_(neural_networks)" class="mw-redirect" title="Rectifier (neural networks)">ReLU</a>. <a href="/wiki/Swish_function" title="Swish function">Swish</a>,<sup id="cite&#95;ref-8" class="reference"><a href="#cite_note-8"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup> <a href="/wiki/Rectifier_(neural_networks)#Mish" class="mw-redirect" title="Rectifier (neural networks)">Mish</a>,<sup id="cite&#95;ref-9" class="reference"><a href="#cite_note-9"><span class="cite-bracket">&#91;</span>7<span class="cite-bracket">&#93;</span></a></sup> and many others.
</p><p>The overall network is a combination of <a href="/wiki/Function_composition" title="Function composition">function composition</a> and <a href="/wiki/Matrix_multiplication" title="Matrix multiplication">matrix multiplication</a>:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle g(x):=f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{1}(W^{1}x)\cdots ))}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>g</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>:=</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mo>&#x22EF;<!-- ⋯ --></mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msup>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>&#x22EF;<!-- ⋯ --></mo>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle g(x):=f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{1}(W^{1}x)\cdots ))}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/110b3c8ee790c91cf79a882f1515f6f05f8db335" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:44.566ex; height:3.176ex;" alt="{\displaystyle g(x):=f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{1}(W^{1}x)\cdots ))}"></span></dd></dl>
<p>For a training set there will be a set of input–output pairs, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \left\{(x_{i},y_{i})\right\}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow>
          <mo>{</mo>
          <mrow>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo>,</mo>
            <msub>
              <mi>y</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo stretchy="false">)</mo>
          </mrow>
          <mo>}</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \left\{(x_{i},y_{i})\right\}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/26d74b1e662ee0f0e1604f2e5a63fde1456bb19e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:9.236ex; height:2.843ex;" alt="{\displaystyle \left\{(x_{i},y_{i})\right\}}"></span>. For each input–output pair <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (x_{i},y_{i})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (x_{i},y_{i})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d6dbb919b91ccacf17ed47898048428a1baf9703" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:6.912ex; height:2.843ex;" alt="{\displaystyle (x_{i},y_{i})}"></span> in the training set, the loss of the model on that pair is the cost of the difference between the predicted output <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle g(x_{i})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>g</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle g(x_{i})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3badcf4ca3e9414821942baee2851b8f9bd27294" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:5.055ex; height:2.843ex;" alt="{\displaystyle g(x_{i})}"></span> and the target output <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y_{i}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/67d30d30b6c2dbe4d6f150d699de040937ecc95f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.939ex; height:2.009ex;" alt="{\displaystyle y_{i}}"></span>:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C(y_{i},g(x_{i}))}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mi>g</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C(y_{i},g(x_{i}))}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/57cd91c74155873ce3e1df0fab12d13e7a8f335f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:11.603ex; height:2.843ex;" alt="{\displaystyle C(y_{i},g(x_{i}))}"></span></dd></dl>
<p>Note the distinction: during model evaluation the weights are fixed while the inputs vary (and the target output may be unknown), and the network ends with the output layer (it does not include the loss function). During model training the input–output pair is fixed while the weights vary, and the network ends with the loss function.
</p><p>Backpropagation computes the gradient for a <i>fixed</i> input–output pair <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (x_{i},y_{i})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (x_{i},y_{i})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d6dbb919b91ccacf17ed47898048428a1baf9703" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:6.912ex; height:2.843ex;" alt="{\displaystyle (x_{i},y_{i})}"></span>, where the weights <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{jk}^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mi>k</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{jk}^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8f47c5a00989f34da7a6cde720321223af36a019" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.338ex; width:3.43ex; height:3.509ex;" alt="{\displaystyle w_{jk}^{l}}"></span> can vary. Each individual component of the gradient, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \partial C/\partial w_{jk}^{l},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
        <mi>C</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mi>k</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msubsup>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \partial C/\partial w_{jk}^{l},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/924218a6182ed0f43094ec9f5d8a66c4b9b9026f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.338ex; width:9.642ex; height:3.509ex;" alt="{\displaystyle \partial C/\partial w_{jk}^{l},}"></span> can be computed by the chain rule; but doing this separately for each weight is inefficient. Backpropagation efficiently computes the gradient by avoiding duplicate calculations and not computing unnecessary intermediate values, by computing the gradient of each layer – specifically the gradient of the weighted <i>input</i> of each layer, denoted by <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/249b787ef6af0149f7d71b951651c43633e0d56d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.676ex;" alt="{\displaystyle \delta ^{l}}"></span> – from back to front.
</p><p>Informally, the key point is that since the only way a weight in <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60b87bc8138a82f110491bb1abd0ea6eefe84c22" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:3.23ex; height:2.676ex;" alt="{\displaystyle W^{l}}"></span> affects the loss is through its effect on the <i>next</i> layer, and it does so <i>linearly</i>, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/249b787ef6af0149f7d71b951651c43633e0d56d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.676ex;" alt="{\displaystyle \delta ^{l}}"></span> are the only data you need to compute the gradients of the weights at layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>, and then the gradients of weights of previous layer can be computed by <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l-1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l-1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4ff3faf90011d3ed813a4ac2fe31904765e2963" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:3.877ex; height:2.676ex;" alt="{\displaystyle \delta ^{l-1}}"></span> and repeated recursively. This avoids inefficiency in two ways. First, it avoids duplication because when computing the gradient at layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>, it is unnecessary to recompute all derivatives on later layers <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l+1,l+2,\ldots }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
        <mo>+</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mi>l</mi>
        <mo>+</mo>
        <mn>2</mn>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l+1,l+2,\ldots }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/49f721be70a4213a6196f3d14f58eb5844eebb1a" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:14.183ex; height:2.509ex;" alt="{\displaystyle l+1,l+2,\ldots }"></span> each time. Second, it avoids unnecessary intermediate calculations, because at each stage it directly computes the gradient of the weights with respect to the ultimate output (the loss), rather than unnecessarily computing the derivatives of the values of hidden layers with respect to changes in weights <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \partial a_{j'}^{l'}/\partial w_{jk}^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
        <msubsup>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>j</mi>
              <mo>&#x2032;</mo>
            </msup>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>l</mi>
              <mo>&#x2032;</mo>
            </msup>
          </mrow>
        </msubsup>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mi>k</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \partial a_{j'}^{l'}/\partial w_{jk}^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/67e885d69c5968ab173f51222b89d0701b3459d6" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.505ex; width:9.9ex; height:4.009ex;" alt="{\displaystyle \partial a_{j&#039;}^{l&#039;}/\partial w_{jk}^{l}}"></span>.
</p><p>Backpropagation can be expressed for simple feedforward networks in terms of <a href="#Matrix_multiplication">matrix multiplication</a>, or more generally in terms of the <a href="#Adjoint_graph">adjoint graph</a>.
</p>
<div class="mw-heading mw-heading2"><h2 id="Matrix_multiplication">Matrix multiplication</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=2" title="Edit section: Matrix multiplication"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>For the basic case of a feedforward network, where nodes in each layer are connected only to nodes in the immediate next layer (without skipping any layers), and there is a loss function that computes a scalar loss for the final output, backpropagation can be understood simply by matrix multiplication.<sup id="cite&#95;ref-10" class="reference"><a href="#cite_note-10"><span class="cite-bracket">&#91;</span>c<span class="cite-bracket">&#93;</span></a></sup> Essentially, backpropagation evaluates the expression for the derivative of the cost function as a product of derivatives between each layer <i>from right to left</i> – "backwards" – with the gradient of the weights between each layer being a simple modification of the partial products (the "backwards propagated error").
</p><p>Given an input–output pair <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (x,y)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (x,y)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/41cf50e4a314ca8e2c30964baa8d26e5be7a9386" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:5.328ex; height:2.843ex;" alt="{\displaystyle (x,y)}"></span>, the loss is:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{2}(W^{2}f^{1}(W^{1}x))\cdots )))}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo>,</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mo>&#x22EF;<!-- ⋯ --></mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msup>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mo>&#x22EF;<!-- ⋯ --></mo>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{2}(W^{2}f^{1}(W^{1}x))\cdots )))}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ca8768a6ab49c9e52086741f958221b56b6d03e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:50.077ex; height:3.176ex;" alt="{\displaystyle C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{2}(W^{2}f^{1}(W^{1}x))\cdots )))}"></span></dd></dl>
<p>To compute this, one starts with the input <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="{\displaystyle x}"></span> and works forward; denote the weighted input of each hidden layer as <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle z^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>z</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle z^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ea4d56d1c5e8fe66b402ea0d1b92368902531dd" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.813ex; height:2.676ex;" alt="{\displaystyle z^{l}}"></span> and the output of hidden layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span> as the activation <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle a^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle a^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/975d9aa9ed78016e94e7438c5af2a6ec8b97a0e9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.952ex; height:2.676ex;" alt="{\displaystyle a^{l}}"></span>. For backpropagation, the activation <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle a^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle a^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/975d9aa9ed78016e94e7438c5af2a6ec8b97a0e9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.952ex; height:2.676ex;" alt="{\displaystyle a^{l}}"></span> as well as the derivatives <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (f^{l})'}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (f^{l})'}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/75a2f054b20672fb790ed501c4a39873233d4745" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:4.537ex; height:3.176ex;" alt="{\displaystyle (f^{l})&#039;}"></span> (evaluated at <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle z^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>z</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle z^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ea4d56d1c5e8fe66b402ea0d1b92368902531dd" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.813ex; height:2.676ex;" alt="{\displaystyle z^{l}}"></span>) must be cached for use during the backwards pass.
</p><p>The derivative of the loss in terms of the inputs is given by the chain rule; note that each term is a <a href="/wiki/Total_derivative" title="Total derivative">total derivative</a>, evaluated at the value of the network (at each node) on the input <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="{\displaystyle x}"></span>:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {dC}{da^{L}}}\cdot {\frac {da^{L}}{dz^{L}}}\cdot {\frac {dz^{L}}{da^{L-1}}}\cdot {\frac {da^{L-1}}{dz^{L-1}}}\cdot {\frac {dz^{L-1}}{da^{L-2}}}\cdot \ldots \cdot {\frac {da^{1}}{dz^{1}}}\cdot {\frac {\partial z^{1}}{\partial x}},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>d</mi>
              <mi>C</mi>
            </mrow>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                </mrow>
              </msup>
            </mrow>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>z</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>z</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                </mrow>
              </msup>
            </mrow>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                </mrow>
              </msup>
            </mrow>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>z</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>z</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                </mrow>
              </msup>
            </mrow>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>2</mn>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>1</mn>
                </mrow>
              </msup>
            </mrow>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>z</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>1</mn>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msup>
                <mi>z</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>1</mn>
                </mrow>
              </msup>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>x</mi>
            </mrow>
          </mfrac>
        </mrow>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {dC}{da^{L}}}\cdot {\frac {da^{L}}{dz^{L}}}\cdot {\frac {dz^{L}}{da^{L-1}}}\cdot {\frac {da^{L-1}}{dz^{L-1}}}\cdot {\frac {dz^{L-1}}{da^{L-2}}}\cdot \ldots \cdot {\frac {da^{1}}{dz^{1}}}\cdot {\frac {\partial z^{1}}{\partial x}},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b758730620b9e4d0b2018f99c3cc270fa8e3bd31" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.171ex; width:53.227ex; height:6.009ex;" alt="{\displaystyle {\frac {dC}{da^{L}}}\cdot {\frac {da^{L}}{dz^{L}}}\cdot {\frac {dz^{L}}{da^{L-1}}}\cdot {\frac {da^{L-1}}{dz^{L-1}}}\cdot {\frac {dz^{L-1}}{da^{L-2}}}\cdot \ldots \cdot {\frac {da^{1}}{dz^{1}}}\cdot {\frac {\partial z^{1}}{\partial x}},}"></span></dd></dl>
<p>where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {da^{L}}{dz^{L}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                </mrow>
              </msup>
            </mrow>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>z</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {da^{L}}{dz^{L}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e72728161114c81bdf209852a4157264296326fb" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.171ex; width:4.633ex; height:6.009ex;" alt="{\displaystyle {\frac {da^{L}}{dz^{L}}}}"></span> is a <a href="/wiki/Diagonal_matrix" title="Diagonal matrix">diagonal matrix</a>.
</p><p>These terms are: the derivative of the loss function;<sup id="cite&#95;ref-11" class="reference"><a href="#cite_note-11"><span class="cite-bracket">&#91;</span>d<span class="cite-bracket">&#93;</span></a></sup> the derivatives of the activation functions;<sup id="cite&#95;ref-12" class="reference"><a href="#cite_note-12"><span class="cite-bracket">&#91;</span>e<span class="cite-bracket">&#93;</span></a></sup> and the matrices of weights:<sup id="cite&#95;ref-13" class="reference"><a href="#cite_note-13"><span class="cite-bracket">&#91;</span>f<span class="cite-bracket">&#93;</span></a></sup>
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {dC}{da^{L}}}\circ (f^{L})'\cdot W^{L}\circ (f^{L-1})'\cdot W^{L-1}\circ \cdots \circ (f^{1})'\cdot W^{1}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>d</mi>
              <mi>C</mi>
            </mrow>
            <mrow>
              <mi>d</mi>
              <msup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>L</mi>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo>&#x22EF;<!-- ⋯ --></mo>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msup>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {dC}{da^{L}}}\circ (f^{L})'\cdot W^{L}\circ (f^{L-1})'\cdot W^{L-1}\circ \cdots \circ (f^{1})'\cdot W^{1}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f46ed48b7054ef8629f889782c3ffbf953818ca" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.171ex; width:52.502ex; height:5.676ex;" alt="{\displaystyle {\frac {dC}{da^{L}}}\circ (f^{L})&#039;\cdot W^{L}\circ (f^{L-1})&#039;\cdot W^{L-1}\circ \cdots \circ (f^{1})&#039;\cdot W^{1}.}"></span></dd></dl>
<p>The gradient <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \nabla }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3d0e93b78c50237f9ea83d027e4ebbdaef354b2" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.936ex; height:2.176ex;" alt="{\displaystyle \nabla }"></span> is the <a href="/wiki/Transpose" title="Transpose">transpose</a> of the derivative of the output in terms of the input, so the matrices are transposed and the order of multiplication is reversed, but the entries are the same:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \nabla _{x}C=(W^{1})^{T}\cdot (f^{1})'\circ \ldots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
          </mrow>
        </msub>
        <mi>C</mi>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <msub>
          <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>a</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>L</mi>
              </mrow>
            </msup>
          </mrow>
        </msub>
        <mi>C</mi>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla _{x}C=(W^{1})^{T}\cdot (f^{1})'\circ \ldots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6ef269aa176ce5488ce5f6038eebcb9c8fe3c728" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:71.315ex; height:3.176ex;" alt="{\displaystyle \nabla _{x}C=(W^{1})^{T}\cdot (f^{1})&#039;\circ \ldots \circ (W^{L-1})^{T}\cdot (f^{L-1})&#039;\circ (W^{L})^{T}\cdot (f^{L})&#039;\circ \nabla _{a^{L}}C.}"></span></dd></dl>
<p>Backpropagation then consists essentially of evaluating this expression from right to left (equivalently, multiplying the previous expression for the derivative from left to right), computing the gradient at each layer on the way; there is an added step, because the gradient of the weights is not just a subexpression: there's an extra multiplication.
</p><p>Introducing the auxiliary quantity <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/249b787ef6af0149f7d71b951651c43633e0d56d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.676ex;" alt="{\displaystyle \delta ^{l}}"></span> for the partial products (multiplying from right to left), interpreted as the "error at level <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>" and defined as the gradient of the input values at level <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l}:=(f^{l})'\circ (W^{l+1})^{T}\cdot (f^{l+1})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
        <mo>:=</mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo>&#x22EF;<!-- ⋯ --></mo>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>L</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <msub>
          <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>a</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>L</mi>
              </mrow>
            </msup>
          </mrow>
        </msub>
        <mi>C</mi>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l}:=(f^{l})'\circ (W^{l+1})^{T}\cdot (f^{l+1})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/85f95e97ceff3617728642e31c4090869970cad9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:79.132ex; height:3.176ex;" alt="{\displaystyle \delta ^{l}:=(f^{l})&#039;\circ (W^{l+1})^{T}\cdot (f^{l+1})&#039;\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})&#039;\circ (W^{L})^{T}\cdot (f^{L})&#039;\circ \nabla _{a^{L}}C.}"></span></dd></dl>
<p>Note that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/249b787ef6af0149f7d71b951651c43633e0d56d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.676ex;" alt="{\displaystyle \delta ^{l}}"></span> is a vector, of length equal to the number of nodes in level <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>; each component is interpreted as the "cost attributable to (the value of) that node".
</p><p>The gradient of the weights in layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span> is then:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \nabla _{W^{l}}C=\delta ^{l}(a^{l-1})^{T}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>W</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>l</mi>
              </mrow>
            </msup>
          </mrow>
        </msub>
        <mi>C</mi>
        <mo>=</mo>
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla _{W^{l}}C=\delta ^{l}(a^{l-1})^{T}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ef23e829ba87604d15ab01965708906c2baa1e4d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:19.043ex; height:3.343ex;" alt="{\displaystyle \nabla _{W^{l}}C=\delta ^{l}(a^{l-1})^{T}.}"></span></dd></dl>
<p>The factor of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle a^{l-1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle a^{l-1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5a66d0e6ecb4f486f7e1b63c6ae0a91448624163" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:4.053ex; height:2.676ex;" alt="{\displaystyle a^{l-1}}"></span> is because the weights <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60b87bc8138a82f110491bb1abd0ea6eefe84c22" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:3.23ex; height:2.676ex;" alt="{\displaystyle W^{l}}"></span> between level <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l-1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l-1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4e0d448948a353d0b2469b88ca918f34e32c8752" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.505ex; width:4.696ex; height:2.343ex;" alt="{\displaystyle l-1}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span> affect level <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span> proportionally to the inputs (activations): the inputs are fixed, the weights vary.
</p><p>The <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/249b787ef6af0149f7d71b951651c43633e0d56d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.676ex;" alt="{\displaystyle \delta ^{l}}"></span> can easily be computed recursively, going from right to left, as:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l-1}:=(f^{l-1})'\circ (W^{l})^{T}\cdot \delta ^{l}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mo>:=</mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l-1}:=(f^{l-1})'\circ (W^{l})^{T}\cdot \delta ^{l}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/16ccb3d3b717f3fa8e1aa72c3a46bbc70945e4cf" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:26.985ex; height:3.176ex;" alt="{\displaystyle \delta ^{l-1}:=(f^{l-1})&#039;\circ (W^{l})^{T}\cdot \delta ^{l}.}"></span></dd></dl>
<p>The gradients of the weights can thus be computed using a few matrix multiplications for each level; this is backpropagation.
</p><p>Compared with naively computing forwards (using the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/249b787ef6af0149f7d71b951651c43633e0d56d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.676ex;" alt="{\displaystyle \delta ^{l}}"></span> for illustration):
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{aligned}\delta ^{1}&amp;=(f^{1})'\circ (W^{2})^{T}\cdot (f^{2})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\\delta ^{2}&amp;=(f^{2})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\&amp;\vdots \\\delta ^{L-1}&amp;=(f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\\delta ^{L}&amp;=(f^{L})'\circ \nabla _{a^{L}}C,\end{aligned}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
            <mtr>
              <mtd>
                <msup>
                  <mi>&#x03B4;<!-- δ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msup>
              </mtd>
              <mtd>
                <mi></mi>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>T</mi>
                  </mrow>
                </msup>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <mo>&#x22EF;<!-- ⋯ --></mo>
                <mo>&#x2218;<!-- ∘ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                    <mo>&#x2212;<!-- − --></mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>T</mi>
                  </mrow>
                </msup>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                    <mo>&#x2212;<!-- − --></mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>T</mi>
                  </mrow>
                </msup>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <msub>
                  <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msup>
                      <mi>a</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>L</mi>
                      </mrow>
                    </msup>
                  </mrow>
                </msub>
                <mi>C</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msup>
                  <mi>&#x03B4;<!-- δ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msup>
              </mtd>
              <mtd>
                <mi></mi>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <mo>&#x22EF;<!-- ⋯ --></mo>
                <mo>&#x2218;<!-- ∘ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                    <mo>&#x2212;<!-- − --></mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>T</mi>
                  </mrow>
                </msup>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                    <mo>&#x2212;<!-- − --></mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>T</mi>
                  </mrow>
                </msup>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <msub>
                  <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msup>
                      <mi>a</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>L</mi>
                      </mrow>
                    </msup>
                  </mrow>
                </msub>
                <mi>C</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd />
              <mtd>
                <mi></mi>
                <mo>&#x22EE;<!-- ⋮ --></mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msup>
                  <mi>&#x03B4;<!-- δ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                    <mo>&#x2212;<!-- − --></mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
              </mtd>
              <mtd>
                <mi></mi>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                    <mo>&#x2212;<!-- − --></mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>W</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>T</mi>
                  </mrow>
                </msup>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <msub>
                  <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msup>
                      <mi>a</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>L</mi>
                      </mrow>
                    </msup>
                  </mrow>
                </msub>
                <mi>C</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msup>
                  <mi>&#x03B4;<!-- δ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                  </mrow>
                </msup>
              </mtd>
              <mtd>
                <mi></mi>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <msup>
                  <mi>f</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>L</mi>
                  </mrow>
                </msup>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>&#x2032;</mo>
                </msup>
                <mo>&#x2218;<!-- ∘ --></mo>
                <msub>
                  <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msup>
                      <mi>a</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>L</mi>
                      </mrow>
                    </msup>
                  </mrow>
                </msub>
                <mi>C</mi>
                <mo>,</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}\delta ^{1}&amp;=(f^{1})'\circ (W^{2})^{T}\cdot (f^{2})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\\delta ^{2}&amp;=(f^{2})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\&amp;\vdots \\\delta ^{L-1}&amp;=(f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\\delta ^{L}&amp;=(f^{L})'\circ \nabla _{a^{L}}C,\end{aligned}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65693d61321a41ee6a79f9e24e944f581a33240a" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -7.93ex; margin-bottom: -0.242ex; width:78.114ex; height:17.509ex;" alt="{\displaystyle {\begin{aligned}\delta ^{1}&amp;=(f^{1})&#039;\circ (W^{2})^{T}\cdot (f^{2})&#039;\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})&#039;\circ (W^{L})^{T}\cdot (f^{L})&#039;\circ \nabla _{a^{L}}C\\\delta ^{2}&amp;=(f^{2})&#039;\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})&#039;\circ (W^{L})^{T}\cdot (f^{L})&#039;\circ \nabla _{a^{L}}C\\&amp;\vdots \\\delta ^{L-1}&amp;=(f^{L-1})&#039;\circ (W^{L})^{T}\cdot (f^{L})&#039;\circ \nabla _{a^{L}}C\\\delta ^{L}&amp;=(f^{L})&#039;\circ \nabla _{a^{L}}C,\end{aligned}}}"></span></dd></dl>
<p>There are two key differences with backpropagation:
</p>
<ol><li>Computing <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l-1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l-1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4ff3faf90011d3ed813a4ac2fe31904765e2963" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:3.877ex; height:2.676ex;" alt="{\displaystyle \delta ^{l-1}}"></span> in terms of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/249b787ef6af0149f7d71b951651c43633e0d56d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.676ex;" alt="{\displaystyle \delta ^{l}}"></span> avoids the obvious duplicate multiplication of layers <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span> and beyond.</li>
<li>Multiplying starting from <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \nabla _{a^{L}}C}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>a</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>L</mi>
              </mrow>
            </msup>
          </mrow>
        </msub>
        <mi>C</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla _{a^{L}}C}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0a3491216364a8fe84026d9f114a9477be761c2c" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:5.877ex; height:2.676ex;" alt="{\displaystyle \nabla _{a^{L}}C}"></span> – propagating the error <i>backwards</i> – means that each step simply multiplies a vector (<span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta ^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta ^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/249b787ef6af0149f7d71b951651c43633e0d56d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.676ex;" alt="{\displaystyle \delta ^{l}}"></span>) by the matrices of weights <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (W^{l})^{T}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>T</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (W^{l})^{T}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/95ded7046a67e4ea5039c86405383ff82c74308e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:6.429ex; height:3.176ex;" alt="{\displaystyle (W^{l})^{T}}"></span> and derivatives of activations <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (f^{l-1})'}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (f^{l-1})'}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/89d6b56875a57b50369e097a55fe50f1be7b9aaf" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:6.638ex; height:3.176ex;" alt="{\displaystyle (f^{l-1})&#039;}"></span>. By contrast, multiplying forwards, starting from the changes at an earlier layer, means that each multiplication multiplies a <i>matrix</i> by a <i>matrix</i>. This is much more expensive, and corresponds to tracking every possible path of a change in one layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span> forward to changes in the layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l+2}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
        <mo>+</mo>
        <mn>2</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l+2}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f96df20468aecf3a1b6dfedd08982cb6edc489bb" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.505ex; width:4.696ex; height:2.343ex;" alt="{\displaystyle l+2}"></span> (for multiplying <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W^{l+1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W^{l+1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2d3f3b2ac57d2c95e81aafd230d2065e8f8cc401" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:5.331ex; height:2.676ex;" alt="{\displaystyle W^{l+1}}"></span> by <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W^{l+2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
            <mo>+</mo>
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W^{l+2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/74caba7e74227959403a15dfc8041720dae8817d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:5.331ex; height:2.676ex;" alt="{\displaystyle W^{l+2}}"></span>, with additional multiplications for the derivatives of the activations), which unnecessarily computes the intermediate quantities of how weight changes affect the values of hidden nodes.</li></ol>
<div class="mw-heading mw-heading2"><h2 id="Adjoint_graph">Adjoint graph</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=3" title="Edit section: Adjoint graph"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1305433154">.mw-parser-output .ambox{border:1px solid #a2a9b1;border-left:10px solid #36c;background-color:#fbfbfb;box-sizing:border-box}.mw-parser-output .ambox+link+.ambox,.mw-parser-output .ambox+link+style+.ambox,.mw-parser-output .ambox+link+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+style+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+link+.ambox{margin-top:-1px}html body.mediawiki .mw-parser-output .ambox.mbox-small-left{margin:4px 1em 4px 0;overflow:hidden;width:238px;border-collapse:collapse;font-size:88%;line-height:1.25em}.mw-parser-output .ambox-speedy{border-left:10px solid #b32424;background-color:#fee7e6}.mw-parser-output .ambox-delete{border-left:10px solid #b32424}.mw-parser-output .ambox-content{border-left:10px solid #f28500}.mw-parser-output .ambox-style{border-left:10px solid #fc3}.mw-parser-output .ambox-move{border-left:10px solid #9932cc}.mw-parser-output .ambox-protection{border-left:10px solid #a2a9b1}.mw-parser-output .ambox .mbox-text{border:none;padding:0.25em 0.5em;width:100%}.mw-parser-output .ambox .mbox-image{border:none;padding:2px 0 2px 0.5em;text-align:center}.mw-parser-output .ambox .mbox-imageright{border:none;padding:2px 0.5em 2px 0;text-align:center}.mw-parser-output .ambox .mbox-empty-cell{border:none;padding:0;width:1px}.mw-parser-output .ambox .mbox-image-div{width:52px}@media(min-width:720px){.mw-parser-output .ambox{margin:0 10%}}@media print{body.ns-0 .mw-parser-output .ambox{display:none!important}}</style><table class="box-Expand&#95;section plainlinks metadata ambox mbox-small-left ambox-content" role="presentation"><tbody><tr><td class="mbox-image"><span typeof="mw:File"><a href="/wiki/File:Wiki_letter_w_cropped.svg" class="mw-file-description"><img alt="[icon]" src="//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/20px-Wiki_letter_w_cropped.svg.png" decoding="async" width="20" height="14" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/40px-Wiki_letter_w_cropped.svg.png 1.5x" data-file-width="44" data-file-height="31" /></a></span></td><td class="mbox-text"><div class="mbox-text-span">This section <b>needs expansion</b>. You can help by <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Backpropagation&amp;action=edit&amp;section=">adding missing information</a>.  <span class="date-container"><i>(<span class="date">November 2019</span>)</i></span></div></td></tr></tbody></table>
<p>For more general graphs, and other advanced variations, backpropagation can be understood in terms of <a href="/wiki/Automatic_differentiation" title="Automatic differentiation">automatic differentiation</a>, where backpropagation is a special case of <a href="/wiki/Reverse_accumulation" class="mw-redirect" title="Reverse accumulation">reverse accumulation</a> (or "reverse mode").<sup id="cite&#95;ref-DL-reverse-mode&#95;5-1" class="reference"><a href="#cite_note-DL-reverse-mode-5"><span class="cite-bracket">&#91;</span>5<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Intuition">Intuition</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=4" title="Edit section: Intuition"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<div class="mw-heading mw-heading3"><h3 id="Motivation">Motivation</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=5" title="Edit section: Motivation"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The goal of any <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> algorithm is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.<sup id="cite&#95;ref-RumelhartHintonWilliams1986a&#95;14-0" class="reference"><a href="#cite_note-RumelhartHintonWilliams1986a-14"><span class="cite-bracket">&#91;</span>8<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Learning_as_an_optimization_problem">Learning as an optimization problem</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=6" title="Edit section: Learning as an optimization problem"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div><p>
To understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example. Consider a simple neural network with two input units, one output unit and no hidden units, and in which each neuron uses a <a href="/wiki/Artificial_neuron#Linear_combination" title="Artificial neuron">linear output</a> (unlike most work on neural networks, in which mapping from inputs to outputs is non-linear)<sup id="cite&#95;ref-15" class="reference"><a href="#cite_note-15"><span class="cite-bracket">&#91;</span>g<span class="cite-bracket">&#93;</span></a></sup> that is the weighted sum of its input. </p><figure typeof="mw:File/Thumb"><a href="/wiki/File:A_simple_neural_network_with_two_input_units_and_one_output_unit.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/4/42/A_simple_neural_network_with_two_input_units_and_one_output_unit.png/250px-A_simple_neural_network_with_two_input_units_and_one_output_unit.png" decoding="async" width="250" height="229" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/42/A_simple_neural_network_with_two_input_units_and_one_output_unit.png/500px-A_simple_neural_network_with_two_input_units_and_one_output_unit.png 1.5x" data-file-width="681" data-file-height="623" /></a><figcaption>A simple neural network with two input units (each with a single input) and one output unit (with two inputs)</figcaption></figure>
<p>Initially, before training, the weights will be set randomly. Then the neuron learns from <a href="/wiki/Training_set" class="mw-redirect" title="Training set">training examples</a>, which in this case consist of a set of <a href="/wiki/Tuple" title="Tuple">tuples</a> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (x_{1},x_{2},t)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (x_{1},x_{2},t)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dbc9205ba047983879ece2f78ff0cf1f7525bdd5" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:9.485ex; height:2.843ex;" alt="{\displaystyle (x_{1},x_{2},t)}"></span> where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a8788bf85d532fa88d1fb25eff6ae382a601c308" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.384ex; height:2.009ex;" alt="{\displaystyle x_{1}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d7af1b928f06e4c7e3e8ebfd60704656719bd766" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.384ex; height:2.009ex;" alt="{\displaystyle x_{2}}"></span> are the inputs to the network and <span class="texhtml mvar" style="font-style:italic;">t</span> is the correct output (the output the network should produce given those inputs, when it has been trained). The initial network, given <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a8788bf85d532fa88d1fb25eff6ae382a601c308" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.384ex; height:2.009ex;" alt="{\displaystyle x_{1}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d7af1b928f06e4c7e3e8ebfd60704656719bd766" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.384ex; height:2.009ex;" alt="{\displaystyle x_{2}}"></span>, will compute an output <span class="texhtml mvar" style="font-style:italic;">y</span> that likely differs from <span class="texhtml mvar" style="font-style:italic;">t</span> (given random weights). A <a href="/wiki/Loss_function" title="Loss function">loss function</a> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle L(t,y)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>L</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L(t,y)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a6d0d207e3b5787cffb4f86784345fe4393893fe" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:6.421ex; height:2.843ex;" alt="{\displaystyle L(t,y)}"></span> is used for measuring the discrepancy between the target output <span class="texhtml mvar" style="font-style:italic;">t</span> and the computed output <span class="texhtml mvar" style="font-style:italic;">y</span>. For <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a> problems the squared error can be used as a loss function, for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> the <a href="/wiki/Cross-entropy" title="Cross-entropy">categorical cross-entropy</a> can be used.
</p><p>As an example consider a regression problem using the square error as a loss:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle L(t,y)=(t-y)^{2}=E,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>L</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mi>y</mi>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>=</mo>
        <mi>E</mi>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L(t,y)=(t-y)^{2}=E,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ca7e3b55f443f8e490f1e6e552470f93802e5e43" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:22.74ex; height:3.176ex;" alt="{\displaystyle L(t,y)=(t-y)^{2}=E,}"></span></dd></dl>
<p>where <span class="texhtml mvar" style="font-style:italic;">E</span> is the discrepancy or error.
</p><p>
Consider the network on a single training case: <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (1,1,0)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>0</mn>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (1,1,0)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5568de79a98cdcb48821cd81b670c095fa4c8b6f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:7.365ex; height:2.843ex;" alt="{\displaystyle (1,1,0)}"></span>. Thus, the input <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a8788bf85d532fa88d1fb25eff6ae382a601c308" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.384ex; height:2.009ex;" alt="{\displaystyle x_{1}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d7af1b928f06e4c7e3e8ebfd60704656719bd766" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.384ex; height:2.009ex;" alt="{\displaystyle x_{2}}"></span> are 1 and 1 respectively and the correct output, <span class="texhtml mvar" style="font-style:italic;">t</span> is 0. Now if the relation is plotted between the network's output <span class="texhtml mvar" style="font-style:italic;">y</span> on the horizontal axis and the error <span class="texhtml mvar" style="font-style:italic;">E</span> on the vertical axis, the result is a parabola. The <a href="/wiki/Maxima_and_minima" class="mw-redirect" title="Maxima and minima">minimum</a> of the <a href="/wiki/Parabola" title="Parabola">parabola</a> corresponds to the output <span class="texhtml mvar" style="font-style:italic;">y</span> which minimizes the error <span class="texhtml mvar" style="font-style:italic;">E</span>. For a single training case, the minimum also touches the horizontal axis, which means the error will be zero and the network can produce an output <span class="texhtml mvar" style="font-style:italic;">y</span> that exactly matches the target output <span class="texhtml mvar" style="font-style:italic;">t</span>. Therefore, the problem of mapping inputs to outputs can be reduced to an <a href="/wiki/Optimization_problem" title="Optimization problem">optimization problem</a> of finding a function that will produce the minimal error. </p><figure class="mw-halign-right" typeof="mw:File/Thumb"><a href="/wiki/File:Error_surface_of_a_linear_neuron_for_a_single_training_case.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Error_surface_of_a_linear_neuron_for_a_single_training_case.png/250px-Error_surface_of_a_linear_neuron_for_a_single_training_case.png" decoding="async" width="250" height="199" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/f/f7/Error_surface_of_a_linear_neuron_for_a_single_training_case.png 1.5x" data-file-width="468" data-file-height="372" /></a><figcaption>Error surface of a linear neuron for a single training case</figcaption></figure>
<p>However, the output of a neuron depends on the weighted sum of all its inputs:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y=x_{1}w_{1}+x_{2}w_{2},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>y</mi>
        <mo>=</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>+</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y=x_{1}w_{1}+x_{2}w_{2},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6297e881012aa2988838c056f86c589828b80e7d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:17.946ex; height:2.343ex;" alt="{\displaystyle y=x_{1}w_{1}+x_{2}w_{2},}"></span></dd></dl>
<p>where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f6728d2b30f42f88b52281be5ae0584fdc9df64" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.718ex; height:2.009ex;" alt="{\displaystyle w_{1}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8998e0957bb573a19e7d9d934ced62ee68ab8fb8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.718ex; height:2.009ex;" alt="{\displaystyle w_{2}}"></span> are the weights on the connection from the input units to the output unit. Therefore, the error also depends on the incoming weights to the neuron, which is ultimately what needs to be changed in the network to enable learning.
</p><p>In this example, upon injecting the training data <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (1,1,0)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>0</mn>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (1,1,0)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5568de79a98cdcb48821cd81b670c095fa4c8b6f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:7.365ex; height:2.843ex;" alt="{\displaystyle (1,1,0)}"></span>, the loss function becomes
</p><p><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E=(t-y)^{2}=y^{2}=(x_{1}w_{1}+x_{2}w_{2})^{2}=(w_{1}+w_{2})^{2}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mi>y</mi>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>=</mo>
        <msup>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>+</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>+</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E=(t-y)^{2}=y^{2}=(x_{1}w_{1}+x_{2}w_{2})^{2}=(w_{1}+w_{2})^{2}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9642579185c542b0e8312a7edeeacf6abbb8934f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:51.779ex; height:3.176ex;" alt="{\displaystyle E=(t-y)^{2}=y^{2}=(x_{1}w_{1}+x_{2}w_{2})^{2}=(w_{1}+w_{2})^{2}.}"></span>
</p><p>Then, the loss function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.176ex;" alt="{\displaystyle E}"></span> takes the form of a parabolic cylinder with its base directed along <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{1}=-w_{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{1}=-w_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/839b25f7684ef2b1c6f960f962d64c0e993520b2" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:10.343ex; height:2.343ex;" alt="{\displaystyle w_{1}=-w_{2}}"></span>. Since all sets of weights that satisfy <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{1}=-w_{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{1}=-w_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/839b25f7684ef2b1c6f960f962d64c0e993520b2" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:10.343ex; height:2.343ex;" alt="{\displaystyle w_{1}=-w_{2}}"></span> minimize the loss function, in this case additional constraints are required to converge to a unique solution. Additional constraints could either be generated by setting specific conditions to the weights, or by injecting additional training data.
</p><p>One commonly used algorithm to find the set of weights that minimizes the error is <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a>. By backpropagation, the steepest descent direction is calculated of the loss function versus the present synaptic weights. Then, the weights can be modified along the steepest descent direction, and the error is minimized in an efficient way.
</p>
<div class="mw-heading mw-heading2"><h2 id="Derivation">Derivation</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=7" title="Edit section: Derivation"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The gradient descent method involves calculating the derivative of the loss function with respect to the weights of the network. This is normally done using backpropagation. Assuming one output neuron,<sup id="cite&#95;ref-16" class="reference"><a href="#cite_note-16"><span class="cite-bracket">&#91;</span>h<span class="cite-bracket">&#93;</span></a></sup> the squared error function is
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E=L(t,y)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
        <mo>=</mo>
        <mi>L</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E=L(t,y)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9db5e7ae55377b930b4bdaa8821d33fef2217792" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:11.295ex; height:2.843ex;" alt="{\displaystyle E=L(t,y)}"></span></dd></dl>
<p>where
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle L}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>L</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/103168b86f781fe6e9a4a87b8ea1cebe0ad4ede8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.583ex; height:2.176ex;" alt="{\displaystyle L}"></span> is the loss for the output <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>y</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;" alt="{\displaystyle y}"></span> and target value <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle t}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>t</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle t}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;" alt="{\displaystyle t}"></span>,</dd>
<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle t}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>t</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle t}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;" alt="{\displaystyle t}"></span> is the target output for a training sample, and</dd>
<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>y</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;" alt="{\displaystyle y}"></span> is the actual output of the output neuron.</dd></dl>
<p>In this section, the order of the weight indexes are reversed relative to the prior section: <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span> is weight from the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle i}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;" alt="{\displaystyle i}"></span>th to the
<span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span>th unit. <sup id="cite&#95;ref-17" class="reference"><a href="#cite_note-17"><span class="cite-bracket">&#91;</span>i<span class="cite-bracket">&#93;</span></a></sup> For each neuron <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span>, its output <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0229f31d42e856d5af6aa970042abed37bce87f9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:2.037ex; height:2.343ex;" alt="{\displaystyle o_{j}}"></span> is defined as
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{j}=\varphi ({\text{net}}_{j})=\varphi \left(\sum _{k=1}^{n}w_{kj}x_{k}\right),}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>&#x03C6;<!-- φ --></mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>net</mtext>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>&#x03C6;<!-- φ --></mi>
        <mrow>
          <mo>(</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>k</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>n</mi>
              </mrow>
            </munderover>
            <msub>
              <mi>w</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>k</mi>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>k</mi>
              </mrow>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{j}=\varphi ({\text{net}}_{j})=\varphi \left(\sum _{k=1}^{n}w_{kj}x_{k}\right),}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/538346d8b13891ef0ca6ae18e6e4ccd68964458a" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:31.916ex; height:7.509ex;" alt="{\displaystyle o_{j}=\varphi ({\text{net}}_{j})=\varphi \left(\sum _{k=1}^{n}w_{kj}x_{k}\right),}"></span></dd></dl>
<p>where the <a href="/wiki/Activation_function" title="Activation function">activation function</a> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \varphi }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C6;<!-- φ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \varphi }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/33ee699558d09cf9d653f6351f9fda0b2f4aaa3e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.52ex; height:2.176ex;" alt="{\displaystyle \varphi }"></span> is <a href="/wiki/Non-linear" class="mw-redirect" title="Non-linear">non-linear</a> and <a href="/wiki/Differentiable_function" title="Differentiable function">differentiable</a> over the activation region (the ReLU is not differentiable at one point). A historically used activation function is the <a href="/wiki/Logistic_function" title="Logistic function">logistic function</a>:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \varphi (z)={\frac {1}{1+e^{-z}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C6;<!-- φ --></mi>
        <mo stretchy="false">(</mo>
        <mi>z</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mrow>
              <mn>1</mn>
              <mo>+</mo>
              <msup>
                <mi>e</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>&#x2212;<!-- − --></mo>
                  <mi>z</mi>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \varphi (z)={\frac {1}{1+e^{-z}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3d4d1a31c11f8fc840738cf30326c08cfcdba81" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.171ex; width:15.719ex; height:5.509ex;" alt="{\displaystyle \varphi (z)={\frac {1}{1+e^{-z}}}}"></span></dd></dl>
<p>which has a <a href="/wiki/Logistic_function#Mathematical_properties" title="Logistic function">convenient</a> derivative of:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {d\varphi }{dz}}=\varphi (z)(1-\varphi (z))}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>d</mi>
              <mi>&#x03C6;<!-- φ --></mi>
            </mrow>
            <mrow>
              <mi>d</mi>
              <mi>z</mi>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mi>&#x03C6;<!-- φ --></mi>
        <mo stretchy="false">(</mo>
        <mi>z</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03C6;<!-- φ --></mi>
        <mo stretchy="false">(</mo>
        <mi>z</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {d\varphi }{dz}}=\varphi (z)(1-\varphi (z))}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/52ed6ebbb4540ca08b3be2f1dbddd08d43304a83" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.005ex; width:21.318ex; height:5.509ex;" alt="{\displaystyle {\frac {d\varphi }{dz}}=\varphi (z)(1-\varphi (z))}"></span></dd></dl>
<p>The input <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{net}}_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>net</mtext>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{net}}_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ac5fc6b518b9eb768d6dfd4946640ea28f217dd3" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:4.139ex; height:2.676ex;" alt="{\displaystyle {\text{net}}_{j}}"></span> to a neuron is the weighted sum of outputs <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{k}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{k}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1e57d19af3dbdeb11490339d1e35976411d5dc35" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.216ex; height:2.009ex;" alt="{\displaystyle o_{k}}"></span> of previous neurons. If the neuron is in the first layer after the input layer, the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{k}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{k}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1e57d19af3dbdeb11490339d1e35976411d5dc35" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.216ex; height:2.009ex;" alt="{\displaystyle o_{k}}"></span> of the input layer are simply the inputs <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{k}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{k}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6d2b88c64c76a03611549fb9b4cf4ed060b56002" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.418ex; height:2.009ex;" alt="{\displaystyle x_{k}}"></span> to the network. The number of input units to the neuron is <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;" alt="{\displaystyle n}"></span>. The variable <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{kj}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{kj}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f648fa9eb42b941e1320b741cbb73467a5bffcb3" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.43ex; height:2.343ex;" alt="{\displaystyle w_{kj}}"></span> denotes the weight between neuron <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.211ex; height:2.176ex;" alt="{\displaystyle k}"></span> of the previous layer and neuron <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span> of the current layer.
</p>
<div class="mw-heading mw-heading3"><h3 id="Finding_the_derivative_of_the_error">Finding the derivative of the error</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=8" title="Edit section: Finding the derivative of the error"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure typeof="mw:File/Thumb"><a href="/wiki/File:ArtificialNeuronModel_english.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/500px-ArtificialNeuronModel_english.png" decoding="async" width="400" height="190" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/960px-ArtificialNeuronModel_english.png 1.5x" data-file-width="1682" data-file-height="799" /></a><figcaption>Diagram of an artificial neural network to illustrate the notation used here</figcaption></figure>
<p>Calculating the <a href="/wiki/Partial_derivative" title="Partial derivative">partial derivative</a> of the error with respect to a weight <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span> is done using the <a href="/wiki/Chain_rule" title="Chain rule">chain rule</a> twice:
</p>
<style data-mw-deduplicate="TemplateStyles:r1266403038">.mw-parser-output table.numblk{border-collapse:collapse;border:none;margin-top:0;margin-right:0;margin-bottom:0}.mw-parser-output table.numblk>tbody>tr>td{vertical-align:middle;padding:0}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2){width:99%}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table{border-collapse:collapse;margin:0;border:none;width:100%}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td:first-child,.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td:last-child{padding:0 0.4ex}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td:nth-child(2){width:100%;padding:0}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:last-child>td{padding:0}.mw-parser-output table.numblk>tbody>tr>td:last-child{font-weight:bold}.mw-parser-output table.numblk.numblk-raw-n>tbody>tr>td:last-child{font-weight:unset}.mw-parser-output table.numblk>tbody>tr>td:last-child::before{content:"("}.mw-parser-output table.numblk>tbody>tr>td:last-child::after{content:")"}.mw-parser-output table.numblk.numblk-raw-n>tbody>tr>td:last-child::before,.mw-parser-output table.numblk.numblk-raw-n>tbody>tr>td:last-child::after{content:none}.mw-parser-output table.numblk>tbody>tr>td{border:none}.mw-parser-output table.numblk.numblk-border>tbody>tr>td{border:thin solid}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td{border:none}.mw-parser-output table.numblk.numblk-border>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td{border:thin solid}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:last-child>td{border-left:none;border-right:none;border-bottom:none}.mw-parser-output table.numblk.numblk-border>tbody>tr>td:nth-child(2)>table>tbody>tr:last-child>td{border-left:thin solid;border-right:thin solid;border-bottom:thin solid}.mw-parser-output table.numblk:target{color:var(--color-base,#202122);background-color:#cfe8fd}@media screen{html.skin-theme-clientpref-night .mw-parser-output table.numblk:target{color:var(--color-base,#eaecf0);background-color:#301702}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output table.numblk:target{color:var(--color-base,#eaecf0);background-color:#301702}}</style><table role="presentation" class="numblk" style="margin-left: 1.6em;"><tbody><tr><td class="nowrap"><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7c054e1332677921beedc7def4b32ceca971b44" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:37.758ex; height:6.509ex;" alt="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}}"></span></td> <td></td> <td class="nowrap"><span id="math&#95;Eq.&#95;1" class="reference nourlexpansion" style="font-weight:bold;">Eq. 1</span></td></tr></tbody></table>
<p>In the last factor of the right-hand side of the above, only one term in the sum <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\text{net}}_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>net</mtext>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\text{net}}_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ac5fc6b518b9eb768d6dfd4946640ea28f217dd3" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:4.139ex; height:2.676ex;" alt="{\displaystyle {\text{net}}_{j}}"></span> depends on <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span>, so that
</p>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1266403038" /><table role="presentation" class="numblk" style="margin-left: 1.6em;"><tbody><tr><td class="nowrap"><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}={\frac {\partial }{\partial w_{ij}}}\left(\sum _{k=1}^{n}w_{kj}o_{k}\right)={\frac {\partial }{\partial w_{ij}}}w_{ij}o_{i}=o_{i}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mrow>
          <mo>(</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>k</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>n</mi>
              </mrow>
            </munderover>
            <msub>
              <mi>w</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>k</mi>
                <mi>j</mi>
              </mrow>
            </msub>
            <msub>
              <mi>o</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>k</mi>
              </mrow>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}={\frac {\partial }{\partial w_{ij}}}\left(\sum _{k=1}^{n}w_{kj}o_{k}\right)={\frac {\partial }{\partial w_{ij}}}w_{ij}o_{i}=o_{i}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87fdc283f4d19b228713c6fdadf64deffd4ce295" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:47.28ex; height:7.509ex;" alt="{\displaystyle {\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}={\frac {\partial }{\partial w_{ij}}}\left(\sum _{k=1}^{n}w_{kj}o_{k}\right)={\frac {\partial }{\partial w_{ij}}}w_{ij}o_{i}=o_{i}.}"></span></td> <td></td> <td class="nowrap"><span id="math&#95;Eq.&#95;2" class="reference nourlexpansion" style="font-weight:bold;">Eq. 2</span></td></tr></tbody></table>
<p>If the neuron is in the first layer after the input layer, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{i}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c81b6f1251a0d2e12011a746b586c5fa9a132d48" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.927ex; height:2.009ex;" alt="{\displaystyle o_{i}}"></span> is just <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;" alt="{\displaystyle x_{i}}"></span>.
</p><p>The derivative of the output of neuron <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span> with respect to its input is simply the partial derivative of the activation function:
</p>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1266403038" /><table role="presentation" class="numblk" style="margin-left: 1.6em;"><tbody><tr><td class="nowrap"><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\frac {\partial \varphi ({\text{net}}_{j})}{\partial {\text{net}}_{j}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>&#x03C6;<!-- φ --></mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\frac {\partial \varphi ({\text{net}}_{j})}{\partial {\text{net}}_{j}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/73b9894f8477285b84c45f6a9715958c02160aee" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:19.015ex; height:6.509ex;" alt="{\displaystyle {\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\frac {\partial \varphi ({\text{net}}_{j})}{\partial {\text{net}}_{j}}}}"></span></td> <td></td> <td class="nowrap"><span id="math&#95;Eq.&#95;3" class="reference nourlexpansion" style="font-weight:bold;">Eq. 3</span></td></tr></tbody></table>
<p>which for the <a href="/wiki/Logistic_function" title="Logistic function">logistic activation function</a> 
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\frac {\partial }{\partial {\text{net}}_{j}}}\varphi ({\text{net}}_{j})=\varphi ({\text{net}}_{j})(1-\varphi ({\text{net}}_{j}))=o_{j}(1-o_{j})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mi>&#x03C6;<!-- φ --></mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>net</mtext>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>&#x03C6;<!-- φ --></mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>net</mtext>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03C6;<!-- φ --></mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>net</mtext>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\frac {\partial }{\partial {\text{net}}_{j}}}\varphi ({\text{net}}_{j})=\varphi ({\text{net}}_{j})(1-\varphi ({\text{net}}_{j}))=o_{j}(1-o_{j})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ed36b5ea1321f0aca92fcb00068a8d083ec006f1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:59.987ex; height:6.509ex;" alt="{\displaystyle {\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\frac {\partial }{\partial {\text{net}}_{j}}}\varphi ({\text{net}}_{j})=\varphi ({\text{net}}_{j})(1-\varphi ({\text{net}}_{j}))=o_{j}(1-o_{j})}"></span></dd></dl>
<p>This is the reason why backpropagation requires that the activation function be <a href="/wiki/Differentiable_function" title="Differentiable function">differentiable</a>. (Nevertheless, the <a href="/wiki/ReLU" class="mw-redirect" title="ReLU">ReLU</a> activation function, which is non-differentiable at 0, has become quite popular, e.g. in <a href="/wiki/AlexNet" title="AlexNet">AlexNet</a>)
</p><p>The first factor is straightforward to evaluate if the neuron is in the output layer, because then <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{j}=y}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>y</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{j}=y}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0543b379df7f7599d89d6317e7e11013941e2885" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:6.291ex; height:2.343ex;" alt="{\displaystyle o_{j}=y}"></span> and
</p>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1266403038" /><table role="presentation" class="numblk" style="margin-left: 1.6em;"><tbody><tr><td class="nowrap"><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial E}{\partial o_{j}}}={\frac {\partial E}{\partial y}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>y</mi>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial E}{\partial o_{j}}}={\frac {\partial E}{\partial y}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2d8fc0377bcf7268953a268d289a47b23e2dc37f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:11.22ex; height:6.176ex;" alt="{\displaystyle {\frac {\partial E}{\partial o_{j}}}={\frac {\partial E}{\partial y}}}"></span></td> <td></td> <td class="nowrap"><span id="math&#95;Eq.&#95;4" class="reference nourlexpansion" style="font-weight:bold;">Eq. 4</span></td></tr></tbody></table>
<p>If half of the square error is used as loss function we can rewrite it as
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial E}{\partial o_{j}}}={\frac {\partial E}{\partial y}}={\frac {\partial }{\partial y}}{\frac {1}{2}}(t-y)^{2}=y-t}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>y</mi>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>y</mi>
            </mrow>
          </mfrac>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mn>2</mn>
          </mfrac>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mi>y</mi>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>=</mo>
        <mi>y</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mi>t</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial E}{\partial o_{j}}}={\frac {\partial E}{\partial y}}={\frac {\partial }{\partial y}}{\frac {1}{2}}(t-y)^{2}=y-t}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/39d55b81fdb2681ab5957f3e03b9150b6d043c36" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:35.26ex; height:6.176ex;" alt="{\displaystyle {\frac {\partial E}{\partial o_{j}}}={\frac {\partial E}{\partial y}}={\frac {\partial }{\partial y}}{\frac {1}{2}}(t-y)^{2}=y-t}"></span></dd></dl>
<p>However, if <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span> is in an arbitrary inner layer of the network, finding the derivative <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.176ex;" alt="{\displaystyle E}"></span> with respect to <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0229f31d42e856d5af6aa970042abed37bce87f9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:2.037ex; height:2.343ex;" alt="{\displaystyle o_{j}}"></span> is less obvious.
</p><p>Considering <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.176ex;" alt="{\displaystyle E}"></span> as a function with the inputs being all neurons <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle L=\{u,v,\dots ,w\}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>L</mi>
        <mo>=</mo>
        <mo fence="false" stretchy="false">{</mo>
        <mi>u</mi>
        <mo>,</mo>
        <mi>v</mi>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <mi>w</mi>
        <mo fence="false" stretchy="false">}</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L=\{u,v,\dots ,w\}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c4abf35b7ef5693f8765266665cb87e9819cb2a7" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:17.34ex; height:2.843ex;" alt="{\displaystyle L=\{u,v,\dots ,w\}}"></span> receiving input from neuron <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span>,
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial E(o_{j})}{\partial o_{j}}}={\frac {\partial E(\mathrm {net} _{u},{\text{net}}_{v},\dots ,\mathrm {net} _{w})}{\partial o_{j}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="normal">n</mi>
                  <mi mathvariant="normal">e</mi>
                  <mi mathvariant="normal">t</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>u</mi>
                </mrow>
              </msub>
              <mo>,</mo>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>v</mi>
                </mrow>
              </msub>
              <mo>,</mo>
              <mo>&#x2026;<!-- … --></mo>
              <mo>,</mo>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="normal">n</mi>
                  <mi mathvariant="normal">e</mi>
                  <mi mathvariant="normal">t</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>w</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial E(o_{j})}{\partial o_{j}}}={\frac {\partial E(\mathrm {net} _{u},{\text{net}}_{v},\dots ,\mathrm {net} _{w})}{\partial o_{j}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e660d78517bc99de7a438a69171dcdb7dd88740b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:36.126ex; height:6.509ex;" alt="{\displaystyle {\frac {\partial E(o_{j})}{\partial o_{j}}}={\frac {\partial E(\mathrm {net} _{u},{\text{net}}_{v},\dots ,\mathrm {net} _{w})}{\partial o_{j}}}}"></span></dd></dl>
<p>and taking the <a href="/wiki/Total_derivative" title="Total derivative">total derivative</a> with respect to <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0229f31d42e856d5af6aa970042abed37bce87f9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:2.037ex; height:2.343ex;" alt="{\displaystyle o_{j}}"></span>, a recursive expression for the derivative is obtained:
</p>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1266403038" /><table role="presentation" class="numblk" style="margin-left: 1.6em;"><tbody><tr><td class="nowrap"><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial E}{\partial o_{j}}}=\sum _{\ell \in L}\left({\frac {\partial E}{\partial {\text{net}}_{\ell }}}{\frac {\partial {\text{net}}_{\ell }}{\partial o_{j}}}\right)=\sum _{\ell \in L}\left({\frac {\partial E}{\partial o_{\ell }}}{\frac {\partial o_{\ell }}{\partial {\text{net}}_{\ell }}}{\frac {\partial {\text{net}}_{\ell }}{\partial o_{j}}}\right)=\sum _{\ell \in L}\left({\frac {\partial E}{\partial o_{\ell }}}{\frac {\partial o_{\ell }}{\partial {\text{net}}_{\ell }}}w_{j\ell }\right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x2113;<!-- ℓ --></mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>L</mi>
          </mrow>
        </munder>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <mi>E</mi>
                </mrow>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mtext>net</mtext>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mtext>net</mtext>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                </mrow>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x2113;<!-- ℓ --></mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>L</mi>
          </mrow>
        </munder>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <mi>E</mi>
                </mrow>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                </mrow>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mtext>net</mtext>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mtext>net</mtext>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                </mrow>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x2113;<!-- ℓ --></mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>L</mi>
          </mrow>
        </munder>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <mi>E</mi>
                </mrow>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                </mrow>
                <mrow>
                  <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mtext>net</mtext>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
            <msub>
              <mi>w</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
                <mi>&#x2113;<!-- ℓ --></mi>
              </mrow>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial E}{\partial o_{j}}}=\sum _{\ell \in L}\left({\frac {\partial E}{\partial {\text{net}}_{\ell }}}{\frac {\partial {\text{net}}_{\ell }}{\partial o_{j}}}\right)=\sum _{\ell \in L}\left({\frac {\partial E}{\partial o_{\ell }}}{\frac {\partial o_{\ell }}{\partial {\text{net}}_{\ell }}}{\frac {\partial {\text{net}}_{\ell }}{\partial o_{j}}}\right)=\sum _{\ell \in L}\left({\frac {\partial E}{\partial o_{\ell }}}{\frac {\partial o_{\ell }}{\partial {\text{net}}_{\ell }}}w_{j\ell }\right)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c9887a99d44591e541893407e952ea0d50ed798e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:78.144ex; height:6.843ex;" alt="{\displaystyle {\frac {\partial E}{\partial o_{j}}}=\sum _{\ell \in L}\left({\frac {\partial E}{\partial {\text{net}}_{\ell }}}{\frac {\partial {\text{net}}_{\ell }}{\partial o_{j}}}\right)=\sum _{\ell \in L}\left({\frac {\partial E}{\partial o_{\ell }}}{\frac {\partial o_{\ell }}{\partial {\text{net}}_{\ell }}}{\frac {\partial {\text{net}}_{\ell }}{\partial o_{j}}}\right)=\sum _{\ell \in L}\left({\frac {\partial E}{\partial o_{\ell }}}{\frac {\partial o_{\ell }}{\partial {\text{net}}_{\ell }}}w_{j\ell }\right)}"></span></td> <td></td> <td class="nowrap"><span id="math&#95;Eq.&#95;5" class="reference nourlexpansion" style="font-weight:bold;">Eq. 5</span></td></tr></tbody></table>
<p>Therefore, the derivative with respect to <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0229f31d42e856d5af6aa970042abed37bce87f9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:2.037ex; height:2.343ex;" alt="{\displaystyle o_{j}}"></span> can be calculated if all the derivatives with respect to the outputs <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle o_{\ell }}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x2113;<!-- ℓ --></mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle o_{\ell }}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c2901ae7228e6fb253eb223344d54c70ccccaf00" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.046ex; height:2.009ex;" alt="{\displaystyle o_{\ell }}"></span> of the next layer – the ones closer to the output neuron – are known. [Note, if any of the neurons in set <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle L}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>L</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/103168b86f781fe6e9a4a87b8ea1cebe0ad4ede8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.583ex; height:2.176ex;" alt="{\displaystyle L}"></span> were not connected to neuron <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span>, they would be independent of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span> and the corresponding partial derivative under the summation would vanish to 0.]
</p><p>Substituting <b><a href="#math_Eq._2">Eq. 2</a></b>, <b><a href="#math_Eq._3">Eq. 3</a></b> <b><a href="#math_Eq.4">Eq.4</a></b> and <b><a href="#math_Eq._5">Eq. 5</a></b> in <b><a href="#math_Eq._1">Eq. 1</a></b> we obtain:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}o_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}o_{i}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ae55d1c0135131a9b5c0db89f5e20ffae8bc7040" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:40.683ex; height:6.509ex;" alt="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}o_{i}}"></span></dd>
<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}=o_{i}\delta _{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <msub>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial E}{\partial w_{ij}}}=o_{i}\delta _{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e86f74f9e33c38bc1d5664ddd9794da381976af3" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:12.263ex; height:6.176ex;" alt="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}=o_{i}\delta _{j}}"></span></dd></dl>
<p>with
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}{\frac {\partial L(t,o_{j})}{\partial o_{j}}}{\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&amp;{\text{if }}j{\text{ is an output neuron,}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell }){\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&amp;{\text{if }}j{\text{ is an inner neuron.}}\end{cases}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>{</mo>
            <mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false">
              <mtr>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mfrac>
                      <mrow>
                        <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                        <mi>L</mi>
                        <mo stretchy="false">(</mo>
                        <mi>t</mi>
                        <mo>,</mo>
                        <msub>
                          <mi>o</mi>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mi>j</mi>
                          </mrow>
                        </msub>
                        <mo stretchy="false">)</mo>
                      </mrow>
                      <mrow>
                        <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                        <msub>
                          <mi>o</mi>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mi>j</mi>
                          </mrow>
                        </msub>
                      </mrow>
                    </mfrac>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mfrac>
                      <mrow>
                        <mi>d</mi>
                        <mi>&#x03C6;<!-- φ --></mi>
                        <mo stretchy="false">(</mo>
                        <msub>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mtext>net</mtext>
                          </mrow>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mi>j</mi>
                          </mrow>
                        </msub>
                        <mo stretchy="false">)</mo>
                      </mrow>
                      <mrow>
                        <mi>d</mi>
                        <msub>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mtext>net</mtext>
                          </mrow>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mi>j</mi>
                          </mrow>
                        </msub>
                      </mrow>
                    </mfrac>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>if&#xA0;</mtext>
                  </mrow>
                  <mi>j</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>&#xA0;is an output neuron,</mtext>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo stretchy="false">(</mo>
                  <munder>
                    <mo>&#x2211;<!-- ∑ --></mo>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                      <mo>&#x2208;<!-- ∈ --></mo>
                      <mi>L</mi>
                    </mrow>
                  </munder>
                  <msub>
                    <mi>w</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>&#x03B4;<!-- δ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mfrac>
                      <mrow>
                        <mi>d</mi>
                        <mi>&#x03C6;<!-- φ --></mi>
                        <mo stretchy="false">(</mo>
                        <msub>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mtext>net</mtext>
                          </mrow>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mi>j</mi>
                          </mrow>
                        </msub>
                        <mo stretchy="false">)</mo>
                      </mrow>
                      <mrow>
                        <mi>d</mi>
                        <msub>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mtext>net</mtext>
                          </mrow>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mi>j</mi>
                          </mrow>
                        </msub>
                      </mrow>
                    </mfrac>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>if&#xA0;</mtext>
                  </mrow>
                  <mi>j</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>&#xA0;is an inner neuron.</mtext>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
            <mo fence="true" stretchy="true" symmetric="true"></mo>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}{\frac {\partial L(t,o_{j})}{\partial o_{j}}}{\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&amp;{\text{if }}j{\text{ is an output neuron,}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell }){\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&amp;{\text{if }}j{\text{ is an inner neuron.}}\end{cases}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54de826820eed0648c7dba5e345120bb23483261" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -4.338ex; width:67.524ex; height:9.843ex;" alt="{\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}{\frac {\partial L(t,o_{j})}{\partial o_{j}}}{\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&amp;{\text{if }}j{\text{ is an output neuron,}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell }){\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&amp;{\text{if }}j{\text{ is an inner neuron.}}\end{cases}}}"></span></dd></dl>
<p>if <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \varphi }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C6;<!-- φ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \varphi }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/33ee699558d09cf9d653f6351f9fda0b2f4aaa3e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.52ex; height:2.176ex;" alt="{\displaystyle \varphi }"></span> is the logistic function, and the error is the square error:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}(o_{j}-t_{j})o_{j}(1-o_{j})&amp;{\text{if }}j{\text{ is an output neuron,}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell })o_{j}(1-o_{j})&amp;{\text{if }}j{\text{ is an inner neuron.}}\end{cases}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>o</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>net</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>{</mo>
            <mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false">
              <mtr>
                <mtd>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                  <mo>&#x2212;<!-- − --></mo>
                  <msub>
                    <mi>t</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo>&#x2212;<!-- − --></mo>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>if&#xA0;</mtext>
                  </mrow>
                  <mi>j</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>&#xA0;is an output neuron,</mtext>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo stretchy="false">(</mo>
                  <munder>
                    <mo>&#x2211;<!-- ∑ --></mo>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                      <mo>&#x2208;<!-- ∈ --></mo>
                      <mi>L</mi>
                    </mrow>
                  </munder>
                  <msub>
                    <mi>w</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>&#x03B4;<!-- δ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x2113;<!-- ℓ --></mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo>&#x2212;<!-- − --></mo>
                  <msub>
                    <mi>o</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>if&#xA0;</mtext>
                  </mrow>
                  <mi>j</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>&#xA0;is an inner neuron.</mtext>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
            <mo fence="true" stretchy="true" symmetric="true"></mo>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}(o_{j}-t_{j})o_{j}(1-o_{j})&amp;{\text{if }}j{\text{ is an output neuron,}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell })o_{j}(1-o_{j})&amp;{\text{if }}j{\text{ is an inner neuron.}}\end{cases}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/991c8f020800ec1da130849e20a3a415613e9bdb" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:70.04ex; height:6.509ex;" alt="{\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}(o_{j}-t_{j})o_{j}(1-o_{j})&amp;{\text{if }}j{\text{ is an output neuron,}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell })o_{j}(1-o_{j})&amp;{\text{if }}j{\text{ is an inner neuron.}}\end{cases}}}"></span></dd></dl>
<p>To update the weight <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span> using gradient descent, one must choose a learning rate, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \eta &gt;0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03B7;<!-- η --></mi>
        <mo>&gt;</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta &gt;0}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ce70a0b6474dcb5aaeea68b799038e8f60b54ef1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:5.43ex; height:2.676ex;" alt="{\displaystyle \eta &gt;0}"></span>. The change in weight needs to reflect the impact on <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.176ex;" alt="{\displaystyle E}"></span> of an increase or decrease in <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span>. If <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}&gt;0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>&gt;</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial E}{\partial w_{ij}}}&gt;0}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/52a50d4d1cc9a82d13f38ab6317adcbdb24b9112" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:9.556ex; height:6.176ex;" alt="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}&gt;0}"></span>, an increase in <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span> increases <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.176ex;" alt="{\displaystyle E}"></span>; conversely, if <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}&lt;0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>&lt;</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial E}{\partial w_{ij}}}&lt;0}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5669ee0bda9e01723c570b9496abbe4bdbd96357" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:9.556ex; height:6.176ex;" alt="{\displaystyle {\frac {\partial E}{\partial w_{ij}}}&lt;0}"></span>, an increase in <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span> decreases <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.176ex;" alt="{\displaystyle E}"></span>. The new <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \Delta w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x0394;<!-- Δ --></mi>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Delta w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/75343e68050b7d0bc74986719687a845095ad59f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:5.077ex; height:2.843ex;" alt="{\displaystyle \Delta w_{ij}}"></span> is added to the old weight, and the product of the learning rate and the gradient, multiplied by <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle -1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo>&#x2212;<!-- − --></mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle -1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/704fb0427140d054dd267925495e78164fee9aac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.505ex; width:2.971ex; height:2.343ex;" alt="{\displaystyle -1}"></span> guarantees that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span> changes in a way that always decreases <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.176ex;" alt="{\displaystyle E}"></span>. In other words, in the equation immediately below, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle -\eta {\frac {\partial E}{\partial w_{ij}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03B7;<!-- η --></mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle -\eta {\frac {\partial E}{\partial w_{ij}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5f908b3074295c5281b9f6363c4dd65cca7476c" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:8.273ex; height:6.176ex;" alt="{\displaystyle -\eta {\frac {\partial E}{\partial w_{ij}}}}"></span> always changes <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ij}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ij}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3302ff355269436b43bc2fbe180303881c09321" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:3.141ex; height:2.343ex;" alt="{\displaystyle w_{ij}}"></span> in such a way that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.776ex; height:2.176ex;" alt="{\displaystyle E}"></span> is decreased:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \Delta w_{ij}=-\eta {\frac {\partial E}{\partial w_{ij}}}=-\eta o_{i}\delta _{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x0394;<!-- Δ --></mi>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03B7;<!-- η --></mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>E</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msub>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03B7;<!-- η --></mi>
        <msub>
          <mi>o</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <msub>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Delta w_{ij}=-\eta {\frac {\partial E}{\partial w_{ij}}}=-\eta o_{i}\delta _{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/19e12758145196a841af866e8dfa2437d35c82e8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:26.394ex; height:6.176ex;" alt="{\displaystyle \Delta w_{ij}=-\eta {\frac {\partial E}{\partial w_{ij}}}=-\eta o_{i}\delta _{j}}"></span></dd></dl>
<div class="mw-heading mw-heading2"><h2 id="Second-order_gradient_descent">Second-order gradient descent</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=9" title="Edit section: Second-order gradient descent"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p><span class="anchor" id="Second&#95;order"></span><span class="anchor" id="Hessian"></span>
</p><p>Using a <a href="/wiki/Hessian_matrix" title="Hessian matrix">Hessian matrix</a> of second-order derivatives of the error function, the <a href="/wiki/Levenberg%E2%80%93Marquardt_algorithm" title="Levenberg–Marquardt algorithm">Levenberg–Marquardt algorithm</a> often converges faster than first-order gradient descent, especially when the topology of the error function is complicated.<sup id="cite&#95;ref-Tan2018&#95;18-0" class="reference"><a href="#cite_note-Tan2018-18"><span class="cite-bracket">&#91;</span>9<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-Wiliamowski2010&#95;19-0" class="reference"><a href="#cite_note-Wiliamowski2010-19"><span class="cite-bracket">&#91;</span>10<span class="cite-bracket">&#93;</span></a></sup> It may also find solutions in smaller node counts for which other methods might not converge.<sup id="cite&#95;ref-Wiliamowski2010&#95;19-1" class="reference"><a href="#cite_note-Wiliamowski2010-19"><span class="cite-bracket">&#91;</span>10<span class="cite-bracket">&#93;</span></a></sup> The Hessian can be approximated by the <a href="/wiki/Fisher_information" title="Fisher information">Fisher information</a> matrix.<sup id="cite&#95;ref-Martens2020&#95;20-0" class="reference"><a href="#cite_note-Martens2020-20"><span class="cite-bracket">&#91;</span>11<span class="cite-bracket">&#93;</span></a></sup>
</p><p>As an example, consider a simple feedforward network. At the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>-th layer, we have<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x_{i}^{(l)},\quad a_{i}^{(l)}=f(x_{i}^{(l)}),\quad x_{i}^{(l+1)}=\sum _{j}W_{ij}a_{j}^{(l)}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>l</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>,</mo>
        <mspace width="1em" />
        <msubsup>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>l</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>l</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mspace width="1em" />
        <msubsup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>l</mi>
            <mo>+</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </munder>
        <msub>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <msubsup>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>l</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}^{(l)},\quad a_{i}^{(l)}=f(x_{i}^{(l)}),\quad x_{i}^{(l+1)}=\sum _{j}W_{ij}a_{j}^{(l)}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1cdaaafce02bf292e9b26824f05290d5150178f9" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.338ex; width:41.969ex; height:6.009ex;" alt="{\displaystyle x_{i}^{(l)},\quad a_{i}^{(l)}=f(x_{i}^{(l)}),\quad x_{i}^{(l+1)}=\sum _{j}W_{ij}a_{j}^{(l)}}"></span>where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="{\displaystyle x}"></span> are the pre-activations, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle a}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>a</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle a}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.23ex; height:1.676ex;" alt="{\displaystyle a}"></span> are the activations, and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>W</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54a9c4c547f4d6111f81946cad242b18298d70b7" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.435ex; height:2.176ex;" alt="{\displaystyle W}"></span> is the weight matrix. Given a loss function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle L}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>L</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/103168b86f781fe6e9a4a87b8ea1cebe0ad4ede8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.583ex; height:2.176ex;" alt="{\displaystyle L}"></span>, the first-order backpropagation states that<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial L}{\partial a_{j}^{(l)}}}=\sum _{j}W_{ij}{\frac {\partial L}{\partial x_{i}^{(l+1)}}},\quad {\frac {\partial L}{\partial x_{j}^{(l)}}}=f'(x_{j}^{(l)}){\frac {\partial L}{\partial a_{j}^{(l)}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>L</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </munder>
        <msub>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>L</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>,</mo>
        <mspace width="1em" />
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>L</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <msup>
          <mi>f</mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>l</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>L</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial L}{\partial a_{j}^{(l)}}}=\sum _{j}W_{ij}{\frac {\partial L}{\partial x_{i}^{(l+1)}}},\quad {\frac {\partial L}{\partial x_{j}^{(l)}}}=f'(x_{j}^{(l)}){\frac {\partial L}{\partial a_{j}^{(l)}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/45f97891ebab184acc93b38f48bf5c5c0b8e460e" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.838ex; width:47.956ex; height:7.343ex;" alt="{\displaystyle {\frac {\partial L}{\partial a_{j}^{(l)}}}=\sum _{j}W_{ij}{\frac {\partial L}{\partial x_{i}^{(l+1)}}},\quad {\frac {\partial L}{\partial x_{j}^{(l)}}}=f&#039;(x_{j}^{(l)}){\frac {\partial L}{\partial a_{j}^{(l)}}}}"></span>and the second-order backpropagation states that<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\frac {\partial ^{2}L}{\partial a_{j_{1}}^{(l)}\partial a_{j_{2}}^{(l)}}}=\sum _{j_{1}j_{2}}W_{i_{1}j_{1}}W_{i_{2}j_{2}}{\frac {\partial ^{2}L}{\partial x_{i_{1}}^{(l+1)}\partial x_{i_{2}}^{(l+1)}}},\quad {\frac {\partial ^{2}L}{\partial x_{j_{1}}^{(l)}\partial x_{j_{2}}^{(l)}}}=f'(x_{j_{1}}^{(l)})f'(x_{j_{2}}^{(l)}){\frac {\partial ^{2}L}{\partial a_{j_{1}}^{(l)}\partial a_{j_{2}}^{(l)}}}+\delta _{j_{1}j_{2}}f''(x_{j_{1}}^{(l)}){\frac {\partial L}{\partial a_{j_{1}}^{(l)}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <msup>
                <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>2</mn>
                </mrow>
              </msup>
              <mi>L</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>j</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>j</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>j</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
            <msub>
              <mi>j</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msub>
          </mrow>
        </munder>
        <msub>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>i</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
            <msub>
              <mi>j</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
          </mrow>
        </msub>
        <msub>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>i</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msub>
            <msub>
              <mi>j</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msub>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <msup>
                <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>2</mn>
                </mrow>
              </msup>
              <mi>L</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>i</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>i</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>,</mo>
        <mspace width="1em" />
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <msup>
                <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>2</mn>
                </mrow>
              </msup>
              <mi>L</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>j</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>j</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <msup>
          <mi>f</mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>j</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>l</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
        <msup>
          <mi>f</mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>j</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msub>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>l</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <msup>
                <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>2</mn>
                </mrow>
              </msup>
              <mi>L</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>j</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>j</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </mfrac>
        </mrow>
        <mo>+</mo>
        <msub>
          <mi>&#x03B4;<!-- δ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>j</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
            <msub>
              <mi>j</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msub>
          </mrow>
        </msub>
        <msup>
          <mi>f</mi>
          <mo>&#x2033;</mo>
        </msup>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>j</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>l</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <mi>L</mi>
            </mrow>
            <mrow>
              <mi mathvariant="normal">&#x2202;<!-- ∂ --></mi>
              <msubsup>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>j</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>l</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {\partial ^{2}L}{\partial a_{j_{1}}^{(l)}\partial a_{j_{2}}^{(l)}}}=\sum _{j_{1}j_{2}}W_{i_{1}j_{1}}W_{i_{2}j_{2}}{\frac {\partial ^{2}L}{\partial x_{i_{1}}^{(l+1)}\partial x_{i_{2}}^{(l+1)}}},\quad {\frac {\partial ^{2}L}{\partial x_{j_{1}}^{(l)}\partial x_{j_{2}}^{(l)}}}=f'(x_{j_{1}}^{(l)})f'(x_{j_{2}}^{(l)}){\frac {\partial ^{2}L}{\partial a_{j_{1}}^{(l)}\partial a_{j_{2}}^{(l)}}}+\delta _{j_{1}j_{2}}f''(x_{j_{1}}^{(l)}){\frac {\partial L}{\partial a_{j_{1}}^{(l)}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bde7b4d660b092bee32b69146d5d6caf6b43cd69" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.838ex; width:102.706ex; height:7.676ex;" alt="{\displaystyle {\frac {\partial ^{2}L}{\partial a_{j_{1}}^{(l)}\partial a_{j_{2}}^{(l)}}}=\sum _{j_{1}j_{2}}W_{i_{1}j_{1}}W_{i_{2}j_{2}}{\frac {\partial ^{2}L}{\partial x_{i_{1}}^{(l+1)}\partial x_{i_{2}}^{(l+1)}}},\quad {\frac {\partial ^{2}L}{\partial x_{j_{1}}^{(l)}\partial x_{j_{2}}^{(l)}}}=f&#039;(x_{j_{1}}^{(l)})f&#039;(x_{j_{2}}^{(l)}){\frac {\partial ^{2}L}{\partial a_{j_{1}}^{(l)}\partial a_{j_{2}}^{(l)}}}+\delta _{j_{1}j_{2}}f&#039;&#039;(x_{j_{1}}^{(l)}){\frac {\partial L}{\partial a_{j_{1}}^{(l)}}}}"></span>where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \delta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03B4;<!-- δ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \delta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c5321cfa797202b3e1f8620663ff43c4660ea03a" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.049ex; height:2.343ex;" alt="{\displaystyle \delta }"></span> is the <a href="/wiki/Dirac_delta_function" title="Dirac delta function">Dirac delta symbol</a>.
</p><p>Arbitrary-order derivatives in arbitrary computational graphs can be computed with backpropagation, but with more complex expressions for higher orders.
</p>
<div class="mw-heading mw-heading2"><h2 id="Loss_function">Loss function</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=10" title="Edit section: Loss function"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1320445320" /><div role="note" class="hatnote navigation-not-searchable">Further information: <a href="/wiki/Loss_function" title="Loss function">Loss function</a></div>
<p>The loss function is a function that maps values of one or more variables onto a <a href="/wiki/Real_number" title="Real number">real number</a> intuitively representing some "cost" associated with those values. For backpropagation, the loss function calculates the difference between the network output and its expected output, after a training example has propagated through the network.
</p>
<div class="mw-heading mw-heading3"><h3 id="Assumptions">Assumptions</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=11" title="Edit section: Assumptions"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The mathematical expression of the loss function must fulfill two conditions in order for it to be possibly used in backpropagation.<sup id="cite&#95;ref-21" class="reference"><a href="#cite_note-21"><span class="cite-bracket">&#91;</span>12<span class="cite-bracket">&#93;</span></a></sup> The first is that it can be written as an average <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle E={\frac {1}{n}}\sum _{x}E_{x}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <mi>E</mi>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mi>n</mi>
          </mfrac>
        </mrow>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
          </mrow>
        </munder>
        <msub>
          <mi>E</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle E={\frac {1}{n}}\sum _{x}E_{x}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/10d86f4c725f4158341b538f99eacc618e640b78" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:13.985ex; height:3.343ex;" alt="{\textstyle E={\frac {1}{n}}\sum _{x}E_{x}}"></span> over error functions <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle E_{x}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <msub>
          <mi>E</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle E_{x}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fa289d8c2030c92095fe10836f648984bd80d10b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.888ex; height:2.509ex;" alt="{\textstyle E_{x}}"></span>, for <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc6e1f880981346a604257ebcacdef24c0aca2d6" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;" alt="{\textstyle n}"></span> individual training examples, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle x}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d951e0f3b54b6a3d73bb9a0a005749046cbce781" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="{\textstyle x}"></span>. The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example, which needs to be generalized to the overall error function. The second assumption is that it can be written as a function of the outputs from the neural network.
</p>
<div class="mw-heading mw-heading3"><h3 id="Example_loss_function">Example loss function</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=12" title="Edit section: Example loss function"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Let <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y,y'}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>y</mi>
        <mo>,</mo>
        <msup>
          <mi>y</mi>
          <mo>&#x2032;</mo>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y,y'}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7de53e950ff33763c8d524e32280498ec480f86" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:4.035ex; height:2.843ex;" alt="{\displaystyle y,y&#039;}"></span> be vectors in <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbb {R} ^{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbb {R} ^{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c510b63578322050121fe966f2e5770bea43308d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.897ex; height:2.343ex;" alt="{\displaystyle \mathbb {R} ^{n}}"></span>.
</p><p>Select an error function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E(y,y')}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo>,</mo>
        <msup>
          <mi>y</mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E(y,y')}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/15c4c7f9edb4d4a5ca9844bfb82f947363d0d40f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:7.62ex; height:3.009ex;" alt="{\displaystyle E(y,y&#039;)}"></span> measuring the difference between two outputs. The standard choice is the square of the <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> between the vectors <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>y</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;" alt="{\displaystyle y}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y'}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>y</mi>
          <mo>&#x2032;</mo>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y'}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3a535de94a2183d7130731eab8a83531d7c35c6b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.845ex; height:2.843ex;" alt="{\displaystyle y&#039;}"></span>:<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E(y,y')={\tfrac {1}{2}}\lVert y-y'\rVert ^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo>,</mo>
        <msup>
          <mi>y</mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mstyle displaystyle="false" scriptlevel="0">
            <mfrac>
              <mn>1</mn>
              <mn>2</mn>
            </mfrac>
          </mstyle>
        </mrow>
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <mi>y</mi>
        <mo>&#x2212;<!-- − --></mo>
        <msup>
          <mi>y</mi>
          <mo>&#x2032;</mo>
        </msup>
        <msup>
          <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E(y,y')={\tfrac {1}{2}}\lVert y-y'\rVert ^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/11ae60805a3914707a33ece46b0d46700262e028" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.171ex; width:21.596ex; height:3.509ex;" alt="{\displaystyle E(y,y&#039;)={\tfrac {1}{2}}\lVert y-y&#039;\rVert ^{2}}"></span>The error function over <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc6e1f880981346a604257ebcacdef24c0aca2d6" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;" alt="{\textstyle n}"></span> training examples can then be written as an average of losses over individual examples:<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle E={\frac {1}{2n}}\sum _{x}\lVert (y(x)-y'(x))\rVert ^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mrow>
              <mn>2</mn>
              <mi>n</mi>
            </mrow>
          </mfrac>
        </mrow>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
          </mrow>
        </munder>
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>&#x2212;<!-- − --></mo>
        <msup>
          <mi>y</mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <msup>
          <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E={\frac {1}{2n}}\sum _{x}\lVert (y(x)-y'(x))\rVert ^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/774b3a3904cbcc2ea2768491f4df2cf1c041563c" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.005ex; width:29.317ex; height:6.343ex;" alt="{\displaystyle E={\frac {1}{2n}}\sum _{x}\lVert (y(x)-y&#039;(x))\rVert ^{2}}"></span>
</p>
<div class="mw-heading mw-heading2"><h2 id="Limitations">Limitations</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=13" title="Edit section: Limitations"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure typeof="mw:File/Thumb"><a href="/wiki/File:Extrema_example.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Extrema_example.svg/250px-Extrema_example.svg.png" decoding="async" width="250" height="200" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Extrema_example.svg/500px-Extrema_example.svg.png 1.5x" data-file-width="600" data-file-height="480" /></a><figcaption>Gradient descent may find a local minimum instead of the global minimum.</figcaption></figure>
<ul><li>Gradient descent with backpropagation is not guaranteed to find the <a href="/wiki/Maxima_and_minima" class="mw-redirect" title="Maxima and minima">global minimum</a> of the error function, but only a local minimum; also, it has trouble crossing <a href="/wiki/Plateau_(mathematics)" title="Plateau (mathematics)">plateaus</a> in the error function landscape. This issue, caused by the <a href="/wiki/Convex_optimization" title="Convex optimization">non-convexity</a> of error functions in neural networks, was long thought to be a major drawback, but <a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a> <i>et al.</i> argue that in many practical problems, it is not.<sup id="cite&#95;ref-22" class="reference"><a href="#cite_note-22"><span class="cite-bracket">&#91;</span>13<span class="cite-bracket">&#93;</span></a></sup></li>
<li>Backpropagation learning does not require normalization of input vectors; however, normalization could improve performance.<sup id="cite&#95;ref-23" class="reference"><a href="#cite_note-23"><span class="cite-bracket">&#91;</span>14<span class="cite-bracket">&#93;</span></a></sup></li>
<li>Backpropagation requires the derivatives of activation functions to be known at network design time.</li></ul>
<div class="mw-heading mw-heading2"><h2 id="History">History</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=14" title="Edit section: History"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1320445320" /><div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Perceptron#History" title="Perceptron">History of perceptron</a></div>
<div class="mw-heading mw-heading3"><h3 id="Precursors">Precursors</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=15" title="Edit section: Precursors"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Backpropagation had been derived repeatedly, as it is essentially an efficient application of the <a href="/wiki/Chain_rule" title="Chain rule">chain rule</a> (first written down by <a href="/wiki/Gottfried_Wilhelm_Leibniz" title="Gottfried Wilhelm Leibniz">Gottfried Wilhelm Leibniz</a> in 1676)<sup id="cite&#95;ref-leibniz1676&#95;24-0" class="reference"><a href="#cite_note-leibniz1676-24"><span class="cite-bracket">&#91;</span>15<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-25" class="reference"><a href="#cite_note-25"><span class="cite-bracket">&#91;</span>16<span class="cite-bracket">&#93;</span></a></sup> to neural networks.
</p><p>The terminology "back-propagating error correction" was introduced in 1962 by <a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a>, but he did not know how to implement this.<sup id="cite&#95;ref-26" class="reference"><a href="#cite_note-26"><span class="cite-bracket">&#91;</span>17<span class="cite-bracket">&#93;</span></a></sup> In any case, he only studied neurons whose outputs were discrete levels, which only had zero derivatives, making backpropagation impossible.
</p><p>Precursors to backpropagation appeared in <a href="/wiki/Optimal_control" title="Optimal control">optimal control theory</a> since 1950s. <a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a> et al credits 1950s work by <a href="/wiki/Lev_Pontryagin" title="Lev Pontryagin">Pontryagin</a> and others in optimal control theory, especially the <a href="/wiki/Adjoint_state_method" title="Adjoint state method">adjoint state method</a>, for being a continuous-time version of backpropagation.<sup id="cite&#95;ref-27" class="reference"><a href="#cite_note-27"><span class="cite-bracket">&#91;</span>18<span class="cite-bracket">&#93;</span></a></sup> <a href="/wiki/Robert_Hecht-Nielsen" title="Robert Hecht-Nielsen">Hecht-Nielsen</a><sup id="cite&#95;ref-28" class="reference"><a href="#cite_note-28"><span class="cite-bracket">&#91;</span>19<span class="cite-bracket">&#93;</span></a></sup> credits the <a href="/wiki/Stochastic_approximation" title="Stochastic approximation">Robbins–Monro algorithm</a> (1951)<sup id="cite&#95;ref-robbins1951&#95;29-0" class="reference"><a href="#cite_note-robbins1951-29"><span class="cite-bracket">&#91;</span>20<span class="cite-bracket">&#93;</span></a></sup> and <a href="/wiki/Arthur_E._Bryson" title="Arthur E. Bryson">Arthur Bryson</a> and <a href="/wiki/Yu-Chi_Ho" title="Yu-Chi Ho">Yu-Chi Ho</a>'s <i>Applied Optimal Control</i> (1969) as presages of backpropagation. Other precursors were <a href="/wiki/Henry_J._Kelley" title="Henry J. Kelley">Henry J. Kelley</a> 1960,<sup id="cite&#95;ref-kelley1960&#95;1-1" class="reference"><a href="#cite_note-kelley1960-1"><span class="cite-bracket">&#91;</span>1<span class="cite-bracket">&#93;</span></a></sup> and <a href="/wiki/Arthur_E._Bryson" title="Arthur E. Bryson">Arthur E. Bryson</a> (1961).<sup id="cite&#95;ref-bryson1961&#95;2-1" class="reference"><a href="#cite_note-bryson1961-2"><span class="cite-bracket">&#91;</span>2<span class="cite-bracket">&#93;</span></a></sup> In 1962, <a href="/wiki/Stuart_Dreyfus" title="Stuart Dreyfus">Stuart Dreyfus</a> published a simpler derivation based only on the <a href="/wiki/Chain_rule" title="Chain rule">chain rule</a>.<sup id="cite&#95;ref-30" class="reference"><a href="#cite_note-30"><span class="cite-bracket">&#91;</span>21<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-dreyfus1990&#95;31-0" class="reference"><a href="#cite_note-dreyfus1990-31"><span class="cite-bracket">&#91;</span>22<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-32" class="reference"><a href="#cite_note-32"><span class="cite-bracket">&#91;</span>23<span class="cite-bracket">&#93;</span></a></sup> In 1973, he adapted <a href="/wiki/Parameter" title="Parameter">parameters</a> of controllers in proportion to error gradients.<sup id="cite&#95;ref-dreyfus1973&#95;33-0" class="reference"><a href="#cite_note-dreyfus1973-33"><span class="cite-bracket">&#91;</span>24<span class="cite-bracket">&#93;</span></a></sup> Unlike modern backpropagation, these precursors used standard Jacobian matrix calculations from one stage to the previous one, neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity.<sup id="cite&#95;ref-DLhistory&#95;34-0" class="reference"><a href="#cite_note-DLhistory-34"><span class="cite-bracket">&#91;</span>25<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The <a href="/wiki/ADALINE" title="ADALINE">ADALINE</a> (1960) learning algorithm was gradient descent with a squared error loss for a single layer. The first <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptron</a> (MLP) with more than one layer trained by <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">stochastic gradient descent</a><sup id="cite&#95;ref-robbins1951&#95;29-1" class="reference"><a href="#cite_note-robbins1951-29"><span class="cite-bracket">&#91;</span>20<span class="cite-bracket">&#93;</span></a></sup> was published in 1967 by <a href="/wiki/Shun%27ichi_Amari" title="Shun&#39;ichi Amari">Shun'ichi Amari</a>.<sup id="cite&#95;ref-Amari1967&#95;35-0" class="reference"><a href="#cite_note-Amari1967-35"><span class="cite-bracket">&#91;</span>26<span class="cite-bracket">&#93;</span></a></sup> The MLP had 5 layers, with 2 learnable layers, and it learned to classify patterns not linearly separable.<sup id="cite&#95;ref-DLhistory&#95;34-1" class="reference"><a href="#cite_note-DLhistory-34"><span class="cite-bracket">&#91;</span>25<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Modern_backpropagation">Modern backpropagation</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=16" title="Edit section: Modern backpropagation"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Modern backpropagation was first published by <a href="/wiki/Seppo_Linnainmaa" title="Seppo Linnainmaa">Seppo Linnainmaa</a> as "reverse mode of <a href="/wiki/Automatic_differentiation" title="Automatic differentiation">automatic differentiation</a>" (1970)<sup id="cite&#95;ref-lin1970&#95;36-0" class="reference"><a href="#cite_note-lin1970-36"><span class="cite-bracket">&#91;</span>27<span class="cite-bracket">&#93;</span></a></sup> for discrete connected networks of nested <a href="/wiki/Differentiable_function" title="Differentiable function">differentiable</a> functions.<sup id="cite&#95;ref-lin1976&#95;37-0" class="reference"><a href="#cite_note-lin1976-37"><span class="cite-bracket">&#91;</span>28<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-grie2012&#95;38-0" class="reference"><a href="#cite_note-grie2012-38"><span class="cite-bracket">&#91;</span>29<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-grie2008&#95;39-0" class="reference"><a href="#cite_note-grie2008-39"><span class="cite-bracket">&#91;</span>30<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In 1982, <a href="/wiki/Paul_Werbos" title="Paul Werbos">Paul Werbos</a> applied backpropagation to MLPs in the way that has become standard.<sup id="cite&#95;ref-werbos1982&#95;40-0" class="reference"><a href="#cite_note-werbos1982-40"><span class="cite-bracket">&#91;</span>31<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-werbos1974&#95;41-0" class="reference"><a href="#cite_note-werbos1974-41"><span class="cite-bracket">&#91;</span>32<span class="cite-bracket">&#93;</span></a></sup> Werbos described how he developed backpropagation in an interview. In 1971, during his PhD work, he developed backpropagation to mathematicize <a href="/wiki/Sigmund_Freud" title="Sigmund Freud">Freud</a>'s "flow of psychic energy". He faced repeated difficulty in publishing the work, only managing in 1981.<sup id="cite&#95;ref-:1&#95;42-0" class="reference"><a href="#cite_note-:1-42"><span class="cite-bracket">&#91;</span>33<span class="cite-bracket">&#93;</span></a></sup> He also claimed that "the first practical application of back-propagation was for estimating a dynamic model to predict nationalism and social communications in 1974" by him.<sup id="cite&#95;ref-43" class="reference"><a href="#cite_note-43"><span class="cite-bracket">&#91;</span>34<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Around 1982,<sup id="cite&#95;ref-:1&#95;42-1" class="reference"><a href="#cite_note-:1-42"><span class="cite-bracket">&#91;</span>33<span class="cite-bracket">&#93;</span></a></sup><sup class="reference nowrap"><span title="Page / location: 376">&#58;&#8202;376&#8202;</span></sup> <a href="/wiki/David_E._Rumelhart" class="mw-redirect" title="David E. Rumelhart">David E. Rumelhart</a> independently developed<sup id="cite&#95;ref-44" class="reference"><a href="#cite_note-44"><span class="cite-bracket">&#91;</span>35<span class="cite-bracket">&#93;</span></a></sup><sup class="reference nowrap"><span title="Page / location: 252">&#58;&#8202;252&#8202;</span></sup> backpropagation and taught the algorithm to others in his research circle. He did not cite previous work as he was unaware of them. He published the algorithm first in a 1985 paper, then in a 1986 <i><a href="/wiki/Nature_(journal)" title="Nature (journal)">Nature</a></i> paper an experimental analysis of the technique.<sup id="cite&#95;ref-learning-representations&#95;45-0" class="reference"><a href="#cite_note-learning-representations-45"><span class="cite-bracket">&#91;</span>36<span class="cite-bracket">&#93;</span></a></sup> These papers became highly cited, contributed to the popularization of backpropagation, and coincided with the resurging research interest in neural networks during the 1980s.<sup id="cite&#95;ref-RumelhartHintonWilliams1986a&#95;14-2" class="reference"><a href="#cite_note-RumelhartHintonWilliams1986a-14"><span class="cite-bracket">&#91;</span>8<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-RumelhartHintonWilliams1986b&#95;46-0" class="reference"><a href="#cite_note-RumelhartHintonWilliams1986b-46"><span class="cite-bracket">&#91;</span>37<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-47" class="reference"><a href="#cite_note-47"><span class="cite-bracket">&#91;</span>38<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In 1985, the method was also described by David Parker.<sup id="cite&#95;ref-48" class="reference"><a href="#cite_note-48"><span class="cite-bracket">&#91;</span>39<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-:0&#95;49-0" class="reference"><a href="#cite_note-:0-49"><span class="cite-bracket">&#91;</span>40<span class="cite-bracket">&#93;</span></a></sup> <a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a> proposed an alternative form of backpropagation for neural networks in his PhD thesis in 1987.<sup id="cite&#95;ref-50" class="reference"><a href="#cite_note-50"><span class="cite-bracket">&#91;</span>41<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Gradient descent took a considerable amount of time to reach acceptance. Some early objections were: there were no guarantees that gradient descent could reach a global minimum, only local minimum; neurons were "known" by physiologists as making discrete signals (0/1), not continuous ones, and with discrete signals, there is no gradient to take. See the interview with <a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a>,<sup id="cite&#95;ref-:1&#95;42-2" class="reference"><a href="#cite_note-:1-42"><span class="cite-bracket">&#91;</span>33<span class="cite-bracket">&#93;</span></a></sup> who was awarded the 2024 <a href="/wiki/Nobel_Prize_in_Physics" title="Nobel Prize in Physics">Nobel Prize in Physics</a> for his contributions to the field.<sup id="cite&#95;ref-51" class="reference"><a href="#cite_note-51"><span class="cite-bracket">&#91;</span>42<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Early_successes">Early successes</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=17" title="Edit section: Early successes"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Contributing to the acceptance were several applications in training neural networks via backpropagation, sometimes achieving popularity outside the research circles.
</p><p>In 1987, <a href="/wiki/NETtalk_(artificial_neural_network)" title="NETtalk (artificial neural network)">NETtalk</a> learned to convert English text into pronunciation. Sejnowski tried training it with both backpropagation and Boltzmann machine, but found the backpropagation significantly faster, so he used it for the final NETtalk.<sup id="cite&#95;ref-:1&#95;42-3" class="reference"><a href="#cite_note-:1-42"><span class="cite-bracket">&#91;</span>33<span class="cite-bracket">&#93;</span></a></sup><sup class="reference nowrap"><span title="Page: 324">&#58;&#8202;324&#8202;</span></sup> The NETtalk program became a popular success, appearing on the <a href="/wiki/Today_(American_TV_program)" title="Today (American TV program)"><i>Today</i> show</a>.<sup id="cite&#95;ref-:02&#95;52-0" class="reference"><a href="#cite_note-:02-52"><span class="cite-bracket">&#91;</span>43<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In 1989, Dean A. Pomerleau published ALVINN, a neural network trained to <a href="/wiki/Vehicular_automation" title="Vehicular automation">drive autonomously</a> using backpropagation.<sup id="cite&#95;ref-53" class="reference"><a href="#cite_note-53"><span class="cite-bracket">&#91;</span>44<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The <a href="/wiki/LeNet" title="LeNet">LeNet</a> was published in 1989 to recognize handwritten zip codes.
</p><p>In 1992, <a href="/wiki/TD-Gammon" title="TD-Gammon">TD-Gammon</a> achieved top human level play in backgammon. It was a reinforcement learning agent with a neural network with two layers, trained by backpropagation.<sup id="cite&#95;ref-54" class="reference"><a href="#cite_note-54"><span class="cite-bracket">&#91;</span>45<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In 1993, Eric Wan won an international pattern recognition contest through backpropagation.<sup id="cite&#95;ref-schmidhuber2015&#95;55-0" class="reference"><a href="#cite_note-schmidhuber2015-55"><span class="cite-bracket">&#91;</span>46<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-56" class="reference"><a href="#cite_note-56"><span class="cite-bracket">&#91;</span>47<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="After_backpropagation">After backpropagation</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=18" title="Edit section: After backpropagation"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>During the 2000s it fell out of favour<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (February 2022)">citation needed</span></a></i>&#93;</sup>, but returned in the 2010s, benefiting from cheap, powerful <a href="/wiki/GPU" class="mw-redirect" title="GPU">GPU</a>-based computing systems. This has been especially so in <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, <a href="/wiki/Machine_vision" title="Machine vision">machine vision</a>, <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>, and language structure learning research (in which it has been used to explain a variety of phenomena related to first<sup id="cite&#95;ref-57" class="reference"><a href="#cite_note-57"><span class="cite-bracket">&#91;</span>48<span class="cite-bracket">&#93;</span></a></sup> and second language learning.<sup id="cite&#95;ref-58" class="reference"><a href="#cite_note-58"><span class="cite-bracket">&#91;</span>49<span class="cite-bracket">&#93;</span></a></sup>)<sup id="cite&#95;ref-59" class="reference"><a href="#cite_note-59"><span class="cite-bracket">&#91;</span>50<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Error backpropagation has been suggested to explain human brain <a href="/wiki/Event-related_potential" title="Event-related potential">event-related potential</a> (ERP) components like the <a href="/wiki/N400_(neuroscience)" title="N400 (neuroscience)">N400</a> and <a href="/wiki/P600_(neuroscience)" title="P600 (neuroscience)">P600</a>.<sup id="cite&#95;ref-60" class="reference"><a href="#cite_note-60"><span class="cite-bracket">&#91;</span>51<span class="cite-bracket">&#93;</span></a></sup>
</p><p>In 2023, a backpropagation algorithm was implemented on a <a href="/wiki/Photonic_processor" class="mw-redirect" title="Photonic processor">photonic processor</a> by a team at <a href="/wiki/Stanford_University" title="Stanford University">Stanford University</a>.<sup id="cite&#95;ref-61" class="reference"><a href="#cite_note-61"><span class="cite-bracket">&#91;</span>52<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="See_also">See also</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=19" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">Artificial neural network</a></li>
<li><a href="/wiki/Neural_circuit" title="Neural circuit">Neural circuit</a></li>
<li><a href="/wiki/Catastrophic_interference" title="Catastrophic interference">Catastrophic interference</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensemble learning</a></li>
<li><a href="/wiki/AdaBoost" title="AdaBoost">AdaBoost</a></li>
<li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li>
<li><a href="/wiki/Neural_backpropagation" title="Neural backpropagation">Neural backpropagation</a></li>
<li><a href="/wiki/Backpropagation_through_time" title="Backpropagation through time">Backpropagation through time</a></li>
<li><a href="/wiki/Backpropagation_through_structure" title="Backpropagation through structure">Backpropagation through structure</a></li>
<li><a href="/wiki/Three-factor_learning" title="Three-factor learning">Three-factor learning</a></li></ul>
<div class="mw-heading mw-heading2"><h2 id="Notes">Notes</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=20" title="Edit section: Notes"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1327269900">.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}body.skin-vector-2022 .mw-parser-output .reflist-columns-2{column-width:27em}body.skin-vector-2022 .mw-parser-output .reflist-columns-3{column-width:22.5em}.mw-parser-output .references[data-mw-group=upper-alpha]{list-style-type:upper-alpha}.mw-parser-output .references[data-mw-group=upper-roman]{list-style-type:upper-roman}.mw-parser-output .references[data-mw-group=lower-alpha]{list-style-type:lower-alpha}.mw-parser-output .references[data-mw-group=lower-greek]{list-style-type:lower-greek}.mw-parser-output .references[data-mw-group=lower-roman]{list-style-type:lower-roman}.mw-parser-output div.reflist-liststyle-upper-alpha .references{list-style-type:upper-alpha}.mw-parser-output div.reflist-liststyle-upper-roman .references{list-style-type:upper-roman}.mw-parser-output div.reflist-liststyle-lower-alpha .references{list-style-type:lower-alpha}.mw-parser-output div.reflist-liststyle-lower-greek .references{list-style-type:lower-greek}.mw-parser-output div.reflist-liststyle-lower-roman .references{list-style-type:lower-roman}</style><div>
<div class="mw-references-wrap"><ol class="references" data-mw-group="lower-alpha">
<li id="cite&#95;note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text">Use <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fc55753007cd3c18576f7933f6f089196732029" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.766ex; height:2.176ex;" alt="{\displaystyle C}"></span> for the loss function to allow <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle L}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>L</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/103168b86f781fe6e9a4a87b8ea1cebe0ad4ede8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.583ex; height:2.176ex;" alt="{\displaystyle L}"></span> to be used for the number of layers</span>
</li>
<li id="cite&#95;note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text">This follows <a href="#CITEREFNielsen2015">Nielsen (2015)</a>, and means (left) multiplication by the matrix <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle W^{l}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>W</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W^{l}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60b87bc8138a82f110491bb1abd0ea6eefe84c22" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:3.23ex; height:2.676ex;" alt="{\displaystyle W^{l}}"></span> corresponds to converting output values of layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l-1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l-1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4e0d448948a353d0b2469b88ca918f34e32c8752" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.505ex; width:4.696ex; height:2.343ex;" alt="{\displaystyle l-1}"></span> to input values of layer <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle l}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>l</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle l}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;" alt="{\displaystyle l}"></span>: columns correspond to input coordinates, rows correspond to output coordinates.</span>
</li>
<li id="cite&#95;note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">This section largely follows and summarizes <a href="#CITEREFNielsen2015">Nielsen (2015)</a>.</span>
</li>
<li id="cite&#95;note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">The derivative of the loss function is a <a href="/wiki/Covector" class="mw-redirect" title="Covector">covector</a>, since the loss function is a <a href="/wiki/Scalar-valued_function" class="mw-redirect" title="Scalar-valued function">scalar-valued function</a> of several variables.</span>
</li>
<li id="cite&#95;note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text">The activation function is applied to each node separately, so the derivative is just the diagonal matrix of the derivative on each node. This is often represented as the <a href="/wiki/Hadamard_product_(matrices)" title="Hadamard product (matrices)">Hadamard product</a> with the vector of derivatives, denoted by <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (f^{l})'\odot }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msup>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>l</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2299;<!-- ⊙ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (f^{l})'\odot }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ee57033dfe04a65842e5260c85240f9295cb464c" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:6.345ex; height:3.176ex;" alt="{\displaystyle (f^{l})&#039;\odot }"></span>, which is mathematically identical but better matches the internal representation of the derivatives as a vector, rather than a diagonal matrix.</span>
</li>
<li id="cite&#95;note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text">Since matrix multiplication is linear, the derivative of multiplying by a matrix is just the matrix: <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (Wx)'=W}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mi>W</mi>
        <mi>x</mi>
        <msup>
          <mo stretchy="false">)</mo>
          <mo>&#x2032;</mo>
        </msup>
        <mo>=</mo>
        <mi>W</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (Wx)'=W}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0beeadf03249427d72b4c95732f23143fdfd6c30" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:11.793ex; height:3.009ex;" alt="{\displaystyle (Wx)&#039;=W}"></span>.</span>
</li>
<li id="cite&#95;note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text">One may notice that multi-layer neural networks use non-linear activation functions, so an example with linear neurons seems obscure. However, even though the error surface of multi-layer networks are much more complicated, locally they can be approximated by a paraboloid. Therefore, linear neurons are used for simplicity and easier understanding.</span>
</li>
<li id="cite&#95;note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">There can be multiple output neurons, in which case the error is the squared norm of the difference vector.</span>
</li>
<li id="cite&#95;note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text">This order follows (Rumelhart, Hinton &amp; Williams, 1986a):<sup id="cite&#95;ref-RumelhartHintonWilliams1986a&#95;14-1" class="reference"><a href="#cite_note-RumelhartHintonWilliams1986a-14"><span class="cite-bracket">&#91;</span>8<span class="cite-bracket">&#93;</span></a></sup> "<span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \Delta w_{i}j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x0394;<!-- Δ --></mi>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Delta w_{i}j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/06827536202e269493d147fd8987b9918f7a4aab" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:5.358ex; height:2.509ex;" alt="{\displaystyle \Delta w_{i}j}"></span> is the change to be made to the weight from the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle i}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;" alt="{\displaystyle i}"></span>th to the
<span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="{\displaystyle j}"></span>th unit"</span>
</li>
</ol></div></div>
<div class="mw-heading mw-heading2"><h2 id="References">References</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=21" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1327269900" /><div class="mw-references-wrap mw-references-columns" style="column-width: calc( 0.9 * 30em );"><ol class="references">
<li id="cite&#95;note-kelley1960-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-kelley1960_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-kelley1960_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1333433106">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#bf3c2c)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#bf3c2c)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}</style><cite id="CITEREFKelley1960" class="citation journal cs1"><a href="/wiki/Henry_J._Kelley" title="Henry J. Kelley">Kelley, Henry J.</a> (1960). "Gradient theory of optimal flight paths". <i>ARS Journal</i>. <b>30</b> (10): <span class="nowrap">947–</span>954. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.2514%2F8.5282">10.2514/8.5282</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ARS+Journal&amp;rft.atitle=Gradient+theory+of+optimal+flight+paths&amp;rft.volume=30&amp;rft.issue=10&amp;rft.pages=947-954&amp;rft.date=1960&amp;rft&#95;id=info%3Adoi%2F10.2514%2F8.5282&amp;rft.aulast=Kelley&amp;rft.aufirst=Henry+J.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-bryson1961-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-bryson1961_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bryson1961_2-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBryson1962" class="citation book cs1">Bryson, Arthur E. (1962). "A gradient method for optimizing multi-stage allocation processes". <i>Proceedings of the Harvard Univ. Symposium on digital computers and their applications, 3–6 April 1961</i>. Cambridge: Harvard University Press. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/oclc/498866871">498866871</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+gradient+method+for+optimizing+multi-stage+allocation+processes&amp;rft.btitle=Proceedings+of+the+Harvard+Univ.+Symposium+on+digital+computers+and+their+applications%2C+3%E2%80%936+April+1961&amp;rft.place=Cambridge&amp;rft.pub=Harvard+University+Press&amp;rft.date=1962&amp;rft&#95;id=info%3Aoclcnum%2F498866871&amp;rft.aulast=Bryson&amp;rft.aufirst=Arthur+E.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-FOOTNOTEGoodfellowBengioCourville2016&#91;httpswwwdeeplearningbookorgcontentsmlphtmlpf33&#95;214&#93;-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-FOOTNOTEGoodfellowBengioCourville2016[httpswwwdeeplearningbookorgcontentsmlphtmlpf33_214]_3-0">^</a></b></span> <span class="reference-text"><a href="#CITEREFGoodfellowBengioCourville2016">Goodfellow, Bengio &amp; Courville 2016</a>, p.&#160;<a rel="nofollow" class="external text" href="https://www.deeplearningbook.org/contents/mlp.html#pf33">214</a>, "This table-filling strategy is sometimes called <i>dynamic programming</i>."</span>
</li>
<li id="cite&#95;note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><a href="#CITEREFGoodfellowBengioCourville2016">Goodfellow, Bengio &amp; Courville 2016</a>, p.&#160;<a rel="nofollow" class="external text" href="https://www.deeplearningbook.org/contents/mlp.html#pf25">200</a>, "The term back-propagation is often misunderstood as meaning the whole learning algorithm for multilayer neural networks. Backpropagation refers only to the method for computing the gradient, while other algorithms, such as stochastic gradient descent, is used to perform learning using this gradient."</span>
</li>
<li id="cite&#95;note-DL-reverse-mode-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-DL-reverse-mode_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-DL-reverse-mode_5-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a href="#CITEREFGoodfellowBengioCourville2016">Goodfellow, Bengio &amp; Courville (2016</a>, p.&#160;<a rel="nofollow" class="external text" href="https://www.deeplearningbook.org/contents/mlp.html#pf36">217</a>–218), "The back-propagation algorithm described here is only one approach to automatic differentiation. It is a special case of a broader class of techniques called <i>reverse mode accumulation</i>."</span>
</li>
<li id="cite&#95;note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRamachandranZophLe2017" class="citation arxiv cs1">Ramachandran, Prajit; Zoph, Barret; Le, Quoc V. (2017-10-27). "Searching for Activation Functions". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1710.05941">1710.05941</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.NE">cs.NE</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Searching+for+Activation+Functions&amp;rft.date=2017-10-27&amp;rft&#95;id=info%3Aarxiv%2F1710.05941&amp;rft.aulast=Ramachandran&amp;rft.aufirst=Prajit&amp;rft.au=Zoph%2C+Barret&amp;rft.au=Le%2C+Quoc+V.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFMisra2019" class="citation arxiv cs1">Misra, Diganta (2019-08-23). "Mish: A Self Regularized Non-Monotonic Activation Function". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1908.08681">1908.08681</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Mish%3A+A+Self+Regularized+Non-Monotonic+Activation+Function&amp;rft.date=2019-08-23&amp;rft&#95;id=info%3Aarxiv%2F1908.08681&amp;rft.aulast=Misra&amp;rft.aufirst=Diganta&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-RumelhartHintonWilliams1986a-14"><span class="mw-cite-backlink">^ <a href="#cite_ref-RumelhartHintonWilliams1986a_14-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-RumelhartHintonWilliams1986a_14-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-RumelhartHintonWilliams1986a_14-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRumelhartHintonWilliams1986a" class="citation journal cs1"><a href="/wiki/David_E._Rumelhart" class="mw-redirect" title="David E. Rumelhart">Rumelhart, David E.</a>; <a href="/wiki/Geoffrey_E._Hinton" class="mw-redirect" title="Geoffrey E. Hinton">Hinton, Geoffrey E.</a>; <a href="/wiki/Ronald_J._Williams" title="Ronald J. Williams">Williams, Ronald J.</a> (1986a). "Learning representations by back-propagating errors". <i>Nature</i>. <b>323</b> (6088): <span class="nowrap">533–</span>536. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/1986Natur.323..533R">1986Natur.323..533R</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2F323533a0">10.1038/323533a0</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:205001834">205001834</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Learning+representations+by+back-propagating+errors&amp;rft.volume=323&amp;rft.issue=6088&amp;rft.pages=533-536&amp;rft.date=1986&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A205001834%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1038%2F323533a0&amp;rft&#95;id=info%3Abibcode%2F1986Natur.323..533R&amp;rft.aulast=Rumelhart&amp;rft.aufirst=David+E.&amp;rft.au=Hinton%2C+Geoffrey+E.&amp;rft.au=Williams%2C+Ronald+J.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Tan2018-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-Tan2018_18-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFTanLim2019" class="citation journal cs1">Tan, Hong Hui; Lim, King Han (2019). <a rel="nofollow" class="external text" href="https://doi.org/10.1088%2F1757-899X%2F495%2F1%2F012003">"Review of second-order optimization techniques in artificial neural networks backpropagation"</a>. <i>IOP Conference Series: Materials Science and Engineering</i>. <b>495</b> (1) 012003. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2019MS&amp;E..495a2003T">2019MS&#38;E..495a2003T</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1088%2F1757-899X%2F495%2F1%2F012003">10.1088/1757-899X/495/1/012003</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:208124487">208124487</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IOP+Conference+Series%3A+Materials+Science+and+Engineering&amp;rft.atitle=Review+of+second-order+optimization+techniques+in+artificial+neural+networks+backpropagation&amp;rft.volume=495&amp;rft.issue=1&amp;rft.artnum=012003&amp;rft.date=2019&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A208124487%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1088%2F1757-899X%2F495%2F1%2F012003&amp;rft&#95;id=info%3Abibcode%2F2019MS%26E..495a2003T&amp;rft.aulast=Tan&amp;rft.aufirst=Hong+Hui&amp;rft.au=Lim%2C+King+Han&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.1088%252F1757-899X%252F495%252F1%252F012003&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Wiliamowski2010-19"><span class="mw-cite-backlink">^ <a href="#cite_ref-Wiliamowski2010_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Wiliamowski2010_19-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFWiliamowskiYu2010" class="citation journal cs1">Wiliamowski, Bogdan; Yu, Hao (June 2010). <a rel="nofollow" class="external text" href="https://www.eng.auburn.edu/~wilambm/pap/2010/Improved%20Computation%20for%20LM%20Training.pdf">"Improved Computation for Levenberg–Marquardt Training"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Neural Networks and Learning Systems</i>. <b>21</b> (6): 930. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2010ITNN...21..930W">2010ITNN...21..930W</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTNN.2010.2045657">10.1109/TNN.2010.2045657</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Neural+Networks+and+Learning+Systems&amp;rft.atitle=Improved+Computation+for+Levenberg%E2%80%93Marquardt+Training&amp;rft.volume=21&amp;rft.issue=6&amp;rft.pages=930&amp;rft.date=2010-06&amp;rft&#95;id=info%3Adoi%2F10.1109%2FTNN.2010.2045657&amp;rft&#95;id=info%3Abibcode%2F2010ITNN...21..930W&amp;rft.aulast=Wiliamowski&amp;rft.aufirst=Bogdan&amp;rft.au=Yu%2C+Hao&amp;rft&#95;id=https%3A%2F%2Fwww.eng.auburn.edu%2F~wilambm%2Fpap%2F2010%2FImproved%2520Computation%2520for%2520LM%2520Training.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Martens2020-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-Martens2020_20-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFMartens2020" class="citation journal cs1">Martens, James (August 2020). "New Insights and Perspectives on the Natural Gradient Method". <i>Journal of Machine Learning Research</i> (21). <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1412.1193">1412.1193</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=New+Insights+and+Perspectives+on+the+Natural+Gradient+Method&amp;rft.issue=21&amp;rft.date=2020-08&amp;rft&#95;id=info%3Aarxiv%2F1412.1193&amp;rft.aulast=Martens&amp;rft.aufirst=James&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><a href="#CITEREFNielsen2015">Nielsen (2015)</a>, "[W]hat assumptions do we need to make about our cost function ... in order that backpropagation can be applied? The first assumption we need is that the cost function can be written as an average ... over cost functions ... for individual training examples ... The second assumption we make about the cost is that it can be written as a function of the outputs from the neural network ..."</span>
</li>
<li id="cite&#95;note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLeCunBengioHinton2015" class="citation journal cs1"><a href="/wiki/Yann_LeCun" title="Yann LeCun">LeCun, Yann</a>; Bengio, Yoshua; Hinton, Geoffrey (2015). <a rel="nofollow" class="external text" href="https://hal.science/hal-04206682/file/Lecun2015.pdf">"Deep learning"</a> <span class="cs1-format">(PDF)</span>. <i>Nature</i>. <b>521</b> (7553): <span class="nowrap">436–</span>444. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2015Natur.521..436L">2015Natur.521..436L</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fnature14539">10.1038/nature14539</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/26017442">26017442</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:3074096">3074096</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Deep+learning&amp;rft.volume=521&amp;rft.issue=7553&amp;rft.pages=436-444&amp;rft.date=2015&amp;rft&#95;id=info%3Adoi%2F10.1038%2Fnature14539&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A3074096%23id-name%3DS2CID&amp;rft&#95;id=info%3Apmid%2F26017442&amp;rft&#95;id=info%3Abibcode%2F2015Natur.521..436L&amp;rft.aulast=LeCun&amp;rft.aufirst=Yann&amp;rft.au=Bengio%2C+Yoshua&amp;rft.au=Hinton%2C+Geoffrey&amp;rft&#95;id=https%3A%2F%2Fhal.science%2Fhal-04206682%2Ffile%2FLecun2015.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBucklandCollins2002" class="citation book cs1">Buckland, Matt; Collins, Mark (2002). <i>AI Techniques for Game Programming</i>. Boston: Premier Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1-931841-08-X" title="Special:BookSources/1-931841-08-X"><bdi>1-931841-08-X</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=AI+Techniques+for+Game+Programming&amp;rft.place=Boston&amp;rft.pub=Premier+Press&amp;rft.date=2002&amp;rft.isbn=1-931841-08-X&amp;rft.aulast=Buckland&amp;rft.aufirst=Matt&amp;rft.au=Collins%2C+Mark&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-leibniz1676-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-leibniz1676_24-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLeibniz1920" class="citation book cs1"><a href="/wiki/Gottfried_Wilhelm_Leibniz" title="Gottfried Wilhelm Leibniz">Leibniz, Gottfried Wilhelm Freiherr von</a> (1920). <a rel="nofollow" class="external text" href="https://books.google.com/books?id=bOIGAAAAYAAJ&amp;q=leibniz+altered+manuscripts&amp;pg=PA90"><i>The Early Mathematical Manuscripts of Leibniz: Translated from the Latin Texts Published by Carl Immanuel Gerhardt with Critical and Historical Notes (Leibniz published the chain rule in a 1676 memoir)</i></a>. Open court publishing Company. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-598-81846-1" title="Special:BookSources/978-0-598-81846-1"><bdi>978-0-598-81846-1</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Early+Mathematical+Manuscripts+of+Leibniz%3A+Translated+from+the+Latin+Texts+Published+by+Carl+Immanuel+Gerhardt+with+Critical+and+Historical+Notes+%28Leibniz+published+the+chain+rule+in+a+1676+memoir%29&amp;rft.pub=Open+court+publishing+Company&amp;rft.date=1920&amp;rft.isbn=978-0-598-81846-1&amp;rft.aulast=Leibniz&amp;rft.aufirst=Gottfried+Wilhelm+Freiherr+von&amp;rft&#95;id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DbOIGAAAAYAAJ%26q%3Dleibniz%2Baltered%2Bmanuscripts%26pg%3DPA90&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span> <span class="cs1-hidden-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_book" title="Template:Cite book">cite book</a>}}</code>: </span><span class="cs1-hidden-error citation-comment">ISBN / Date incompatibility (<a href="/wiki/Help:CS1_errors#invalid_isbn_date" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite&#95;note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRodríguezLópez&#95;Fernández2010" class="citation journal cs1">Rodríguez, Omar Hernández; López Fernández, Jorge M. (2010). <a rel="nofollow" class="external text" href="https://scholarworks.umt.edu/tme/vol7/iss2/10/">"A Semiotic Reflection on the Didactics of the Chain Rule"</a>. <i>The Mathematics Enthusiast</i>. <b>7</b> (2): <span class="nowrap">321–</span>332. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.54870%2F1551-3440.1191">10.54870/1551-3440.1191</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:29739148">29739148</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-08-04</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Mathematics+Enthusiast&amp;rft.atitle=A+Semiotic+Reflection+on+the+Didactics+of+the+Chain+Rule&amp;rft.volume=7&amp;rft.issue=2&amp;rft.pages=321-332&amp;rft.date=2010&amp;rft&#95;id=info%3Adoi%2F10.54870%2F1551-3440.1191&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A29739148%23id-name%3DS2CID&amp;rft.aulast=Rodr%C3%ADguez&amp;rft.aufirst=Omar+Hern%C3%A1ndez&amp;rft.au=L%C3%B3pez+Fern%C3%A1ndez%2C+Jorge+M.&amp;rft&#95;id=https%3A%2F%2Fscholarworks.umt.edu%2Ftme%2Fvol7%2Fiss2%2F10%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRosenblatt1962" class="citation book cs1"><a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Rosenblatt, Frank</a> (1962). <i>Principles of Neurodynamics</i>. Spartan, New York. pp.&#160;<span class="nowrap">287–</span>298.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Principles+of+Neurodynamics&amp;rft.pages=287-298&amp;rft.pub=Spartan%2C+New+York&amp;rft.date=1962&amp;rft.aulast=Rosenblatt&amp;rft.aufirst=Frank&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text">LeCun, Yann, et al. "A theoretical framework for back-propagation." <i>Proceedings of the 1988 connectionist models summer school</i>. Vol. 1. 1988.</span>
</li>
<li id="cite&#95;note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFHecht-Nielsen1990" class="citation book cs1">Hecht-Nielsen, Robert (1990). <a rel="nofollow" class="external text" href="http://archive.org/details/neurocomputing0000hech"><i>Neurocomputing</i></a>. Internet Archive. Reading, Mass.&#160;: Addison-Wesley Pub. Co. pp.&#160;<span class="nowrap">124–</span>125. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-201-09355-1" title="Special:BookSources/978-0-201-09355-1"><bdi>978-0-201-09355-1</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Neurocomputing&amp;rft.pages=124-125&amp;rft.pub=Reading%2C+Mass.+%3A+Addison-Wesley+Pub.+Co.&amp;rft.date=1990&amp;rft.isbn=978-0-201-09355-1&amp;rft.aulast=Hecht-Nielsen&amp;rft.aufirst=Robert&amp;rft&#95;id=http%3A%2F%2Farchive.org%2Fdetails%2Fneurocomputing0000hech&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-robbins1951-29"><span class="mw-cite-backlink">^ <a href="#cite_ref-robbins1951_29-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-robbins1951_29-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRobbinsMonro1951" class="citation journal cs1"><a href="/wiki/Herbert_Robbins" title="Herbert Robbins">Robbins, H.</a>; Monro, S. (1951). <a rel="nofollow" class="external text" href="https://doi.org/10.1214%2Faoms%2F1177729586">"A Stochastic Approximation Method"</a>. <i>The Annals of Mathematical Statistics</i>. <b>22</b> (3): 400. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1214%2Faoms%2F1177729586">10.1214/aoms/1177729586</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Annals+of+Mathematical+Statistics&amp;rft.atitle=A+Stochastic+Approximation+Method&amp;rft.volume=22&amp;rft.issue=3&amp;rft.pages=400&amp;rft.date=1951&amp;rft&#95;id=info%3Adoi%2F10.1214%2Faoms%2F1177729586&amp;rft.aulast=Robbins&amp;rft.aufirst=H.&amp;rft.au=Monro%2C+S.&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.1214%252Faoms%252F1177729586&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFDreyfus1962" class="citation journal cs1">Dreyfus, Stuart (1962). <a rel="nofollow" class="external text" href="https://doi.org/10.1016%2F0022-247x%2862%2990004-5">"The numerical solution of variational problems"</a>. <i>Journal of Mathematical Analysis and Applications</i>. <b>5</b> (1): <span class="nowrap">30–</span>45. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1016%2F0022-247x%2862%2990004-5">10.1016/0022-247x(62)90004-5</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Mathematical+Analysis+and+Applications&amp;rft.atitle=The+numerical+solution+of+variational+problems&amp;rft.volume=5&amp;rft.issue=1&amp;rft.pages=30-45&amp;rft.date=1962&amp;rft&#95;id=info%3Adoi%2F10.1016%2F0022-247x%2862%2990004-5&amp;rft.aulast=Dreyfus&amp;rft.aufirst=Stuart&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.1016%252F0022-247x%252862%252990004-5&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-dreyfus1990-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-dreyfus1990_31-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFDreyfus1990" class="citation journal cs1"><a href="/wiki/Stuart_Dreyfus" title="Stuart Dreyfus">Dreyfus, Stuart E.</a> (1990). "Artificial Neural Networks, Back Propagation, and the Kelley-Bryson Gradient Procedure". <i>Journal of Guidance, Control, and Dynamics</i>. <b>13</b> (5): <span class="nowrap">926–</span>928. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/1990JGCD...13..926D">1990JGCD...13..926D</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.2514%2F3.25422">10.2514/3.25422</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Guidance%2C+Control%2C+and+Dynamics&amp;rft.atitle=Artificial+Neural+Networks%2C+Back+Propagation%2C+and+the+Kelley-Bryson+Gradient+Procedure&amp;rft.volume=13&amp;rft.issue=5&amp;rft.pages=926-928&amp;rft.date=1990&amp;rft&#95;id=info%3Adoi%2F10.2514%2F3.25422&amp;rft&#95;id=info%3Abibcode%2F1990JGCD...13..926D&amp;rft.aulast=Dreyfus&amp;rft.aufirst=Stuart+E.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFMizutaniDreyfusNishio2000" class="citation web cs1">Mizutani, Eiji; Dreyfus, Stuart; Nishio, Kenichi (July 2000). <a rel="nofollow" class="external text" href="https://coeieor.wpengine.com/wp-content/uploads/2019/03/ijcnn2k.pdf">"On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application"</a> <span class="cs1-format">(PDF)</span>. Proceedings of the IEEE International Joint Conference on Neural Networks.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=On+derivation+of+MLP+backpropagation+from+the+Kelley-Bryson+optimal-control+gradient+formula+and+its+application&amp;rft.pub=Proceedings+of+the+IEEE+International+Joint+Conference+on+Neural+Networks&amp;rft.date=2000-07&amp;rft.aulast=Mizutani&amp;rft.aufirst=Eiji&amp;rft.au=Dreyfus%2C+Stuart&amp;rft.au=Nishio%2C+Kenichi&amp;rft&#95;id=https%3A%2F%2Fcoeieor.wpengine.com%2Fwp-content%2Fuploads%2F2019%2F03%2Fijcnn2k.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-dreyfus1973-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-dreyfus1973_33-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFDreyfus1973" class="citation journal cs1"><a href="/wiki/Stuart_Dreyfus" title="Stuart Dreyfus">Dreyfus, Stuart</a> (1973). "The computational solution of optimal control problems with time lag". <i>IEEE Transactions on Automatic Control</i>. <b>18</b> (4): <span class="nowrap">383–</span>385. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2Ftac.1973.1100330">10.1109/tac.1973.1100330</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Automatic+Control&amp;rft.atitle=The+computational+solution+of+optimal+control+problems+with+time+lag&amp;rft.volume=18&amp;rft.issue=4&amp;rft.pages=383-385&amp;rft.date=1973&amp;rft&#95;id=info%3Adoi%2F10.1109%2Ftac.1973.1100330&amp;rft.aulast=Dreyfus&amp;rft.aufirst=Stuart&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-DLhistory-34"><span class="mw-cite-backlink">^ <a href="#cite_ref-DLhistory_34-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-DLhistory_34-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSchmidhuber2022" class="citation arxiv cs1"><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Schmidhuber, Jürgen</a> (2022). "Annotated History of Modern AI and Deep Learning". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2212.11279">2212.11279</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.NE">cs.NE</a>].</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Annotated+History+of+Modern+AI+and+Deep+Learning&amp;rft.date=2022&amp;rft&#95;id=info%3Aarxiv%2F2212.11279&amp;rft.aulast=Schmidhuber&amp;rft.aufirst=J%C3%BCrgen&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Amari1967-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-Amari1967_35-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFAmari1967" class="citation journal cs1"><a href="/wiki/Shun%27ichi_Amari" title="Shun&#39;ichi Amari">Amari, Shun'ichi</a> (1967). "A theory of adaptive pattern classifier". <i>IEEE Transactions</i>. <b>EC</b> (16): <span class="nowrap">279–</span>307.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions&amp;rft.atitle=A+theory+of+adaptive+pattern+classifier&amp;rft.volume=EC&amp;rft.issue=16&amp;rft.pages=279-307&amp;rft.date=1967&amp;rft.aulast=Amari&amp;rft.aufirst=Shun%27ichi&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-lin1970-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-lin1970_36-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLinnainmaa1970" class="citation thesis cs1 cs1-prop-foreign-lang-source"><a href="/wiki/Seppo_Linnainmaa" title="Seppo Linnainmaa">Linnainmaa, Seppo</a> (1970). <i>The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors</i> (Masters) (in Finnish). University of Helsinki. pp.&#160;<span class="nowrap">6–</span>7.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=The+representation+of+the+cumulative+rounding+error+of+an+algorithm+as+a+Taylor+expansion+of+the+local+rounding+errors&amp;rft.inst=University+of+Helsinki&amp;rft.date=1970&amp;rft.aulast=Linnainmaa&amp;rft.aufirst=Seppo&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-lin1976-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-lin1976_37-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLinnainmaa1976" class="citation journal cs1"><a href="/wiki/Seppo_Linnainmaa" title="Seppo Linnainmaa">Linnainmaa, Seppo</a> (1976). "Taylor expansion of the accumulated rounding error". <i>BIT Numerical Mathematics</i>. <b>16</b> (2): <span class="nowrap">146–</span>160. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fbf01931367">10.1007/bf01931367</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:122357351">122357351</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BIT+Numerical+Mathematics&amp;rft.atitle=Taylor+expansion+of+the+accumulated+rounding+error&amp;rft.volume=16&amp;rft.issue=2&amp;rft.pages=146-160&amp;rft.date=1976&amp;rft&#95;id=info%3Adoi%2F10.1007%2Fbf01931367&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A122357351%23id-name%3DS2CID&amp;rft.aulast=Linnainmaa&amp;rft.aufirst=Seppo&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-grie2012-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-grie2012_38-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFGriewank2012" class="citation book cs1">Griewank, Andreas (2012). "Who Invented the Reverse Mode of Differentiation?". <i>Optimization Stories</i>. Documenta Mathematica, Extra Volume ISMP. pp.&#160;<span class="nowrap">389–</span>400. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:15568746">15568746</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Who+Invented+the+Reverse+Mode+of+Differentiation%3F&amp;rft.btitle=Optimization+Stories&amp;rft.series=Documenta+Mathematica%2C+Extra+Volume+ISMP&amp;rft.pages=389-400&amp;rft.date=2012&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A15568746%23id-name%3DS2CID&amp;rft.aulast=Griewank&amp;rft.aufirst=Andreas&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-grie2008-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-grie2008_39-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFGriewankWalther2008" class="citation book cs1">Griewank, Andreas; <a href="/wiki/Andrea_Walther" title="Andrea Walther">Walther, Andrea</a> (2008). <a rel="nofollow" class="external text" href="https://books.google.com/books?id=xoiiLaRxcbEC"><i>Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition</i></a>. SIAM. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-89871-776-1" title="Special:BookSources/978-0-89871-776-1"><bdi>978-0-89871-776-1</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Evaluating+Derivatives%3A+Principles+and+Techniques+of+Algorithmic+Differentiation%2C+Second+Edition&amp;rft.pub=SIAM&amp;rft.date=2008&amp;rft.isbn=978-0-89871-776-1&amp;rft.aulast=Griewank&amp;rft.aufirst=Andreas&amp;rft.au=Walther%2C+Andrea&amp;rft&#95;id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DxoiiLaRxcbEC&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-werbos1982-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-werbos1982_40-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFWerbos1982" class="citation book cs1"><a href="/wiki/Paul_Werbos" title="Paul Werbos">Werbos, Paul</a> (1982). <a rel="nofollow" class="external text" href="http://werbos.com/Neural/SensitivityIFIPSeptember1981.pdf">"Applications of advances in nonlinear sensitivity analysis"</a> <span class="cs1-format">(PDF)</span>. <i>System modeling and optimization</i>. Springer. pp.&#160;<span class="nowrap">762–</span>770. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20160414055503/http://werbos.com/Neural/SensitivityIFIPSeptember1981.pdf">Archived</a> <span class="cs1-format">(PDF)</span> from the original on 14 April 2016<span class="reference-accessdate">. Retrieved <span class="nowrap">2 July</span> 2017</span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Applications+of+advances+in+nonlinear+sensitivity+analysis&amp;rft.btitle=System+modeling+and+optimization&amp;rft.pages=762-770&amp;rft.pub=Springer&amp;rft.date=1982&amp;rft.aulast=Werbos&amp;rft.aufirst=Paul&amp;rft&#95;id=http%3A%2F%2Fwerbos.com%2FNeural%2FSensitivityIFIPSeptember1981.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-werbos1974-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-werbos1974_41-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFWerbos1994" class="citation book cs1">Werbos, Paul J. (1994). <i>The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting</i>. New York: John Wiley &amp; Sons. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-471-59897-6" title="Special:BookSources/0-471-59897-6"><bdi>0-471-59897-6</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Roots+of+Backpropagation%3A+From+Ordered+Derivatives+to+Neural+Networks+and+Political+Forecasting&amp;rft.place=New+York&amp;rft.pub=John+Wiley+%26+Sons&amp;rft.date=1994&amp;rft.isbn=0-471-59897-6&amp;rft.aulast=Werbos&amp;rft.aufirst=Paul+J.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:1-42"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_42-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_42-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:1_42-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:1_42-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFAndersonRosenfeld2000" class="citation book cs1">Anderson, James A.; Rosenfeld, Edward, eds. (2000). <a rel="nofollow" class="external text" href="https://direct.mit.edu/books/book/4886/Talking-NetsAn-Oral-History-of-Neural-Networks"><i>Talking Nets: An Oral History of Neural Networks</i></a>. The MIT Press. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.7551%2Fmitpress%2F6626.003.0016">10.7551/mitpress/6626.003.0016</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-262-26715-1" title="Special:BookSources/978-0-262-26715-1"><bdi>978-0-262-26715-1</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Talking+Nets%3A+An+Oral+History+of+Neural+Networks&amp;rft.pub=The+MIT+Press&amp;rft.date=2000&amp;rft&#95;id=info%3Adoi%2F10.7551%2Fmitpress%2F6626.003.0016&amp;rft.isbn=978-0-262-26715-1&amp;rft&#95;id=https%3A%2F%2Fdirect.mit.edu%2Fbooks%2Fbook%2F4886%2FTalking-NetsAn-Oral-History-of-Neural-Networks&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text">P. J. Werbos, "Backpropagation through time: what it does and how to do it," in Proceedings of the IEEE, vol. 78, no. 10, pp. 1550–1560, Oct. 1990, <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2F5.58337">10.1109/5.58337</a></span>
</li>
<li id="cite&#95;note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text">Olazaran Rodriguez, Jose Miguel. <i><a rel="nofollow" class="external text" href="https://web.archive.org/web/20221111165150/https://era.ed.ac.uk/bitstream/handle/1842/20075/Olazaran-RodriguezJM_1991redux.pdf?sequence=1&amp;isAllowed=y">A historical sociology of neural network research</a></i>. PhD Dissertation. University of Edinburgh, 1991.</span>
</li>
<li id="cite&#95;note-learning-representations-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-learning-representations_45-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRumelhartHintonWilliams1986" class="citation journal cs1">Rumelhart; Hinton; Williams (1986). <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">"Learning representations by back-propagating errors"</a> <span class="cs1-format">(PDF)</span>. <i>Nature</i>. <b>323</b> (6088): <span class="nowrap">533–</span>536. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/1986Natur.323..533R">1986Natur.323..533R</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2F323533a0">10.1038/323533a0</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:205001834">205001834</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Learning+representations+by+back-propagating+errors&amp;rft.volume=323&amp;rft.issue=6088&amp;rft.pages=533-536&amp;rft.date=1986&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A205001834%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1038%2F323533a0&amp;rft&#95;id=info%3Abibcode%2F1986Natur.323..533R&amp;rft.au=Rumelhart&amp;rft.au=Hinton&amp;rft.au=Williams&amp;rft&#95;id=http%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2Fnaturebp.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-RumelhartHintonWilliams1986b-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-RumelhartHintonWilliams1986b_46-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRumelhartHintonWilliams1986b" class="citation book cs1 cs1-prop-long-vol"><a href="/wiki/David_E._Rumelhart" class="mw-redirect" title="David E. Rumelhart">Rumelhart, David E.</a>; <a href="/wiki/Geoffrey_E._Hinton" class="mw-redirect" title="Geoffrey E. Hinton">Hinton, Geoffrey E.</a>; <a href="/wiki/Ronald_J._Williams" title="Ronald J. Williams">Williams, Ronald J.</a> (1986b). <span class="id-lock-registration" title="Free registration required"><a rel="nofollow" class="external text" href="https://archive.org/details/paralleldistribu00rume">"8. Learning Internal Representations by Error Propagation"</a></span>. In <a href="/wiki/David_E._Rumelhart" class="mw-redirect" title="David E. Rumelhart">Rumelhart, David E.</a>; <a href="/wiki/James_McClelland_(psychologist)" title="James McClelland (psychologist)">McClelland, James L.</a> (eds.). <i>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</i>. Vol.&#160;1&#160;: Foundations. Cambridge: MIT Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-262-18120-7" title="Special:BookSources/0-262-18120-7"><bdi>0-262-18120-7</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=8.+Learning+Internal+Representations+by+Error+Propagation&amp;rft.btitle=Parallel+Distributed+Processing%3A+Explorations+in+the+Microstructure+of+Cognition&amp;rft.place=Cambridge&amp;rft.pub=MIT+Press&amp;rft.date=1986&amp;rft.isbn=0-262-18120-7&amp;rft.aulast=Rumelhart&amp;rft.aufirst=David+E.&amp;rft.au=Hinton%2C+Geoffrey+E.&amp;rft.au=Williams%2C+Ronald+J.&amp;rft&#95;id=https%3A%2F%2Farchive.org%2Fdetails%2Fparalleldistribu00rume&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFAlpaydin2010" class="citation book cs1">Alpaydin, Ethem (2010). <a rel="nofollow" class="external text" href="https://books.google.com/books?id=4j9GAQAAIAAJ"><i>Introduction to Machine Learning</i></a>. MIT Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-262-01243-0" title="Special:BookSources/978-0-262-01243-0"><bdi>978-0-262-01243-0</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Machine+Learning&amp;rft.pub=MIT+Press&amp;rft.date=2010&amp;rft.isbn=978-0-262-01243-0&amp;rft.aulast=Alpaydin&amp;rft.aufirst=Ethem&amp;rft&#95;id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3D4j9GAQAAIAAJ&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFParker1985" class="citation report cs1">Parker, D.B. (1985). Learning Logic: Casting the Cortex of the Human Brain in Silicon. Center for Computational Research in Economics and Management Science (Report). Cambridge MA: Massachusetts Institute of Technology. Technical Report TR-47.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Learning+Logic%3A+Casting+the+Cortex+of+the+Human+Brain+in+Silicon&amp;rft.place=Cambridge+MA&amp;rft.pub=Massachusetts+Institute+of+Technology&amp;rft.date=1985&amp;rft.aulast=Parker&amp;rft.aufirst=D.B.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:0-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-:0_49-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFHertz1991" class="citation book cs1">Hertz, John (1991). <i>Introduction to the theory of neural computation</i>. Krogh, Anders., Palmer, Richard G. Redwood City, Calif.: Addison-Wesley. p.&#160;8. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-201-50395-6" title="Special:BookSources/0-201-50395-6"><bdi>0-201-50395-6</bdi></a>. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/oclc/21522159">21522159</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+the+theory+of+neural+computation&amp;rft.place=Redwood+City%2C+Calif.&amp;rft.pages=8&amp;rft.pub=Addison-Wesley&amp;rft.date=1991&amp;rft&#95;id=info%3Aoclcnum%2F21522159&amp;rft.isbn=0-201-50395-6&amp;rft.aulast=Hertz&amp;rft.aufirst=John&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-50">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFLe&#95;Cun1987" class="citation thesis cs1">Le Cun, Yann (1987). <a rel="nofollow" class="external text" href="https://www.sudoc.fr/043586643"><i>Modèles connexionnistes de l'apprentissage</i></a> (Thèse de doctorat d'état thesis). Paris, France: Université Pierre et Marie Curie.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Mod%C3%A8les+connexionnistes+de+l%27apprentissage&amp;rft.degree=Th%C3%A8se+de+doctorat+d%27%C3%A9tat&amp;rft.inst=Universit%C3%A9+Pierre+et+Marie+Curie&amp;rft.date=1987&amp;rft.aulast=Le+Cun&amp;rft.aufirst=Yann&amp;rft&#95;id=https%3A%2F%2Fwww.sudoc.fr%2F043586643&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.nobelprize.org/prizes/physics/2024/press-release/">"The Nobel Prize in Physics 2024"</a>. <i>NobelPrize.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-10-13</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=NobelPrize.org&amp;rft.atitle=The+Nobel+Prize+in+Physics+2024&amp;rft&#95;id=https%3A%2F%2Fwww.nobelprize.org%2Fprizes%2Fphysics%2F2024%2Fpress-release%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:02-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-:02_52-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSejnowski2018" class="citation book cs1">Sejnowski, Terrence J. (2018). <i>The deep learning revolution</i>. Cambridge, Massachusetts London, England: The MIT Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-262-03803-4" title="Special:BookSources/978-0-262-03803-4"><bdi>978-0-262-03803-4</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+deep+learning+revolution&amp;rft.place=Cambridge%2C+Massachusetts+London%2C+England&amp;rft.pub=The+MIT+Press&amp;rft.date=2018&amp;rft.isbn=978-0-262-03803-4&amp;rft.aulast=Sejnowski&amp;rft.aufirst=Terrence+J.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-53">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFPomerleau1988" class="citation journal cs1">Pomerleau, Dean A. (1988). <a rel="nofollow" class="external text" href="https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html">"ALVINN: An Autonomous Land Vehicle in a Neural Network"</a>. <i>Advances in Neural Information Processing Systems</i>. <b>1</b>. Morgan-Kaufmann.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=ALVINN%3A+An+Autonomous+Land+Vehicle+in+a+Neural+Network&amp;rft.volume=1&amp;rft.date=1988&amp;rft.aulast=Pomerleau&amp;rft.aufirst=Dean+A.&amp;rft&#95;id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F1988%2Fhash%2F812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSuttonBarto2018" class="citation book cs1">Sutton, Richard S.; Barto, Andrew G. (2018). <a rel="nofollow" class="external text" href="http://www.incompleteideas.net/book/11/node2.html">"11.1 TD-Gammon"</a>. <i>Reinforcement Learning: An Introduction</i> (2nd&#160;ed.). Cambridge, MA: MIT Press.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=11.1+TD-Gammon&amp;rft.btitle=Reinforcement+Learning%3A+An+Introduction&amp;rft.place=Cambridge%2C+MA&amp;rft.edition=2nd&amp;rft.pub=MIT+Press&amp;rft.date=2018&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard+S.&amp;rft.au=Barto%2C+Andrew+G.&amp;rft&#95;id=http%3A%2F%2Fwww.incompleteideas.net%2Fbook%2F11%2Fnode2.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-schmidhuber2015-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-schmidhuber2015_55-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSchmidhuber2015" class="citation journal cs1"><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Schmidhuber, Jürgen</a> (2015). "Deep learning in neural networks: An overview". <i>Neural Networks</i>. <b>61</b>: <span class="nowrap">85–</span>117. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1404.7828">1404.7828</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neunet.2014.09.003">10.1016/j.neunet.2014.09.003</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/25462637">25462637</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:11715509">11715509</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=Deep+learning+in+neural+networks%3A+An+overview&amp;rft.volume=61&amp;rft.pages=85-117&amp;rft.date=2015&amp;rft&#95;id=info%3Aarxiv%2F1404.7828&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A11715509%23id-name%3DS2CID&amp;rft&#95;id=info%3Apmid%2F25462637&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.neunet.2014.09.003&amp;rft.aulast=Schmidhuber&amp;rft.aufirst=J%C3%BCrgen&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFWan1994" class="citation book cs1">Wan, Eric A. (1994). "Time Series Prediction by Using a Connectionist Network with Internal Delay Lines". In <a href="/wiki/Andreas_Weigend" title="Andreas Weigend">Weigend, Andreas S.</a>; <a href="/wiki/Neil_Gershenfeld" title="Neil Gershenfeld">Gershenfeld, Neil A.</a> (eds.). <i>Time Series Prediction: Forecasting the Future and Understanding the Past</i>. Proceedings of the NATO Advanced Research Workshop on Comparative Time Series Analysis. Vol.&#160;15. Reading: Addison-Wesley. pp.&#160;<span class="nowrap">195–</span>217. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-201-62601-2" title="Special:BookSources/0-201-62601-2"><bdi>0-201-62601-2</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:12652643">12652643</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Time+Series+Prediction+by+Using+a+Connectionist+Network+with+Internal+Delay+Lines&amp;rft.btitle=Time+Series+Prediction%3A+Forecasting+the+Future+and+Understanding+the+Past&amp;rft.place=Reading&amp;rft.series=Proceedings+of+the+NATO+Advanced+Research+Workshop+on+Comparative+Time+Series+Analysis&amp;rft.pages=195-217&amp;rft.pub=Addison-Wesley&amp;rft.date=1994&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A12652643%23id-name%3DS2CID&amp;rft.isbn=0-201-62601-2&amp;rft.aulast=Wan&amp;rft.aufirst=Eric+A.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-57">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFChangDellBock2006" class="citation journal cs1">Chang, Franklin; Dell, Gary S.; Bock, Kathryn (2006). "Becoming syntactic". <i>Psychological Review</i>. <b>113</b> (2): <span class="nowrap">234–</span>272. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1037%2F0033-295x.113.2.234">10.1037/0033-295x.113.2.234</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/16637761">16637761</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Psychological+Review&amp;rft.atitle=Becoming+syntactic.&amp;rft.volume=113&amp;rft.issue=2&amp;rft.pages=234-272&amp;rft.date=2006&amp;rft&#95;id=info%3Adoi%2F10.1037%2F0033-295x.113.2.234&amp;rft&#95;id=info%3Apmid%2F16637761&amp;rft.aulast=Chang&amp;rft.aufirst=Franklin&amp;rft.au=Dell%2C+Gary+S.&amp;rft.au=Bock%2C+Kathryn&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-58">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFJanciauskasChang2018" class="citation journal cs1">Janciauskas, Marius; Chang, Franklin (2018). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001481">"Input and Age-Dependent Variation in Second Language Learning: A Connectionist Account"</a>. <i>Cognitive Science</i>. <b>42</b> (Suppl Suppl 2): <span class="nowrap">519–</span>554. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1111%2Fcogs.12519">10.1111/cogs.12519</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001481">6001481</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/28744901">28744901</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognitive+Science&amp;rft.atitle=Input+and+Age-Dependent+Variation+in+Second+Language+Learning%3A+A+Connectionist+Account&amp;rft.volume=42&amp;rft.issue=Suppl+Suppl+2&amp;rft.pages=519-554&amp;rft.date=2018&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6001481%23id-name%3DPMC&amp;rft&#95;id=info%3Apmid%2F28744901&amp;rft&#95;id=info%3Adoi%2F10.1111%2Fcogs.12519&amp;rft.aulast=Janciauskas&amp;rft.aufirst=Marius&amp;rft.au=Chang%2C+Franklin&amp;rft&#95;id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6001481&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-59">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.janbasktraining.com/tutorials/backpropagation-in-deep-learning">"Decoding the Power of Backpropagation: A Deep Dive into Advanced Neural Network Techniques"</a>. <i>janbasktraining.com</i>. 30 January 2024.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=janbasktraining.com&amp;rft.atitle=Decoding+the+Power+of+Backpropagation%3A+A+Deep+Dive+into+Advanced+Neural+Network+Techniques&amp;rft.date=2024-01-30&amp;rft&#95;id=https%3A%2F%2Fwww.janbasktraining.com%2Ftutorials%2Fbackpropagation-in-deep-learning&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-60"><span class="mw-cite-backlink"><b><a href="#cite_ref-60">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFFitzChang2019" class="citation journal cs1">Fitz, Hartmut; Chang, Franklin (2019). "Language ERPs reflect learning through prediction error propagation". <i>Cognitive Psychology</i>. <b>111</b>: <span class="nowrap">15–</span>52. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.cogpsych.2019.03.002">10.1016/j.cogpsych.2019.03.002</a>. <a href="/wiki/Hdl_(identifier)" class="mw-redirect" title="Hdl (identifier)">hdl</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://hdl.handle.net/21.11116%2F0000-0003-474D-8">21.11116/0000-0003-474D-8</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/30921626">30921626</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:85501792">85501792</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognitive+Psychology&amp;rft.atitle=Language+ERPs+reflect+learning+through+prediction+error+propagation&amp;rft.volume=111&amp;rft.pages=15-52&amp;rft.date=2019&amp;rft&#95;id=info%3Ahdl%2F21.11116%2F0000-0003-474D-8&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A85501792%23id-name%3DS2CID&amp;rft&#95;id=info%3Apmid%2F30921626&amp;rft&#95;id=info%3Adoi%2F10.1016%2Fj.cogpsych.2019.03.002&amp;rft.aulast=Fitz&amp;rft.aufirst=Hartmut&amp;rft.au=Chang%2C+Franklin&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-61"><span class="mw-cite-backlink"><b><a href="#cite_ref-61">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://spectrum.ieee.org/backpropagation-optical-ai">"Photonic Chips Curb AI Training's Energy Appetite - IEEE Spectrum"</a>. <i><a href="/wiki/IEEE" class="mw-redirect" title="IEEE">IEEE</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">2023-05-25</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=IEEE&amp;rft.atitle=Photonic+Chips+Curb+AI+Training%27s+Energy+Appetite+-+IEEE+Spectrum&amp;rft&#95;id=https%3A%2F%2Fspectrum.ieee.org%2Fbackpropagation-optical-ai&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></span>
</li>
</ol></div>
<div class="mw-heading mw-heading2"><h2 id="Further_reading">Further reading</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=22" title="Edit section: Further reading"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFGoodfellowBengioCourville2016" class="citation book cs1"><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Goodfellow, Ian</a>; <a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Bengio, Yoshua</a>; Courville, Aaron (2016). <a rel="nofollow" class="external text" href="https://www.deeplearningbook.org/contents/mlp.html#pf25">"6.5 Back-Propagation and Other Differentiation Algorithms"</a>. <a rel="nofollow" class="external text" href="http://www.deeplearningbook.org"><i>Deep Learning</i></a>. MIT Press. pp.&#160;<span class="nowrap">200–</span>220. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-262-03561-3" title="Special:BookSources/978-0-262-03561-3"><bdi>978-0-262-03561-3</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=6.5+Back-Propagation+and+Other+Differentiation+Algorithms&amp;rft.btitle=Deep+Learning&amp;rft.pages=200-220&amp;rft.pub=MIT+Press&amp;rft.date=2016&amp;rft.isbn=978-0-262-03561-3&amp;rft.aulast=Goodfellow&amp;rft.aufirst=Ian&amp;rft.au=Bengio%2C+Yoshua&amp;rft.au=Courville%2C+Aaron&amp;rft&#95;id=https%3A%2F%2Fwww.deeplearningbook.org%2Fcontents%2Fmlp.html%23pf25&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFNielsen2015" class="citation book cs1"><a href="/wiki/Michael_Nielsen" title="Michael Nielsen">Nielsen, Michael A.</a> (2015). <a rel="nofollow" class="external text" href="http://neuralnetworksanddeeplearning.com/chap2.html">"How the backpropagation algorithm works"</a>. <a rel="nofollow" class="external text" href="http://neuralnetworksanddeeplearning.com"><i>Neural Networks and Deep Learning</i></a>. Determination Press.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=How+the+backpropagation+algorithm+works&amp;rft.btitle=Neural+Networks+and+Deep+Learning&amp;rft.pub=Determination+Press&amp;rft.date=2015&amp;rft.aulast=Nielsen&amp;rft.aufirst=Michael+A.&amp;rft&#95;id=http%3A%2F%2Fneuralnetworksanddeeplearning.com%2Fchap2.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFMcCaffrey2012" class="citation web cs1">McCaffrey, James (October 2012). <a rel="nofollow" class="external text" href="https://docs.microsoft.com/en-us/archive/msdn-magazine/2012/october/test-run-neural-network-back-propagation-for-programmers">"Neural Network Back-Propagation for Programmers"</a>. <i><a href="/wiki/MSDN_Magazine" class="mw-redirect" title="MSDN Magazine">MSDN Magazine</a></i>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MSDN+Magazine&amp;rft.atitle=Neural+Network+Back-Propagation+for+Programmers&amp;rft.date=2012-10&amp;rft.aulast=McCaffrey&amp;rft.aufirst=James&amp;rft&#95;id=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Farchive%2Fmsdn-magazine%2F2012%2Foctober%2Ftest-run-neural-network-back-propagation-for-programmers&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRojas1996" class="citation book cs1"><a href="/wiki/Ra%C3%BAl_Rojas" title="Raúl Rojas">Rojas, Raúl</a> (1996). <a rel="nofollow" class="external text" href="https://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf">"The Backpropagation Algorithm"</a> <span class="cs1-format">(PDF)</span>. <i>Neural Networks: A Systematic Introduction</i>. Berlin: Springer. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/3-540-60505-3" title="Special:BookSources/3-540-60505-3"><bdi>3-540-60505-3</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=The+Backpropagation+Algorithm&amp;rft.btitle=Neural+Networks%3A+A+Systematic+Introduction&amp;rft.place=Berlin&amp;rft.pub=Springer&amp;rft.date=1996&amp;rft.isbn=3-540-60505-3&amp;rft.aulast=Rojas&amp;rft.aufirst=Ra%C3%BAl&amp;rft&#95;id=https%3A%2F%2Fpage.mi.fu-berlin.de%2Frojas%2Fneural%2Fchapter%2FK7.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></li></ul>
<div class="mw-heading mw-heading2"><h2 id="External_links">External links</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Backpropagation&amp;action=edit&amp;section=23" title="Edit section: External links"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><a href="https://en.wikiversity.org/wiki/Learning_and_Neural_Networks" class="extiw" title="wikiversity:Learning and Neural Networks">Backpropagation neural network tutorial at the Wikiversity</a></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBernackiWłodarczyk2004" class="citation web cs1">Bernacki, Mariusz; Włodarczyk, Przemysław (2004). <a rel="nofollow" class="external text" href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html">"Principles of training multi-layer neural network using backpropagation"</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Principles+of+training+multi-layer+neural+network+using+backpropagation&amp;rft.date=2004&amp;rft.aulast=Bernacki&amp;rft.aufirst=Mariusz&amp;rft.au=W%C5%82odarczyk%2C+Przemys%C5%82aw&amp;rft&#95;id=http%3A%2F%2Fgalaxy.agh.edu.pl%2F~vlsi%2FAI%2Fbackp&#95;t&#95;en%2Fbackprop.html&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFKarpathy2016" class="citation web cs1"><a href="/wiki/Andrej_Karpathy" title="Andrej Karpathy">Karpathy, Andrej</a> (2016). <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=i94OvYb6noo&amp;list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&amp;index=4">"Lecture 4: Backpropagation, Neural Networks 1"</a>. <i>CS231n</i>. Stanford University. <a rel="nofollow" class="external text" href="https://ghostarchive.org/varchive/youtube/20211212/i94OvYb6noo">Archived</a> from the original on 2021-12-12 &#8211; via <a href="/wiki/YouTube" title="YouTube">YouTube</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=CS231n&amp;rft.atitle=Lecture+4%3A+Backpropagation%2C+Neural+Networks+1&amp;rft.date=2016&amp;rft.aulast=Karpathy&amp;rft.aufirst=Andrej&amp;rft&#95;id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Di94OvYb6noo%26list%3DPLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC%26index%3D4&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=3">"What is Backpropagation Really Doing?"</a>. <i>3Blue1Brown</i>. November 3, 2017. <a rel="nofollow" class="external text" href="https://ghostarchive.org/varchive/youtube/20211212/Ilg3gGewQ5U">Archived</a> from the original on 2021-12-12 &#8211; via <a href="/wiki/YouTube" title="YouTube">YouTube</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=3Blue1Brown&amp;rft.atitle=What+is+Backpropagation+Really+Doing%3F&amp;rft.date=2017-11-03&amp;rft&#95;id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DIlg3gGewQ5U%26list%3DPLZHQObOWTQDNU6R1&#95;67000Dx&#95;ZCJB-3pi%26index%3D3&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFPutta2022" class="citation web cs1">Putta, Sudeep Raja (2022). <a rel="nofollow" class="external text" href="https://sudeepraja.github.io/BackpropAdjoints/">"Yet Another Derivation of Backpropagation in Matrix Form"</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Yet+Another+Derivation+of+Backpropagation+in+Matrix+Form&amp;rft.date=2022&amp;rft.aulast=Putta&amp;rft.aufirst=Sudeep+Raja&amp;rft&#95;id=https%3A%2F%2Fsudeepraja.github.io%2FBackpropAdjoints%2F&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3ABackpropagation" class="Z3988"></span></li></ul>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333133064" /><style data-mw-deduplicate="TemplateStyles:r1314944253">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd;color:inherit}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf;color:inherit}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf;color:inherit}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff;color:inherit}.mw-parser-output .navbox-even{background-color:#f7f7f7;color:inherit}.mw-parser-output .navbox-odd{background-color:transparent;color:inherit}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}</style></div><div role="navigation" class="navbox" aria-labelledby="Artificial&#95;intelligence&#95;(AI)8549" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333133064" /><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1239400231" /><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Artificial_intelligence_navbox" title="Template:Artificial intelligence navbox"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Artificial_intelligence_navbox" title="Template talk:Artificial intelligence navbox"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Artificial_intelligence_navbox" title="Special:EditPage/Template:Artificial intelligence navbox"><abbr title="Edit this template">e</abbr></a></li></ul></div><div id="Artificial&#95;intelligence&#95;(AI)8549" style="font-size:114%;margin:0 4em"><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a> (AI)</div></th></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><a href="/wiki/History_of_artificial_intelligence" title="History of artificial intelligence">History</a>
<ul><li><a href="/wiki/Timeline_of_artificial_intelligence" title="Timeline of artificial intelligence">timeline</a></li></ul></li>
<li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_companies" title="List of artificial intelligence companies">Companies</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">Projects</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Parameter" title="Parameter">Parameter</a>
<ul><li><a href="/wiki/Hyperparameter_(machine_learning)" title="Hyperparameter (machine learning)">Hyperparameter</a></li></ul></li>
<li><a href="/wiki/Loss_functions_for_classification" title="Loss functions for classification">Loss functions</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a>
<ul><li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Double_descent" title="Double descent">Double descent</a></li>
<li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li></ul></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a>
<ul><li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD</a></li>
<li><a href="/wiki/Quasi-Newton_method" title="Quasi-Newton method">Quasi-Newton method</a></li>
<li><a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">Conjugate gradient method</a></li></ul></li>
<li><a class="mw-selflink selflink">Backpropagation</a></li>
<li><a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">Attention</a></li>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Normalization_(machine_learning)" title="Normalization (machine learning)">Normalization</a>
<ul><li><a href="/wiki/Batch_normalization" title="Batch normalization">Batchnorm</a></li></ul></li>
<li><a href="/wiki/Activation_function" title="Activation function">Activation</a>
<ul><li><a href="/wiki/Softmax_function" title="Softmax function">Softmax</a></li>
<li><a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a></li>
<li><a href="/wiki/Rectifier_(neural_networks)" class="mw-redirect" title="Rectifier (neural networks)">Rectifier</a></li></ul></li>
<li><a href="/wiki/Gating_mechanism" title="Gating mechanism">Gating</a></li>
<li><a href="/wiki/Weight_initialization" title="Weight initialization">Weight initialization</a></li>
<li><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">Regularization</a></li>
<li><a href="/wiki/Training,_validation,_and_test_data_sets" title="Training, validation, and test data sets">Datasets</a>
<ul><li><a href="/wiki/Data_augmentation" title="Data augmentation">Augmentation</a></li></ul></li>
<li><a href="/wiki/Prompt_engineering" title="Prompt engineering">Prompt engineering</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a>
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Imitation_learning" title="Imitation learning">Imitation</a></li>
<li><a href="/wiki/Policy_gradient_method" title="Policy gradient method">Policy gradient</a></li></ul></li>
<li><a href="/wiki/Diffusion_process" title="Diffusion process">Diffusion</a></li>
<li><a href="/wiki/Latent_diffusion_model" title="Latent diffusion model">Latent diffusion model</a></li>
<li><a href="/wiki/Autoregressive_model" title="Autoregressive model">Autoregression</a></li>
<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversary</a></li>
<li><a href="/wiki/Retrieval-augmented_generation" title="Retrieval-augmented generation">RAG</a></li>
<li><a href="/wiki/Uncanny_valley" title="Uncanny valley">Uncanny valley</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reflection_(artificial_intelligence)" class="mw-redirect" title="Reflection (artificial intelligence)">Reflection</a></li>
<li><a href="/wiki/Recursive_self-improvement" title="Recursive self-improvement">Recursive self-improvement</a></li>
<li><a href="/wiki/Hallucination_(artificial_intelligence)" title="Hallucination (artificial intelligence)">Hallucination</a></li>
<li><a href="/wiki/Word_embedding" title="Word embedding">Word embedding</a></li>
<li><a href="/wiki/Vibe_coding" title="Vibe coding">Vibe coding</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Applications_of_artificial_intelligence" title="Applications of artificial intelligence">Applications</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a>
<ul><li><a href="/wiki/Prompt_engineering#In-context_learning" title="Prompt engineering">In-context learning</a></li></ul></li>
<li><a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">Artificial neural network</a>
<ul><li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul></li>
<li><a href="/wiki/Language_model" title="Language model">Language model</a>
<ul><li><a href="/wiki/Large_language_model" title="Large language model">Large</a></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>
<li><a href="/wiki/Reasoning_model" title="Reasoning model">Reasoning</a></li></ul></li>
<li><a href="/wiki/Model_Context_Protocol" title="Model Context Protocol">Model Context Protocol</a></li>
<li><a href="/wiki/Intelligent_agent" title="Intelligent agent">Intelligent agent</a></li>
<li><a href="/wiki/Artificial_human_companion" title="Artificial human companion">Artificial human companion</a></li>
<li><a href="/wiki/Humanity%27s_Last_Exam" title="Humanity&#39;s Last Exam">Humanity's Last Exam</a></li>
<li><a href="/wiki/Lethal_autonomous_weapon" title="Lethal autonomous weapon">Lethal autonomous weapons (LAWs)</a></li>
<li><a href="/wiki/Generative_artificial_intelligence" title="Generative artificial intelligence">Generative artificial intelligence (GenAI)</a></li>
<li>(Hypothetical: <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Artificial general intelligence (AGI)</a>)</li>
<li>(Hypothetical: <a href="/wiki/Artificial_superintelligence" class="mw-redirect" title="Artificial superintelligence">Artificial superintelligence (ASI)</a>)</li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Implementations</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Audio–visual</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>
<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/Deep_learning_speech_synthesis" title="Deep learning speech synthesis">Speech synthesis</a>
<ul><li><a href="/wiki/15.ai" title="15.ai">15.ai</a></li>
<li><a href="/wiki/ElevenLabs" title="ElevenLabs">ElevenLabs</a></li></ul></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a>
<ul><li><a href="/wiki/Whisper_(speech_recognition_system)" title="Whisper (speech recognition system)">Whisper</a></li></ul></li>
<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition</a></li>
<li><a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a></li>
<li><a href="/wiki/Text-to-image_model" title="Text-to-image model">Text-to-image models</a>
<ul><li><a href="/wiki/Aurora_(text-to-image_model)" class="mw-redirect" title="Aurora (text-to-image model)">Aurora</a></li>
<li><a href="/wiki/DALL-E" title="DALL-E">DALL-E</a></li>
<li><a href="/wiki/Adobe_Firefly" title="Adobe Firefly">Firefly</a></li>
<li><a href="/wiki/Flux_(text-to-image_model)" title="Flux (text-to-image model)">Flux</a></li>
<li><a href="/wiki/GPT_Image" title="GPT Image">GPT Image</a></li>
<li><a href="/wiki/Ideogram_(text-to-image_model)" title="Ideogram (text-to-image model)">Ideogram</a></li>
<li><a href="/wiki/Imagen_(text-to-image_model)" title="Imagen (text-to-image model)">Imagen</a></li>
<li><a href="/wiki/Midjourney" title="Midjourney">Midjourney</a></li>
<li><a href="/wiki/Recraft" title="Recraft">Recraft</a></li>
<li><a href="/wiki/Stable_Diffusion" title="Stable Diffusion">Stable Diffusion</a></li></ul></li>
<li><a href="/wiki/Text-to-video_model" title="Text-to-video model">Text-to-video models</a>
<ul><li><a href="/wiki/Dream_Machine_(text-to-video_model)" title="Dream Machine (text-to-video model)">Dream Machine</a></li>
<li><a href="/wiki/Runway_(company)#Services_and_technologies" title="Runway (company)">Runway Gen</a></li>
<li><a href="/wiki/MiniMax_(company)#Hailuo_AI" title="MiniMax (company)">Hailuo AI</a></li>
<li><a href="/wiki/Kling_AI" title="Kling AI">Kling</a></li>
<li><a href="/wiki/Sora_(text-to-video_model)" title="Sora (text-to-video model)">Sora</a></li>
<li><a href="/wiki/Seedance_2.0" title="Seedance 2.0">Seedance</a></li>
<li><a href="/wiki/Veo_(text-to-video_model)" title="Veo (text-to-video model)">Veo</a></li></ul></li>
<li><a href="/wiki/Music_and_artificial_intelligence" title="Music and artificial intelligence">Music generation</a>
<ul><li><a href="/wiki/Riffusion" title="Riffusion">Riffusion</a></li>
<li><a href="/wiki/Suno_AI" class="mw-redirect" title="Suno AI">Suno AI</a></li>
<li><a href="/wiki/Udio" title="Udio">Udio</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Text</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/GloVe" title="GloVe">GloVe</a></li>
<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/T5_(language_model)" title="T5 (language model)">T5</a></li>
<li><a href="/wiki/Llama_(language_model)" title="Llama (language model)">Llama</a></li>
<li><a href="/wiki/Chinchilla_(language_model)" title="Chinchilla (language model)">Chinchilla AI</a></li>
<li><a href="/wiki/PaLM" title="PaLM">PaLM</a></li>
<li><a href="/wiki/Generative_pre-trained_transformer" title="Generative pre-trained transformer">GPT</a>
<ul><li><a href="/wiki/GPT-1" title="GPT-1">1</a></li>
<li><a href="/wiki/GPT-2" title="GPT-2">2</a></li>
<li><a href="/wiki/GPT-3" title="GPT-3">3</a></li>
<li><a href="/wiki/GPT-J" title="GPT-J">J</a></li>
<li><a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a></li>
<li><a href="/wiki/GPT-4" title="GPT-4">4</a></li>
<li><a href="/wiki/GPT-4o" title="GPT-4o">4o</a></li>
<li><a href="/wiki/OpenAI_o1" title="OpenAI o1">o1</a></li>
<li><a href="/wiki/OpenAI_o3" title="OpenAI o3">o3</a></li>
<li><a href="/wiki/GPT-4.5" title="GPT-4.5">4.5</a></li>
<li><a href="/wiki/GPT-4.1" title="GPT-4.1">4.1</a></li>
<li><a href="/wiki/OpenAI_o4-mini" title="OpenAI o4-mini">o4-mini</a></li>
<li><a href="/wiki/GPT-5" title="GPT-5">5</a></li>
<li><a href="/wiki/GPT-5.1" title="GPT-5.1">5.1</a></li>
<li><a href="/wiki/GPT-5.2" title="GPT-5.2">5.2</a></li></ul></li>
<li><a href="/wiki/Claude_(language_model)" title="Claude (language model)">Claude</a></li>
<li><a href="/wiki/Gemini_(chatbot)" class="mw-redirect" title="Gemini (chatbot)">Gemini</a>
<ul><li><a href="/wiki/Gemini_(language_model)" title="Gemini (language model)">Gemini (language model)</a></li>
<li><a href="/wiki/Gemma_(language_model)" title="Gemma (language model)">Gemma</a></li></ul></li>
<li><a href="/wiki/Grok_(chatbot)" title="Grok (chatbot)">Grok</a></li>
<li><a href="/wiki/LaMDA" title="LaMDA">LaMDA</a></li>
<li><a href="/wiki/BLOOM_(language_model)" title="BLOOM (language model)">BLOOM</a></li>
<li><a href="/wiki/DBRX" title="DBRX">DBRX</a></li>
<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>
<li><a href="/wiki/IBM_Watson" title="IBM Watson">IBM Watson</a></li>
<li><a href="/wiki/IBM_Watsonx" title="IBM Watsonx">IBM Watsonx</a></li>
<li><a href="/wiki/IBM_Granite" title="IBM Granite">Granite</a></li>
<li><a href="/wiki/Huawei_PanGu" title="Huawei PanGu">PanGu-Σ</a></li>
<li><a href="/wiki/DeepSeek_(chatbot)" title="DeepSeek (chatbot)">DeepSeek</a></li>
<li><a href="/wiki/Qwen" title="Qwen">Qwen</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Decisional</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>
<li><a href="/wiki/AlphaZero" title="AlphaZero">AlphaZero</a></li>
<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li>
<li><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving car</a></li>
<li><a href="/wiki/MuZero" title="MuZero">MuZero</a></li>
<li><a href="/wiki/Action_selection" title="Action selection">Action selection</a>
<ul><li><a href="/wiki/AutoGPT" title="AutoGPT">AutoGPT</a></li></ul></li>
<li><a href="/wiki/Robot_control" title="Robot control">Robot control</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a></li>
<li><a href="/wiki/Warren_Sturgis_McCulloch" title="Warren Sturgis McCulloch">Warren Sturgis McCulloch</a></li>
<li><a href="/wiki/Walter_Pitts" title="Walter Pitts">Walter Pitts</a></li>
<li><a href="/wiki/John_von_Neumann" title="John von Neumann">John von Neumann</a></li>
<li><a href="/wiki/Christopher_D._Manning" title="Christopher D. Manning">Christopher D. Manning</a></li>
<li><a href="/wiki/Claude_Shannon" title="Claude Shannon">Claude Shannon</a></li>
<li><a href="/wiki/Shun%27ichi_Amari" title="Shun&#39;ichi Amari">Shun'ichi Amari</a></li>
<li><a href="/wiki/Kunihiko_Fukushima" title="Kunihiko Fukushima">Kunihiko Fukushima</a></li>
<li><a href="/wiki/Takeo_Kanade" title="Takeo Kanade">Takeo Kanade</a></li>
<li><a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a></li>
<li><a href="/wiki/John_McCarthy_(computer_scientist)" title="John McCarthy (computer scientist)">John McCarthy</a></li>
<li><a href="/wiki/Nathaniel_Rochester_(computer_scientist)" title="Nathaniel Rochester (computer scientist)">Nathaniel Rochester</a></li>
<li><a href="/wiki/Allen_Newell" title="Allen Newell">Allen Newell</a></li>
<li><a href="/wiki/Cliff_Shaw" title="Cliff Shaw">Cliff Shaw</a></li>
<li><a href="/wiki/Herbert_A._Simon" title="Herbert A. Simon">Herbert A. Simon</a></li>
<li><a href="/wiki/Oliver_Selfridge" title="Oliver Selfridge">Oliver Selfridge</a></li>
<li><a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a></li>
<li><a href="/wiki/Bernard_Widrow" title="Bernard Widrow">Bernard Widrow</a></li>
<li><a href="/wiki/Joseph_Weizenbaum" title="Joseph Weizenbaum">Joseph Weizenbaum</a></li>
<li><a href="/wiki/Seymour_Papert" title="Seymour Papert">Seymour Papert</a></li>
<li><a href="/wiki/Seppo_Linnainmaa" title="Seppo Linnainmaa">Seppo Linnainmaa</a></li>
<li><a href="/wiki/Paul_Werbos" title="Paul Werbos">Paul Werbos</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/John_Hopfield" title="John Hopfield">John Hopfield</a></li>
<li><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a></li>
<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>
<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Lotfi_A._Zadeh" title="Lotfi A. Zadeh">Lotfi A. Zadeh</a></li>
<li><a href="/wiki/Stephen_Grossberg" title="Stephen Grossberg">Stephen Grossberg</a></li>
<li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>
<li><a href="/wiki/James_Goodnight" title="James Goodnight">James Goodnight</a></li>
<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>
<li><a href="/wiki/Fei-Fei_Li" title="Fei-Fei Li">Fei-Fei Li</a></li>
<li><a href="/wiki/Alex_Krizhevsky" title="Alex Krizhevsky">Alex Krizhevsky</a></li>
<li><a href="/wiki/Ilya_Sutskever" title="Ilya Sutskever">Ilya Sutskever</a></li>
<li><a href="/wiki/Oriol_Vinyals" title="Oriol Vinyals">Oriol Vinyals</a></li>
<li><a href="/wiki/Quoc_V._Le" title="Quoc V. Le">Quoc V. Le</a></li>
<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>
<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>
<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li>
<li><a href="/wiki/Andrej_Karpathy" title="Andrej Karpathy">Andrej Karpathy</a></li>
<li><a href="/wiki/Ashish_Vaswani" title="Ashish Vaswani">Ashish Vaswani</a></li>
<li><a href="/wiki/Noam_Shazeer" title="Noam Shazeer">Noam Shazeer</a></li>
<li><a href="/wiki/Aidan_Gomez" title="Aidan Gomez">Aidan Gomez</a></li>
<li><a href="/wiki/John_Schulman" title="John Schulman">John Schulman</a></li>
<li><a href="/wiki/Mustafa_Suleyman" title="Mustafa Suleyman">Mustafa Suleyman</a></li>
<li><a href="/wiki/Jan_Leike" title="Jan Leike">Jan Leike</a></li>
<li><a href="/wiki/Daniel_Kokotajlo_(researcher)" title="Daniel Kokotajlo (researcher)">Daniel Kokotajlo</a></li>
<li><a href="/wiki/Fran%C3%A7ois_Chollet" title="François Chollet">François Chollet</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Architectures</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" class="mw-redirect" title="Transformer (deep learning architecture)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision transformer (ViT)</a></li></ul></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network (RNN)</a></li>
<li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory (LSTM)</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit (GRU)</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">Echo state network</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron (MLP)</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network (CNN)</a></li>
<li><a href="/wiki/Residual_neural_network" title="Residual neural network">Residual neural network (RNN)</a></li>
<li><a href="/wiki/Highway_network" title="Highway network">Highway network</a></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoder (VAE)</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">Generative adversarial network (GAN)</a></li>
<li><a href="/wiki/Graph_neural_network" title="Graph neural network">Graph neural network (GNN)</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Political</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AI_safety" title="AI safety">AI safety</a> (<a href="/wiki/AI_alignment" title="AI alignment">Alignment</a>)</li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of AI</a></li>
<li>EU <a href="/wiki/Artificial_Intelligence_Act" title="Artificial Intelligence Act">AI Act</a></li>
<li><a href="/wiki/Precautionary_principle" title="Precautionary principle">Precautionary principle</a></li>
<li><a href="/wiki/Regulation_of_artificial_intelligence" title="Regulation of artificial intelligence">Regulation of AI</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Social and economic</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AI_boom" title="AI boom">AI boom</a></li>
<li><a href="/wiki/AI_bubble" title="AI bubble">AI bubble</a></li>
<li><a href="/wiki/AI_literacy" title="AI literacy">AI literacy</a></li>
<li><a href="/wiki/AI_slop" title="AI slop">AI slop</a></li>
<li><a href="/wiki/AI_winter" title="AI winter">AI winter</a></li>
<li><a href="/wiki/AI_anthropomorphism" title="AI anthropomorphism">Anthropomorphism</a></li>
<li><a href="/wiki/Artificial_intelligence_in_architecture" title="Artificial intelligence in architecture">In architecture</a></li>
<li><a href="/wiki/Artificial_intelligence_in_education" title="Artificial intelligence in education">In education</a></li>
<li><a href="/wiki/Artificial_intelligence_in_healthcare" title="Artificial intelligence in healthcare">In healthcare</a>
<ul><li><a href="/wiki/Chatbot_psychosis" title="Chatbot psychosis">Chatbot psychosis</a></li>
<li><a href="/wiki/Artificial_intelligence_in_mental_health" title="Artificial intelligence in mental health">Mental health</a></li></ul></li>
<li><a href="/wiki/Artificial_intelligence_visual_art" title="Artificial intelligence visual art">In visual art</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><span class="noviewer" typeof="mw:File"><span title="Category"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/20px-Symbol_category_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/40px-Symbol_category_class.svg.png 1.5x" data-file-width="180" data-file-height="185" /></span></span> <a href="/wiki/Category:Artificial_intelligence" title="Category:Artificial intelligence">Category</a></li></ul>
</div></td></tr></tbody></table></div>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333133064" /><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1314944253" /></div><div role="navigation" class="navbox authority-control" aria-labelledby="Authority&#95;control&#95;databases&#95;frameless&amp;#124;text-top&amp;#124;10px&amp;#124;alt=Edit&#95;this&#95;at&#95;Wikidata&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q798503#identifiers&amp;#124;class=noprint&amp;#124;Edit&#95;this&#95;at&#95;Wikidata644" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div id="Authority&#95;control&#95;databases&#95;frameless&amp;#124;text-top&amp;#124;10px&amp;#124;alt=Edit&#95;this&#95;at&#95;Wikidata&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q798503#identifiers&amp;#124;class=noprint&amp;#124;Edit&#95;this&#95;at&#95;Wikidata644" style="font-size:114%;margin:0 4em"><a href="/wiki/Help:Authority_control" title="Help:Authority control">Authority control databases</a> <span class="mw-valign-text-top noprint" typeof="mw:File/Frameless"><a href="https://www.wikidata.org/wiki/Q798503#identifiers" title="Edit this at Wikidata"><img alt="Edit this at Wikidata" src="//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png" decoding="async" width="10" height="10" class="mw-file-element" data-file-width="20" data-file-height="20" /></a></span></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">National</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"><ul><li><span class="uid"><a rel="nofollow" class="external text" href="https://id.loc.gov/authorities/sh94008320">United States</a></span></li><li><span class="uid"><a rel="nofollow" class="external text" href="https://www.nli.org.il/en/authorities/987007561020705171">Israel</a></span></li></ul></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Other</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em"><ul><li><span class="uid"><a rel="nofollow" class="external text" href="https://lux.collections.yale.edu/view/concept/0689ec27-69b4-483a-b5f9-f71dd8434d10">Yale LUX</a></span></li></ul></div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw‐web.eqiad.main‐6f597bcf‐pfljn
Cached time: 20260224001905
Cache expiry: 85268
Reduced expiry: true
Complications: [vary‐revision‐sha1, prevent‐selective‐update, show‐toc]
CPU time usage: 1.152 seconds
Real time usage: 1.472 seconds
Preprocessor visited node count: 6530/1000000
Revision size: 56814/2097152 bytes
Post‐expand include size: 198156/2097152 bytes
Template argument size: 7102/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 244574/5000000 bytes
Lua time usage: 0.602/10.000 seconds
Lua memory usage: 9545859/52428800 bytes
Number of Wikibase entities loaded: 1/500
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  979.147      1 -total
 37.61%  368.238      2 Template:Reflist
 18.57%  181.802     19 Template:Cite_journal
 11.81%  115.655      1 Template:Machine_learning_bar
 11.36%  111.192      1 Template:Sidebar_with_collapsible_lists
 10.17%   99.589     19 Template:Cite_book
  9.15%   89.617      1 Template:Short_description
  6.54%   64.020      2 Template:Pagetype
  4.75%   46.533      1 Template:Expand_section
  4.55%   44.553      1 Template:Sfn
-->

<!-- Saved in parser cache with key enwiki:pcache:1360091:|#|:idhash:canonical and timestamp 20260224001905 and revision id 1328447875. Rendering was triggered because: page_view
 -->
</div><noscript><img src="https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?useformat=desktop&amp;type=1x1&amp;usesul3=1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Backpropagation&amp;oldid=1328447875">https://en.wikipedia.org/w/index.php?title=Backpropagation&amp;oldid=1328447875</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw-interface=""><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></li><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_errors:_ISBN_date" title="Category:CS1 errors: ISBN date">CS1 errors: ISBN date</a></li><li><a href="/wiki/Category:CS1_Finnish-language_sources_(fi)" title="Category:CS1 Finnish-language sources (fi)">CS1 Finnish-language sources (fi)</a></li><li><a href="/wiki/Category:CS1:_long_volume_value" title="Category:CS1: long volume value">CS1: long volume value</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_matches_Wikidata" title="Category:Short description matches Wikidata">Short description matches Wikidata</a></li><li><a href="/wiki/Category:Articles_to_be_expanded_from_November_2019" title="Category:Articles to be expanded from November 2019">Articles to be expanded from November 2019</a></li><li><a href="/wiki/Category:All_articles_to_be_expanded" title="Category:All articles to be expanded">All articles to be expanded</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_February_2022" title="Category:Articles with unsourced statements from February 2022">Articles with unsourced statements from February 2022</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 19 December 2025, at 23:04<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a href="/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" title="Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License">Creative Commons Attribution-ShareAlike 4.0 License</a>;
additional terms may apply. By using this site, you agree to the <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use" class="extiw" title="foundation:Special:MyLanguage/Policy:Terms of Use">Terms of Use</a> and <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy" class="extiw" title="foundation:Special:MyLanguage/Policy:Privacy policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a rel="nofollow" class="external text" href="https://wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-legal-safety-contacts"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Legal:Wikimedia_Foundation_Legal_and_Safety_Contact_Information">Legal &amp; safety contacts</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
	<li id="footer-places-mobileview"><a href="//en.wikipedia.org/w/index.php?title=Backpropagation&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://www.wikimedia.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/static/images/footer/wikimedia-button.svg" width="84" height="29"><img src="/static/images/footer/wikimedia.svg" width="25" height="25" alt="Wikimedia Foundation" lang="en" loading="lazy"></picture></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/w/resources/assets/poweredby_mediawiki.svg" width="88" height="31"><img src="/w/resources/assets/mediawiki_compact.svg" alt="Powered by MediaWiki" lang="en" width="25" height="25" loading="lazy"></picture></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class="vector-header-container vector-sticky-header-container no-font-mode-scale">
	<div id="vector-sticky-header" class="vector-sticky-header">
		<div class="vector-sticky-header-start">
			<div class="vector-sticky-header-icon-start vector-button-flush-left" aria-hidden="true">
				<button class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-sticky-header-search-toggle" tabindex="-1" data-event-name="ui.vector-sticky-search-form.icon"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
			</button>
		</div>
			
		<div role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box">
			<div class="vector-typeahead-search-container">
				<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail">
					<form action="/w/index.php" id="vector-sticky-search-form" class="cdx-search-input cdx-search-input--has-end-button">
						<div  class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
							<div class="cdx-text-input cdx-text-input--has-start-icon">
								<input
									class="cdx-text-input__input mw-searchInput" autocomplete="off"
									
									type="search" name="search" placeholder="Search Wikipedia">
								<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
							</div>
							<input type="hidden" name="title" value="Special:Search">
						</div>
						<button class="cdx-button cdx-search-input__end-button">Search</button>
					</form>
				</div>
			</div>
		</div>
		<div class="vector-sticky-header-context-bar">
				<nav aria-label="Contents" class="vector-toc-landmark">
						
					<div id="vector-sticky-header-toc" class="vector-dropdown mw-portlet mw-portlet-sticky-header-toc vector-sticky-header-toc vector-button-flush-left"  >
						<input type="checkbox" id="vector-sticky-header-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-sticky-header-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
						<label id="vector-sticky-header-toc-label" for="vector-sticky-header-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
						</label>
						<div class="vector-dropdown-content">
					
						<div id="vector-sticky-header-toc-unpinned-container" class="vector-unpinned-container">
						</div>
					
						</div>
					</div>
			</nav>
				<div class="vector-sticky-header-context-bar-primary" aria-hidden="true" ><span class="mw-page-title-main">Backpropagation</span></div>
			</div>
		</div>
		<div class="vector-sticky-header-end" aria-hidden="true">
			<div class="vector-sticky-header-icons">
				<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-talk-sticky-header" tabindex="-1" data-event-name="talk-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbles mw-ui-icon-wikimedia-speechBubbles"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-subject-sticky-header" tabindex="-1" data-event-name="subject-sticky-header"><span class="vector-icon mw-ui-icon-article mw-ui-icon-wikimedia-article"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-history-sticky-header" tabindex="-1" data-event-name="history-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-history mw-ui-icon-wikimedia-wikimedia-history"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only mw-watchlink" id="ca-watchstar-sticky-header" tabindex="-1" data-event-name="watch-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-star mw-ui-icon-wikimedia-wikimedia-star"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-edit-sticky-header" tabindex="-1" data-event-name="wikitext-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-wikiText mw-ui-icon-wikimedia-wikimedia-wikiText"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-ve-edit-sticky-header" tabindex="-1" data-event-name="ve-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-edit mw-ui-icon-wikimedia-wikimedia-edit"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-viewsource-sticky-header" tabindex="-1" data-event-name="ve-edit-protected-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-editLock mw-ui-icon-wikimedia-wikimedia-editLock"></span>

<span></span>
			</a>
		</div>
			<div class="vector-sticky-header-buttons">
				<button class="cdx-button cdx-button--weight-quiet mw-interlanguage-selector" id="p-lang-btn-sticky-header" tabindex="-1" data-event-name="ui.dropdown-p-lang-btn-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-language mw-ui-icon-wikimedia-wikimedia-language"></span>

<span>24 languages</span>
			</button>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive" id="ca-addsection-sticky-header" tabindex="-1" data-event-name="addsection-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbleAdd-progressive mw-ui-icon-wikimedia-speechBubbleAdd-progressive"></span>

<span>Add topic</span>
			</a>
		</div>
			<div class="vector-sticky-header-icon-end">
				<div class="vector-user-links">
				</div>
			</div>
		</div>
	</div>
</div>
<div class="mw-portlet mw-portlet-dock-bottom emptyPortlet" id="p-dock-bottom">
	<ul>
		
	</ul>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw-web.eqiad.main-6f597bcf-cbfh7","wgBackendResponseTime":198,"wgPageParseReport":{"limitreport":{"cputime":"1.152","walltime":"1.472","ppvisitednodes":{"value":6530,"limit":1000000},"revisionsize":{"value":56814,"limit":2097152},"postexpandincludesize":{"value":198156,"limit":2097152},"templateargumentsize":{"value":7102,"limit":2097152},"expansiondepth":{"value":16,"limit":100},"expensivefunctioncount":{"value":6,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":244574,"limit":5000000},"entityaccesscount":{"value":1,"limit":500},"timingprofile":["100.00%  979.147      1 -total"," 37.61%  368.238      2 Template:Reflist"," 18.57%  181.802     19 Template:Cite_journal"," 11.81%  115.655      1 Template:Machine_learning_bar"," 11.36%  111.192      1 Template:Sidebar_with_collapsible_lists"," 10.17%   99.589     19 Template:Cite_book","  9.15%   89.617      1 Template:Short_description","  6.54%   64.020      2 Template:Pagetype","  4.75%   46.533      1 Template:Expand_section","  4.55%   44.553      1 Template:Sfn"]},"scribunto":{"limitreport-timeusage":{"value":"0.602","limit":"10.000"},"limitreport-memusage":{"value":9545859,"limit":52428800},"limitreport-logs":"anchor_id_list = table#1 {\n    [\"CITEREFAlpaydin2010\"] = 1,\n    [\"CITEREFAmari1967\"] = 1,\n    [\"CITEREFAndersonRosenfeld2000\"] = 1,\n    [\"CITEREFBernackiWłodarczyk2004\"] = 1,\n    [\"CITEREFBryson1962\"] = 1,\n    [\"CITEREFBucklandCollins2002\"] = 1,\n    [\"CITEREFChangDellBock2006\"] = 1,\n    [\"CITEREFDreyfus1962\"] = 1,\n    [\"CITEREFDreyfus1973\"] = 1,\n    [\"CITEREFDreyfus1990\"] = 1,\n    [\"CITEREFFitzChang2019\"] = 1,\n    [\"CITEREFGoodfellowBengioCourville2016\"] = 1,\n    [\"CITEREFGriewank2012\"] = 1,\n    [\"CITEREFGriewankWalther2008\"] = 1,\n    [\"CITEREFHecht-Nielsen1990\"] = 1,\n    [\"CITEREFHertz1991\"] = 1,\n    [\"CITEREFJanciauskasChang2018\"] = 1,\n    [\"CITEREFKarpathy2016\"] = 1,\n    [\"CITEREFKelley1960\"] = 1,\n    [\"CITEREFLeCunBengioHinton2015\"] = 1,\n    [\"CITEREFLe_Cun1987\"] = 1,\n    [\"CITEREFLeibniz1920\"] = 1,\n    [\"CITEREFLinnainmaa1970\"] = 1,\n    [\"CITEREFLinnainmaa1976\"] = 1,\n    [\"CITEREFMartens2020\"] = 1,\n    [\"CITEREFMcCaffrey2012\"] = 1,\n    [\"CITEREFMisra2019\"] = 1,\n    [\"CITEREFMizutaniDreyfusNishio2000\"] = 1,\n    [\"CITEREFNielsen2015\"] = 1,\n    [\"CITEREFParker1985\"] = 1,\n    [\"CITEREFPomerleau1988\"] = 1,\n    [\"CITEREFPutta2022\"] = 1,\n    [\"CITEREFRamachandranZophLe2017\"] = 1,\n    [\"CITEREFRobbinsMonro1951\"] = 1,\n    [\"CITEREFRodríguezLópez_Fernández2010\"] = 1,\n    [\"CITEREFRojas1996\"] = 1,\n    [\"CITEREFRosenblatt1962\"] = 1,\n    [\"CITEREFRumelhartHintonWilliams1986\"] = 1,\n    [\"CITEREFRumelhartHintonWilliams1986a\"] = 1,\n    [\"CITEREFRumelhartHintonWilliams1986b\"] = 1,\n    [\"CITEREFSchmidhuber2015\"] = 1,\n    [\"CITEREFSchmidhuber2022\"] = 1,\n    [\"CITEREFSejnowski2018\"] = 1,\n    [\"CITEREFSuttonBarto2018\"] = 1,\n    [\"CITEREFTanLim2019\"] = 1,\n    [\"CITEREFWan1994\"] = 1,\n    [\"CITEREFWerbos1982\"] = 1,\n    [\"CITEREFWerbos1994\"] = 1,\n    [\"CITEREFWiliamowskiYu2010\"] = 1,\n    [\"Hessian\"] = 1,\n    [\"Second_order\"] = 1,\n}\ntemplate_list = table#1 {\n    [\"About\"] = 1,\n    [\"Anchor\"] = 1,\n    [\"Artificial intelligence navbox\"] = 1,\n    [\"Authority control\"] = 1,\n    [\"Cbignore\"] = 2,\n    [\"Citation needed\"] = 1,\n    [\"Cite arXiv\"] = 3,\n    [\"Cite book\"] = 19,\n    [\"Cite journal\"] = 19,\n    [\"Cite report\"] = 1,\n    [\"Cite thesis\"] = 2,\n    [\"Cite web\"] = 9,\n    [\"Doi\"] = 1,\n    [\"Efn\"] = 9,\n    [\"EquationNote\"] = 5,\n    [\"EquationRef\"] = 5,\n    [\"Expand section\"] = 1,\n    [\"Further\"] = 1,\n    [\"Google books\"] = 2,\n    [\"Harvnb\"] = 1,\n    [\"Harvtxt\"] = 4,\n    [\"Hatnote\"] = 1,\n    [\"Machine learning bar\"] = 1,\n    [\"Mvar\"] = 13,\n    [\"Notelist\"] = 1,\n    [\"NumBlk\"] = 5,\n    [\"Reflist\"] = 1,\n    [\"Rp\"] = 3,\n    [\"See also\"] = 1,\n    [\"Sfn\"] = 1,\n    [\"Short description\"] = 1,\n}\narticle_whitelist = table#1 {\n}\nciteref_patterns = table#1 {\n}\n"},"cachereport":{"origin":"mw-web.eqiad.main-6f597bcf-pfljn","timestamp":"20260224001905","ttl":85268,"transientcontent":true}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Backpropagation","url":"https:\/\/en.wikipedia.org\/wiki\/Backpropagation","sameAs":"http:\/\/www.wikidata.org\/entity\/Q798503","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q798503","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2005-01-04T00:26:53Z","dateModified":"2025-12-19T23:04:54Z","headline":"optimization algorithm for artificial neural networks"}</script>
</body>
</html>