<!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 skin-theme-clientpref-day vector-sticky-header-enabled wp25eastereggs-enable-clientpref-1 vector-toc-available" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>k-nearest neighbors algorithm - Wikipedia</title>
<script>(function(){var className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 skin-theme-clientpref-day vector-sticky-header-enabled wp25eastereggs-enable-clientpref-1 vector-toc-available";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"ea1118ab-7d7a-4227-8a76-2929ed4d80c3","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"K-nearest_neighbors_algorithm","wgTitle":"K-nearest neighbors algorithm","wgCurRevisionId":1328986210,"wgRevisionId":1328986210,"wgArticleId":1775388,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: work parameter with ISBN","Webarchive template wayback links","Articles with short description","Short description is different from Wikidata","All articles with unsourced statements","Articles with unsourced statements from March 2013","Articles with unsourced statements from December 2008","Articles with unsourced statements from September 2019","Wikipedia articles needing clarification from July 2020","Classification algorithms","Search algorithms","Machine learning algorithms","Statistical classification","Nonparametric statistics"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"K-nearest_neighbors_algorithm","wgRelevantArticleId":1775388,"wgTempUserName":null,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgNoticeProject":"wikipedia","wgFlaggedRevsParams":{"tags":{"status":{"levels":1}}},"wgConfirmEditCaptchaNeededForGenericEdit":"hcaptcha","wgConfirmEditHCaptchaVisualEditorOnLoadIntegrationEnabled":false,"wgConfirmEditHCaptchaSiteKey":"5d0c670e-a5f4-4258-ad16-1f42792c9c62","wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":0,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":30000,"wgEditSubmitButtonLabelPublish":true,"wgVisualEditorPageIsDisambiguation":false,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":false,"wgVector2022LanguageInHeader":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q1071612","wgCheckUserClientHintsHeadersJsApi":["brands","architecture","bitness","fullVersionList","mobile","model","platform","platformVersion"],"GEHomepageSuggestedEditsEnableTopics":true,"wgGESuggestedEditsTaskTypes":{"taskTypes":["copyedit","link-recommendation"],"unavailableTaskTypes":[]},"wgGETopicsMatchModeEnabled":false,"wgGELevelingUpEnabledForUser":false,"wgGEUseTestKitchenExtension":true,"wgMetricsPlatformUserExperiments":{"active_experiments":[],"overrides":[],"enrolled":[],"assigned":[],"subject_ids":[],"sampling_units":[],"coordinator":[]},"wgTestKitchenUserExperiments":{"active_experiments":[],"overrides":[],"enrolled":[],"assigned":[],"subject_ids":[],"sampling_units":[],"coordinator":[]}};
RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.wikimediamessages.styles":"ready","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.page.gallery.styles":"ready","skins.vector.search.codex.styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready","ext.wp25EasterEggs.styles":"ready"};RLPAGEMODULES=["ext.parsermigration.survey","ext.cite.ux-enhancements","ext.math.polyfills","mediawiki.page.media","site","mediawiki.page.ready","mediawiki.toc","skins.vector.js","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips","ext.gadget.switcher","ext.urlShortener.toolbar","ext.centralauth.centralautologin","mmv.bootstrap","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.echo.centralauth","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.cx.uls.quick.actions","wikibase.client.vector-2022","wikibase.databox.fromWikidata","ext.checkUser.clientHints","ext.quicksurveys.init","ext.growthExperiments.SuggestedEditSession","ext.xLab","ext.testKitchen","ext.wp25EasterEggs"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediamessages.styles%7Cext.wp25EasterEggs.styles%7Cmediawiki.page.gallery.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.46.0-wmf.16">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta name="viewport" content="width=1120">
<meta property="og:title" content="k-nearest neighbors algorithm - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/rest.php/v1/search" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="auth.wikimedia.org">
</head>
<body class="skin--responsive skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-K-nearest_neighbors_algorithm rootpage-K-nearest_neighbors_algorithm skin-vector-2022 action-view">
<div id="mw-aria-live-region" class="mw-aria-live-region" aria-live="polite"></div><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header no-font-mode-scale">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  title="Main menu" >
	<input type="checkbox" id="vector-main-menu-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-main-menu-dropdown" class="vector-dropdown-checkbox "  aria-label="Main menu"  >
	<label id="vector-main-menu-dropdown-label" for="vector-main-menu-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>

<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li>
		</ul>
		
	</div>
</div>

	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li><li id="n-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li>
		</ul>
		
	</div>
</div>

</div>

				</div>

	</div>
</div>

		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/enwiki-25.svg" alt="" aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container skin-invert">
		<img class="mw-logo-wordmark" alt="Wikipedia" src="/static/images/mobile/copyright/wikipedia-wordmark-en-25.svg" style="width: 8.75em; height: 1.375em;">
		<img class="mw-logo-tagline" alt="The Free Encyclopedia" src="/static/images/mobile/copyright/wikipedia-tagline-en-25.svg" width="140" height="11" style="width: 8.75em; height: 0.6875em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle" title="Search Wikipedia [f]" accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input mw-searchInput" autocomplete="off"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="none" spellcheck="false" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links vector-user-links-wide" aria-label="Personal tools">
	<div class="vector-user-links-main">
	
<div id="p-vector-user-menu-preferences" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-userpage" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	<nav class="vector-appearance-landmark" aria-label="Appearance">
		
<div id="vector-appearance-dropdown" class="vector-dropdown "  title="Change the appearance of the page&#039;s font size, width, and color" >
	<input type="checkbox" id="vector-appearance-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-appearance-dropdown" class="vector-dropdown-checkbox "  aria-label="Appearance"  >
	<label id="vector-appearance-dropdown-label" for="vector-appearance-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-appearance mw-ui-icon-wikimedia-appearance"></span>

<span class="vector-dropdown-label-text">Appearance</span>
	</label>
	<div class="vector-dropdown-content">


			<div id="vector-appearance-unpinned-container" class="vector-unpinned-container">
				
			</div>
		
	</div>
</div>

	</nav>
	
<div id="p-vector-user-menu-notifications" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="pt-sitesupport-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en" class=""><span>Donate</span></a>
</li>
<li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="/w/index.php?title=Special:CreateAccount&amp;returnto=K-nearest+neighbors+algorithm" title="You are encouraged to create an account and log in; however, it is not mandatory" class=""><span>Create account</span></a>
</li>
<li id="pt-login-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw-interface  href="/w/index.php?title=Special:UserLogin&amp;returnto=K-nearest+neighbors+algorithm" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o" class=""><span>Log in</span></a>
</li>

			
		</ul>
		
	</div>
</div>

	</div>
	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out user-links-collapsible-item"  title="Log in and more options" >
	<input type="checkbox" id="vector-user-links-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-user-links-dropdown" class="vector-dropdown-checkbox "  aria-label="Personal tools"  >
	<label id="vector-user-links-dropdown-label" for="vector-user-links-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>

<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-sitesupport" class="user-links-collapsible-item mw-list-item"><a href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en"><span>Donate</span></a></li><li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=K-nearest+neighbors+algorithm" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=K-nearest+neighbors+algorithm" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

	
	</div>
</div>

</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-sitenotice-container">
			<div id="siteNotice"><!-- CentralNotice --><div class="wp25eastereggs-sitenotice"><div class="wp25eastereggs-sitenotice-landmark"></div></div></div>
		</div>
		<div class="vector-column-start">
			<div class="vector-main-menu-container">
		<div id="mw-navigation">
			<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site">
				<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
				</div>
		</nav>
		</div>
	</div>
	<div class="vector-sticky-pinned-container">
				<nav id="mw-panel-toc" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark">
					<div id="vector-toc-pinned-container" class="vector-pinned-container">
					<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-Statistical_setting"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Statistical_setting">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">1</span>
				<span>Statistical setting</span>
			</div>
		</a>
		
		<ul id="toc-Statistical_setting-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Algorithm"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Algorithm">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">2</span>
				<span>Algorithm</span>
			</div>
		</a>
		
		<ul id="toc-Algorithm-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Parameter_selection"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Parameter_selection">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">3</span>
				<span>Parameter selection</span>
			</div>
		</a>
		
		<ul id="toc-Parameter_selection-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-The_1-nearest_neighbor_classifier"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#The_1-nearest_neighbor_classifier">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">4</span>
				<span>The 1-nearest neighbor classifier</span>
			</div>
		</a>
		
		<ul id="toc-The_1-nearest_neighbor_classifier-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-The_weighted_nearest_neighbour_classifier"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#The_weighted_nearest_neighbour_classifier">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">5</span>
				<span>The weighted nearest neighbour classifier</span>
			</div>
		</a>
		
		<ul id="toc-The_weighted_nearest_neighbour_classifier-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Properties"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Properties">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">6</span>
				<span>Properties</span>
			</div>
		</a>
		
		<ul id="toc-Properties-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Error_rates"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Error_rates">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">7</span>
				<span>Error rates</span>
			</div>
		</a>
		
		<ul id="toc-Error_rates-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Metric_learning"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Metric_learning">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">8</span>
				<span>Metric learning</span>
			</div>
		</a>
		
		<ul id="toc-Metric_learning-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Feature_extraction"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Feature_extraction">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">9</span>
				<span>Feature extraction</span>
			</div>
		</a>
		
		<ul id="toc-Feature_extraction-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Dimension_reduction"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Dimension_reduction">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">10</span>
				<span>Dimension reduction</span>
			</div>
		</a>
		
		<ul id="toc-Dimension_reduction-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Decision_boundary"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Decision_boundary">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">11</span>
				<span>Decision boundary</span>
			</div>
		</a>
		
		<ul id="toc-Decision_boundary-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Data_reduction"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Data_reduction">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">12</span>
				<span>Data reduction</span>
			</div>
		</a>
		
			<button aria-controls="toc-Data_reduction-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Data reduction subsection</span>
			</button>
		
		<ul id="toc-Data_reduction-sublist" class="vector-toc-list">
			<li id="toc-Selection_of_class-outliers"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Selection_of_class-outliers">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">12.1</span>
					<span>Selection of class-outliers</span>
				</div>
			</a>
			
			<ul id="toc-Selection_of_class-outliers-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Condensed_Nearest_Neighbor_for_data_reduction"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Condensed_Nearest_Neighbor_for_data_reduction">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">12.2</span>
					<span>Condensed Nearest Neighbor for data reduction</span>
				</div>
			</a>
			
			<ul id="toc-Condensed_Nearest_Neighbor_for_data_reduction-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-k-NN_regression"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#k-NN_regression">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">13</span>
				<span><i>k</i>-NN regression</span>
			</div>
		</a>
		
		<ul id="toc-k-NN_regression-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-k-NN_outlier"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#k-NN_outlier">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">14</span>
				<span><i>k</i>-NN outlier</span>
			</div>
		</a>
		
		<ul id="toc-k-NN_outlier-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Validation_of_results"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Validation_of_results">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">15</span>
				<span>Validation of results</span>
			</div>
		</a>
		
		<ul id="toc-Validation_of_results-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">16</span>
				<span>See also</span>
			</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">17</span>
				<span>References</span>
			</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Further_reading"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Further_reading">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">18</span>
				<span>Further reading</span>
			</div>
		</a>
		
		<ul id="toc-Further_reading-sublist" class="vector-toc-list">
		</ul>
	</li>
</ul>
</div>

					</div>
		</nav>
			</div>
		</div>
		<div class="mw-content-container">
			<main id="content" class="mw-body">
				<header class="mw-body-header vector-page-titlebar no-font-mode-scale">
					<nav aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  title="Table of Contents" >
	<input type="checkbox" id="vector-page-titlebar-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-titlebar-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
	<label id="vector-page-titlebar-toc-label" for="vector-page-titlebar-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>

					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><i>k</i>-nearest neighbors algorithm</h1>
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox" id="p-lang-btn-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-lang-btn" class="vector-dropdown-checkbox mw-interlanguage-selector" aria-label="Go to an article in another language. Available in 24 languages"   >
	<label id="p-lang-btn-label" for="p-lang-btn-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-24" aria-hidden="true"  ><span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>

<span class="vector-dropdown-label-text">24 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list">
				
				<li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D9%83%D9%8A_%D8%A3%D9%82%D8%B1%D8%A8_%D8%AC%D8%A7%D8%B1" title="كي أقرب جار – Arabic" lang="ar" hreflang="ar" data-title="كي أقرب جار" data-language-autonym="العربية" data-language-local-name="Arabic" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Knn" title="Knn – Catalan" lang="ca" hreflang="ca" data-title="Knn" data-language-autonym="Català" data-language-local-name="Catalan" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-ckb mw-list-item"><a href="https://ckb.wikipedia.org/wiki/%DA%A9%DB%95%DB%8C_%D9%86%D8%B2%DB%8C%DA%A9%D8%AA%D8%B1%DB%8C%D9%86_%DA%BE%D8%A7%D9%88%D8%B3%DB%8E%DA%A9%D8%A7%D9%86" title="کەی نزیکترین ھاوسێکان – Central Kurdish" lang="ckb" hreflang="ckb" data-title="کەی نزیکترین ھاوسێکان" data-language-autonym="کوردی" data-language-local-name="Central Kurdish" class="interlanguage-link-target"><span>کوردی</span></a></li><li class="interlanguage-link interwiki-cs mw-list-item"><a href="https://cs.wikipedia.org/wiki/Algoritmus_k-nejbli%C5%BE%C5%A1%C3%ADch_soused%C5%AF" title="Algoritmus k-nejbližších sousedů – Czech" lang="cs" hreflang="cs" data-title="Algoritmus k-nejbližších sousedů" data-language-autonym="Čeština" data-language-local-name="Czech" class="interlanguage-link-target"><span>Čeština</span></a></li><li class="interlanguage-link interwiki-da mw-list-item"><a href="https://da.wikipedia.org/wiki/K-n%C3%A6rmeste_naboer" title="K-nærmeste naboer – Danish" lang="da" hreflang="da" data-title="K-nærmeste naboer" data-language-autonym="Dansk" data-language-local-name="Danish" class="interlanguage-link-target"><span>Dansk</span></a></li><li class="interlanguage-link interwiki-de mw-list-item"><a href="https://de.wikipedia.org/wiki/N%C3%A4chste-Nachbarn-Klassifikation" title="Nächste-Nachbarn-Klassifikation – German" lang="de" hreflang="de" data-title="Nächste-Nachbarn-Klassifikation" data-language-autonym="Deutsch" data-language-local-name="German" class="interlanguage-link-target"><span>Deutsch</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/K_vecinos_m%C3%A1s_pr%C3%B3ximos" title="K vecinos más próximos – Spanish" lang="es" hreflang="es" data-title="K vecinos más próximos" data-language-autonym="Español" data-language-local-name="Spanish" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-eu mw-list-item"><a href="https://eu.wikipedia.org/wiki/K_auzokide_hurbilenak" title="K auzokide hurbilenak – Basque" lang="eu" hreflang="eu" data-title="K auzokide hurbilenak" data-language-autonym="Euskara" data-language-local-name="Basque" class="interlanguage-link-target"><span>Euskara</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85_%DA%A9%DB%8C-%D9%86%D8%B2%D8%AF%DB%8C%DA%A9%E2%80%8C%D8%AA%D8%B1%DB%8C%D9%86_%D9%87%D9%85%D8%B3%D8%A7%DB%8C%D9%87" title="الگوریتم کی-نزدیک‌ترین همسایه – Persian" lang="fa" hreflang="fa" data-title="الگوریتم کی-نزدیک‌ترین همسایه" data-language-autonym="فارسی" data-language-local-name="Persian" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/M%C3%A9thode_des_k_plus_proches_voisins" title="Méthode des k plus proches voisins – French" lang="fr" hreflang="fr" data-title="Méthode des k plus proches voisins" data-language-autonym="Français" data-language-local-name="French" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-he mw-list-item"><a href="https://he.wikipedia.org/wiki/%D7%90%D7%9C%D7%92%D7%95%D7%A8%D7%99%D7%AA%D7%9D_%D7%A9%D7%9B%D7%9F_%D7%A7%D7%A8%D7%95%D7%91" title="אלגוריתם שכן קרוב – Hebrew" lang="he" hreflang="he" data-title="אלגוריתם שכן קרוב" data-language-autonym="עברית" data-language-local-name="Hebrew" class="interlanguage-link-target"><span>עברית</span></a></li><li class="interlanguage-link interwiki-id mw-list-item"><a href="https://id.wikipedia.org/wiki/Algoritma_k_tetangga_terdekat" title="Algoritma k tetangga terdekat – Indonesian" lang="id" hreflang="id" data-title="Algoritma k tetangga terdekat" data-language-autonym="Bahasa Indonesia" data-language-local-name="Indonesian" class="interlanguage-link-target"><span>Bahasa Indonesia</span></a></li><li class="interlanguage-link interwiki-it mw-list-item"><a href="https://it.wikipedia.org/wiki/Algoritmo_k-NN" title="Algoritmo k-NN – Italian" lang="it" hreflang="it" data-title="Algoritmo k-NN" data-language-autonym="Italiano" data-language-local-name="Italian" class="interlanguage-link-target"><span>Italiano</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/K%E8%BF%91%E5%82%8D%E6%B3%95" title="K近傍法 – Japanese" lang="ja" hreflang="ja" data-title="K近傍法" data-language-autonym="日本語" data-language-local-name="Japanese" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98" title="K-최근접 이웃 알고리즘 – Korean" lang="ko" hreflang="ko" data-title="K-최근접 이웃 알고리즘" data-language-autonym="한국어" data-language-local-name="Korean" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-no mw-list-item"><a href="https://no.wikipedia.org/wiki/K-NN" title="K-NN – Norwegian Bokmål" lang="nb" hreflang="nb" data-title="K-NN" data-language-autonym="Norsk bokmål" data-language-local-name="Norwegian Bokmål" class="interlanguage-link-target"><span>Norsk bokmål</span></a></li><li class="interlanguage-link interwiki-pl mw-list-item"><a href="https://pl.wikipedia.org/wiki/K_najbli%C5%BCszych_s%C4%85siad%C3%B3w" title="K najbliższych sąsiadów – Polish" lang="pl" hreflang="pl" data-title="K najbliższych sąsiadów" data-language-autonym="Polski" data-language-local-name="Polish" class="interlanguage-link-target"><span>Polski</span></a></li><li class="interlanguage-link interwiki-pt mw-list-item"><a href="https://pt.wikipedia.org/wiki/Algoritmo_dos_k-vizinhos_mais_pr%C3%B3ximos" title="Algoritmo dos k-vizinhos mais próximos – Portuguese" lang="pt" hreflang="pt" data-title="Algoritmo dos k-vizinhos mais próximos" data-language-autonym="Português" data-language-local-name="Portuguese" class="interlanguage-link-target"><span>Português</span></a></li><li class="interlanguage-link interwiki-ru mw-list-item"><a href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k_%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9" title="Метод k ближайших соседей – Russian" lang="ru" hreflang="ru" data-title="Метод k ближайших соседей" data-language-autonym="Русский" data-language-local-name="Russian" class="interlanguage-link-target"><span>Русский</span></a></li><li class="interlanguage-link interwiki-sr mw-list-item"><a href="https://sr.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%B0%D0%BC_%D0%BA_%D0%BD%D0%B0%D1%98%D0%B1%D0%BB%D0%B8%D0%B6%D0%B8%D1%85_%D1%81%D1%83%D1%81%D0%B5%D0%B4%D0%B0" title="Алгоритам к најближих суседа – Serbian" lang="sr" hreflang="sr" data-title="Алгоритам к најближих суседа" data-language-autonym="Српски / srpski" data-language-local-name="Serbian" class="interlanguage-link-target"><span>Српски / srpski</span></a></li><li class="interlanguage-link interwiki-th mw-list-item"><a href="https://th.wikipedia.org/wiki/%E0%B8%82%E0%B8%B1%E0%B9%89%E0%B8%99%E0%B8%95%E0%B8%AD%E0%B8%99%E0%B8%A7%E0%B8%B4%E0%B8%98%E0%B8%B5%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%84%E0%B9%89%E0%B8%99%E0%B8%AB%E0%B8%B2%E0%B9%80%E0%B8%9E%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%99%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%99%E0%B9%83%E0%B8%81%E0%B8%A5%E0%B9%89%E0%B8%AA%E0%B8%B8%E0%B8%94_k_%E0%B8%95%E0%B8%B1%E0%B8%A7" title="ขั้นตอนวิธีการค้นหาเพื่อนบ้านใกล้สุด k ตัว – Thai" lang="th" hreflang="th" data-title="ขั้นตอนวิธีการค้นหาเพื่อนบ้านใกล้สุด k ตัว" data-language-autonym="ไทย" data-language-local-name="Thai" class="interlanguage-link-target"><span>ไทย</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D0%BD%D0%B0%D0%B9%D0%B1%D0%BB%D0%B8%D0%B6%D1%87%D0%B8%D1%85_%D1%81%D1%83%D1%81%D1%96%D0%B4%D1%96%D0%B2" title="Метод k-найближчих сусідів – Ukrainian" lang="uk" hreflang="uk" data-title="Метод k-найближчих сусідів" data-language-autonym="Українська" data-language-local-name="Ukrainian" class="interlanguage-link-target"><span>Українська</span></a></li><li class="interlanguage-link interwiki-vi mw-list-item"><a href="https://vi.wikipedia.org/wiki/Gi%E1%BA%A3i_thu%E1%BA%ADt_k_h%C3%A0ng_x%C3%B3m_g%E1%BA%A7n_nh%E1%BA%A5t" title="Giải thuật k hàng xóm gần nhất – Vietnamese" lang="vi" hreflang="vi" data-title="Giải thuật k hàng xóm gần nhất" data-language-autonym="Tiếng Việt" data-language-local-name="Vietnamese" class="interlanguage-link-target"><span>Tiếng Việt</span></a></li><li class="interlanguage-link interwiki-zh mw-list-item"><a href="https://zh.wikipedia.org/wiki/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" title="K-近邻算法 – Chinese" lang="zh" hreflang="zh" data-title="K-近邻算法" data-language-autonym="中文" data-language-local-name="Chinese" class="interlanguage-link-target"><span>中文</span></a></li>
			</ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q1071612#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
</header>
				<div class="vector-page-toolbar vector-feature-custom-font-size-clientpref--excluded">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/K-nearest_neighbors_algorithm" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="vector-tab-noicon mw-list-item"><a href="/wiki/Talk:K-nearest_neighbors_algorithm" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

								
<div id="vector-variants-dropdown" class="vector-dropdown emptyPortlet"  >
	<input type="checkbox" id="vector-variants-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-variants-dropdown" class="vector-dropdown-checkbox " aria-label="Change language variant"   >
	<label id="vector-variants-dropdown-label" for="vector-variants-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">


					
<div id="p-variants" class="vector-menu mw-portlet mw-portlet-variants emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

				
	</div>
</div>

							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/K-nearest_neighbors_algorithm"><span>Read</span></a></li><li id="ca-edit" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="Page tools">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox" id="vector-page-tools-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-tools-dropdown" class="vector-dropdown-checkbox "  aria-label="Tools"  >
	<label id="vector-page-tools-dropdown-label" for="vector-page-tools-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/K-nearest_neighbors_algorithm"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=history"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/K-nearest_neighbors_algorithm" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/K-nearest_neighbors_algorithm" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=1328986210" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=K-nearest_neighbors_algorithm&amp;id=1328986210&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-urlshortener" class="mw-list-item"><a href="/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FK-nearest_neighbors_algorithm"><span>Get shortened URL</span></a></li><li id="t-urlshortener-qrcode" class="mw-list-item"><a href="/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FK-nearest_neighbors_algorithm"><span>Download QR code</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=K-nearest_neighbors_algorithm&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-wikibase-otherprojects" class="vector-menu mw-portlet mw-portlet-wikibase-otherprojects"  >
	<div class="vector-menu-heading">
		In other projects
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-wikibase" class="wb-otherproject-link wb-otherproject-wikibase-dataitem mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q1071612" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li>
		</ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>

							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end no-font-mode-scale">
					<div class="vector-sticky-pinned-container">
						<div class="wp25eastereggs-vector-sitenotice-landmark"></div>
						<nav class="vector-page-tools-landmark" aria-label="Page tools">
							<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
				
							</div>
		</nav>
						<nav class="vector-appearance-landmark" aria-label="Appearance">
							<div id="vector-appearance-pinned-container" class="vector-pinned-container">
				<div id="vector-appearance" class="vector-appearance vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-appearance-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="appearance-pinned"
	data-pinnable-element-id="vector-appearance"
	data-pinned-container-id="vector-appearance-pinned-container"
	data-unpinned-container-id="vector-appearance-unpinned-container"
>
	<div class="vector-pinnable-header-label">Appearance</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-appearance.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-appearance.unpin">hide</button>
</div>


</div>

							</div>
		</nav>
					</div>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content"><div class="mw-subjectpageheader">
</div><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Non-parametric classification method</div>
<style data-mw-deduplicate="TemplateStyles:r1320445320">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}</style><div role="note" class="hatnote navigation-not-searchable">Not to be confused with <a href="/wiki/Nearest_neighbor_search" title="Nearest neighbor search">Nearest neighbor search</a>, <a href="/wiki/Nearest_neighbor_interpolation" class="mw-redirect" title="Nearest neighbor interpolation">Nearest neighbor interpolation</a>, or <a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means clustering</a>.</div>
<p>In <a href="/wiki/Statistics" title="Statistics">statistics</a>, the <b><i>k</i>-nearest neighbors algorithm</b> (<b><i>k</i>-NN</b>) is a <a href="/wiki/Non-parametric_statistics" class="mw-redirect" title="Non-parametric statistics">non-parametric</a> <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> method. It was first developed by <a href="/wiki/Evelyn_Fix" title="Evelyn Fix">Evelyn Fix</a> and <a href="/wiki/Joseph_Lawson_Hodges_Jr." title="Joseph Lawson Hodges Jr.">Joseph Hodges</a> in 1951,<sup id="cite&#95;ref-1" class="reference"><a href="#cite_note-1"><span class="cite-bracket">&#91;</span>1<span class="cite-bracket">&#93;</span></a></sup> and later expanded by <a href="/wiki/Thomas_M._Cover" title="Thomas M. Cover">Thomas Cover</a>.<sup id="cite&#95;ref-:1&#95;2-0" class="reference"><a href="#cite_note-:1-2"><span class="cite-bracket">&#91;</span>2<span class="cite-bracket">&#93;</span></a></sup> 
Most often, it is used for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>, as a <b><i>k</i>-NN classifier</b>, the output of which is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its <i>k</i> nearest neighbors (<i>k</i> is a positive <a href="/wiki/Integer" title="Integer">integer</a>, typically small). If <i>k</i>&#160;=&#160;1, then the object is simply assigned to the class of that single nearest neighbor.
</p><p>The <i>k</i>-NN algorithm can also be generalized for <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>. In <i><span class="texhtml mvar" style="font-style:italic;">k</span>-NN regression</i>, also known as <i><a href="/wiki/Nearest_neighbor_smoothing" class="mw-redirect" title="Nearest neighbor smoothing">nearest neighbor smoothing</a></i>, the output is the property value for the object. This value is the average of the values of <i>k</i> nearest neighbors. If <i>k</i>&#160;=&#160;1, then the output is simply assigned to the value of that single nearest neighbor, also known as <i><a href="/wiki/Nearest_neighbor_interpolation" class="mw-redirect" title="Nearest neighbor interpolation">nearest neighbor interpolation</a></i>.
</p><p>For both classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that nearer neighbors contribute more to the average than distant ones. For example, a common weighting scheme consists of giving each neighbor a weight of 1/<i>d</i>, where <i>d</i> is the distance to the neighbor.<sup id="cite&#95;ref-3" class="reference"><a href="#cite_note-3"><span class="cite-bracket">&#91;</span>3<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The input consists of the <i>k</i> closest training examples in a <a href="/wiki/Data_set" title="Data set">data set</a>. 
The neighbors are taken from a set of objects for which the class (for <i>k</i>-NN classification) or the object property value (for <i>k</i>-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
</p><p>A peculiarity (sometimes even a disadvantage) of the <i>k</i>-NN algorithm is its sensitivity to the local structure of the data.
In <i>k</i>-NN classification the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance, if the features represent different physical units or come in vastly different scales, then feature-wise <a href="/wiki/Normalization_(statistics)" title="Normalization (statistics)">normalizing</a> of the training data can greatly improve its accuracy.<sup id="cite&#95;ref-4" class="reference"><a href="#cite_note-4"><span class="cite-bracket">&#91;</span>4<span class="cite-bracket">&#93;</span></a></sup>
</p>
<meta property="mw:PageProp/toc" />
<div class="mw-heading mw-heading2"><h2 id="Statistical_setting">Statistical setting</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=1" title="Edit section: Statistical setting"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Suppose we have pairs <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f16142bcc75fbc4a7fece97df2d93185e831185" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:31.22ex; height:2.843ex;" alt="{\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}"></span> taking values in <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbb {R} ^{d}\times \{1,2\}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
        <mo>&#x00D7;<!-- × --></mo>
        <mo fence="false" stretchy="false">{</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>2</mn>
        <mo fence="false" stretchy="false">}</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbb {R} ^{d}\times \{1,2\}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60671efc51865fcfac4e8939ab2acb643539302d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:11.294ex; height:3.176ex;" alt="{\displaystyle \mathbb {R} ^{d}\times \{1,2\}}"></span>, where <span class="texhtml mvar" style="font-style:italic;">Y</span> is the class label of <span class="texhtml mvar" style="font-style:italic;">X</span>, so that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle X|Y=r\sim P_{r}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>X</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>Y</mi>
        <mo>=</mo>
        <mi>r</mi>
        <mo>&#x223C;<!-- ∼ --></mo>
        <msub>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>r</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle X|Y=r\sim P_{r}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e1b6a01b179f721be03ed284c21e2d3cbb70d4a" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:14.112ex; height:2.843ex;" alt="{\displaystyle X|Y=r\sim P_{r}}"></span> for <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle r=1,2}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>r</mi>
        <mo>=</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>2</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle r=1,2}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6dcdedf1b83451b9a51dc73b95f854eeaf83dc46" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:7.506ex; height:2.509ex;" alt="{\displaystyle r=1,2}"></span> (and probability distributions <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P_{r}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>r</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P_{r}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3e7f814e80c6ff1469112bd4b7430e358e86c7d6" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.466ex; height:2.509ex;" alt="{\displaystyle P_{r}}"></span>). Given some norm <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \|\cdot \|}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|\cdot \|}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/113f0d8fe6108fc1c5e9802f7c3f634f5480b3d1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:4.004ex; height:2.843ex;" alt="{\displaystyle \|\cdot \|}"></span> on <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbb {R} ^{d}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbb {R} ^{d}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a713426956296f1668fce772df3c60b9dde8a685" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.77ex; height:2.676ex;" alt="{\displaystyle \mathbb {R} ^{d}}"></span> and a point <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x\in \mathbb {R} ^{d}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x\in \mathbb {R} ^{d}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f351538c1465ec3881164b501f612b1f54cbfe7e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:6.94ex; height:2.676ex;" alt="{\displaystyle x\in \mathbb {R} ^{d}}"></span>, let <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961b632956473933c75b8cbfc56f8a83ccd41524" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.171ex; width:27.077ex; height:3.176ex;" alt="{\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}"></span> be a reordering of the training data such that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>&#x2212;<!-- − --></mo>
        <mi>x</mi>
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mo>&#x22EF;<!-- ⋯ --></mo>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>&#x2212;<!-- − --></mo>
        <mi>x</mi>
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/364eb25128db023e58f0dc8584f7e6de07e94907" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.171ex; width:30.59ex; height:3.176ex;" alt="{\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}"></span>.
</p>
<div class="mw-heading mw-heading2"><h2 id="Algorithm">Algorithm</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=2" title="Edit section: Algorithm"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure class="mw-default-size mw-halign-right" typeof="mw:File/Thumb"><a href="/wiki/File:KnnClassification.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/250px-KnnClassification.svg.png" decoding="async" width="250" height="226" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/500px-KnnClassification.svg.png 1.5x" data-file-width="279" data-file-height="252" /></a><figcaption>Example of <i>k</i>-NN classification. The test sample (green dot) should be classified either to blue squares or to red triangles.  If <i>k</i> = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle.  If <i>k</i> = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).</figcaption></figure>
<p>The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the <a href="/wiki/Feature_vector" class="mw-redirect" title="Feature vector">feature vectors</a> and class labels of the training samples.
</p><p>In the classification phase, <i>k</i> is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the <i>k</i> training samples nearest to that query point.
</p>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:KNN_decision_surface_animation.gif" class="mw-file-description"><img alt="kNN decision surface" src="//upload.wikimedia.org/wikipedia/commons/thumb/7/78/KNN_decision_surface_animation.gif/250px-KNN_decision_surface_animation.gif" decoding="async" width="250" height="125" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/78/KNN_decision_surface_animation.gif/500px-KNN_decision_surface_animation.gif 1.5x" data-file-width="1200" data-file-height="600" /></a><figcaption>Application of a <i>k-</i>NN classifier considering <i>k</i> = 3 neighbors. Left - Given the test point "?", the algorithm seeks the 3 closest points in the training set, and adopts the majority vote to classify it as "class red". Right - By iteratively repeating the prediction over the whole feature space (X1, X2), one can depict the "decision surface".</figcaption></figure>
<p>A commonly used distance metric for <a href="/wiki/Continuous_variable" class="mw-redirect" title="Continuous variable">continuous variables</a> is <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a>. For discrete variables, such as for text classification, another metric can be used, such as the <b>overlap metric</b> (or <a href="/wiki/Hamming_distance" title="Hamming distance">Hamming distance</a>). In the context of gene expression microarray data, for example, <i>k</i>-NN has been employed with correlation coefficients, such as Pearson and Spearman, as a metric.<sup id="cite&#95;ref-5" class="reference"><a href="#cite_note-5"><span class="cite-bracket">&#91;</span>5<span class="cite-bracket">&#93;</span></a></sup> Often, the classification accuracy of <i>k</i>-NN can be improved significantly if the distance metric is learned with specialized algorithms such as <a href="/wiki/Large_margin_nearest_neighbor" title="Large margin nearest neighbor">large margin nearest neighbor</a> or <a href="/wiki/Neighborhood_components_analysis" class="mw-redirect" title="Neighborhood components analysis">neighborhood components analysis</a>.
</p>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Kmeans_clustering_WHR2023_data.gif" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Kmeans_clustering_WHR2023_data.gif/250px-Kmeans_clustering_WHR2023_data.gif" decoding="async" width="250" height="188" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Kmeans_clustering_WHR2023_data.gif/500px-Kmeans_clustering_WHR2023_data.gif 1.5x" data-file-width="1600" data-file-height="1200" /></a><figcaption>An animated visualization of <i>k</i>-means clustering with <i>k</i> = 3, grouping countries based on life expectancy, GDP, and happiness—demonstrating how <i>k</i>-NN operates in higher dimensions. Click to view the animation.<sup id="cite&#95;ref-6" class="reference"><a href="#cite_note-6"><span class="cite-bracket">&#91;</span>6<span class="cite-bracket">&#93;</span></a></sup></figcaption></figure>
<p>A drawback of the basic "majority voting" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the <i>k</i> nearest neighbors due to their large number.<sup id="cite&#95;ref-Coomans&#95;Massart1982&#95;7-0" class="reference"><a href="#cite_note-Coomans_Massart1982-7"><span class="cite-bracket">&#91;</span>7<span class="cite-bracket">&#93;</span></a></sup> One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its <i>k</i> nearest neighbors. The class (or value, in regression problems) of each of the <i>k</i> nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a <a href="/wiki/Self-organizing_map" title="Self-organizing map">self-organizing map</a> (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. <i>k</i>-NN can then be applied to the SOM.
</p>
<div class="mw-heading mw-heading2"><h2 id="Parameter_selection">Parameter selection</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=3" title="Edit section: Parameter selection"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The best choice of <i>k</i> depends upon the data; generally, larger values of <i>k</i> reduces effect of the noise on the classification,<sup id="cite&#95;ref-8" class="reference"><a href="#cite_note-8"><span class="cite-bracket">&#91;</span>8<span class="cite-bracket">&#93;</span></a></sup> but make boundaries between classes less distinct. A good <i>k</i> can be selected by various <a href="/wiki/Heuristic_(computer_science)" title="Heuristic (computer science)">heuristic</a> techniques (see <a href="/wiki/Hyperparameter_optimization" title="Hyperparameter optimization">hyperparameter optimization</a>). The special case where the class is predicted to be the class of the closest training sample (i.e. when <i>k</i> = 1) is called the nearest neighbor algorithm.
</p><p>The accuracy of the <i>k</i>-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into <a href="/wiki/Feature_selection" title="Feature selection">selecting</a> or <a href="/wiki/Feature_scaling" title="Feature scaling">scaling</a> features to improve classification. A particularly popular<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2013)">citation needed</span></a></i>&#93;</sup> approach is the use of <a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">evolutionary algorithms</a> to optimize feature scaling.<sup id="cite&#95;ref-9" class="reference"><a href="#cite_note-9"><span class="cite-bracket">&#91;</span>9<span class="cite-bracket">&#93;</span></a></sup> Another popular approach is to scale features by the <a href="/wiki/Mutual_information" title="Mutual information">mutual information</a> of the training data with the training classes.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2008)">citation needed</span></a></i>&#93;</sup>
</p><p>In binary (two class) classification problems, it is helpful to choose <i>k</i> to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal <i>k</i> in this setting is via bootstrap method.<sup id="cite&#95;ref-HPS2008&#95;10-0" class="reference"><a href="#cite_note-HPS2008-10"><span class="cite-bracket">&#91;</span>10<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="The_1-nearest_neighbor_classifier">The 1-nearest neighbor classifier</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=4" title="Edit section: The 1-nearest neighbor classifier"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point <span class="texhtml mvar" style="font-style:italic;">x</span> to the class of its closest neighbour in the feature space, that is <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f815904edbff2ce82502172ec0dce3311d57f2bb" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.171ex; width:14.746ex; height:3.343ex;" alt="{\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}"></span>.
</p><p>As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the <a href="/wiki/Bayes_error_rate" title="Bayes error rate">Bayes error rate</a> (the minimum achievable error rate given the distribution of the data).
</p>
<div class="mw-heading mw-heading2"><h2 id="The_weighted_nearest_neighbour_classifier">The weighted nearest neighbour classifier</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=5" title="Edit section: The weighted nearest neighbour classifier"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The <span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbour classifier can be viewed as assigning the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbours a weight <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle 1/k}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mn>1</mn>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi>k</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 1/k}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a7e9fedad8c70c6331b2640b56c23cef8c884e1f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:3.536ex; height:2.843ex;" alt="{\displaystyle 1/k}"></span> and all others <span class="texhtml mvar" style="font-style:italic;">0</span> weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the <span class="texhtml mvar" style="font-style:italic;">i</span>th nearest neighbour is assigned a weight <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ni}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ni}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6d76326293e410139d081d073068b9eb32a0777" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:3.45ex; height:2.009ex;" alt="{\displaystyle w_{ni}}"></span>, with <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle \sum _{i=1}^{n}w_{ni}=1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle \sum _{i=1}^{n}w_{ni}=1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/453bf04bad39cefb5ac7d959617a064c8fb019f4" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:13.452ex; height:3.176ex;" alt="{\textstyle \sum _{i=1}^{n}w_{ni}=1}"></span>. An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds.<sup id="cite&#95;ref-Stone&#95;11-0" class="reference"><a href="#cite_note-Stone-11"><span class="cite-bracket">&#91;</span>11<span class="cite-bracket">&#93;</span></a></sup>
</p><p>Let <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C_{n}^{wnn}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>w</mi>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C_{n}^{wnn}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e88b657ee88d912408396f8c9ef6af3483bfdf01" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:5.179ex; height:2.509ex;" alt="{\displaystyle C_{n}^{wnn}}"></span> denote the weighted nearest classifier with weights <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \{w_{ni}\}_{i=1}^{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">{</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
        </msub>
        <msubsup>
          <mo fence="false" stretchy="false">}</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \{w_{ni}\}_{i=1}^{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/efaf258e02ccae2b27c279885d6fb898adaf331d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:8.675ex; height:3.009ex;" alt="{\displaystyle \{w_{ni}\}_{i=1}^{n}}"></span>. Subject to regularity conditions, which in asymptotic theory are conditional variables which require assumptions to differentiate among parameters with some criteria. On the class distributions the excess risk has the following asymptotic expansion<sup id="cite&#95;ref-Samworth12&#95;12-0" class="reference"><a href="#cite_note-Samworth12-12"><span class="cite-bracket">&#91;</span>12<span class="cite-bracket">&#93;</span></a></sup>
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{wnn})-{\mathcal {R}}_{\mathcal {R}}(C^{\text{Bayes}})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>w</mi>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>Bayes</mtext>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mi>B</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
            <msubsup>
              <mi>s</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>n</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msubsup>
            <mo>+</mo>
            <msub>
              <mi>B</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msub>
            <msubsup>
              <mi>t</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>n</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msubsup>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo fence="false" stretchy="false">{</mo>
        <mn>1</mn>
        <mo>+</mo>
        <mi>o</mi>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo fence="false" stretchy="false">}</mo>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{wnn})-{\mathcal {R}}_{\mathcal {R}}(C^{\text{Bayes}})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c7c982e82048fdd777a07ab75b9badca94c91f8a" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:54.626ex; height:3.343ex;" alt="{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{wnn})-{\mathcal {R}}_{\mathcal {R}}(C^{\text{Bayes}})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},}"></span>
for constants <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle B_{1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>B</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle B_{1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fa091eb428443c9c5c5fcf32a69d3665c89e00c" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;" alt="{\displaystyle B_{1}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle B_{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>B</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle B_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/199944d59dcc18842dfd1deab6000a1d1dadcbae" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;" alt="{\displaystyle B_{2}}"></span> where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle s_{n}^{2}=\sum _{i=1}^{n}w_{ni}^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>s</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle s_{n}^{2}=\sum _{i=1}^{n}w_{ni}^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ed6dbd702b3141f1649ce10ccff3bac0acd55299" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.005ex; width:12.599ex; height:6.843ex;" alt="{\displaystyle s_{n}^{2}=\sum _{i=1}^{n}w_{ni}^{2}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle t_{n}=n^{-2/d}\sum _{i=1}^{n}w_{ni}\left\{i^{1+2/d}-(i-1)^{1+2/d}\right\}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mn>2</mn>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>/</mo>
            </mrow>
            <mi>d</mi>
          </mrow>
        </msup>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
        </msub>
        <mrow>
          <mo>{</mo>
          <mrow>
            <msup>
              <mi>i</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
                <mo>+</mo>
                <mn>2</mn>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>/</mo>
                </mrow>
                <mi>d</mi>
              </mrow>
            </msup>
            <mo>&#x2212;<!-- − --></mo>
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
            <msup>
              <mo stretchy="false">)</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
                <mo>+</mo>
                <mn>2</mn>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>/</mo>
                </mrow>
                <mi>d</mi>
              </mrow>
            </msup>
          </mrow>
          <mo>}</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle t_{n}=n^{-2/d}\sum _{i=1}^{n}w_{ni}\left\{i^{1+2/d}-(i-1)^{1+2/d}\right\}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04727a01d2d66dd364bf81b04e9682bd5f5879a5" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.005ex; width:41.563ex; height:6.843ex;" alt="{\displaystyle t_{n}=n^{-2/d}\sum _{i=1}^{n}w_{ni}\left\{i^{1+2/d}-(i-1)^{1+2/d}\right\}}"></span>.
</p><p>The optimal weighting scheme <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \{w_{ni}^{*}\}_{i=1}^{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">{</mo>
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msubsup>
        <msubsup>
          <mo fence="false" stretchy="false">}</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \{w_{ni}^{*}\}_{i=1}^{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f97b387c9e937fac91f0644ac895c5c95d9a4921" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:8.675ex; height:3.009ex;" alt="{\displaystyle \{w_{ni}^{*}\}_{i=1}^{n}}"></span>, that balances the two terms in the display above, is given as follows: set <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mo fence="false" stretchy="false">&#x230A;<!-- ⌊ --></mo>
        <mi>B</mi>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mfrac>
              <mn>4</mn>
              <mrow>
                <mi>d</mi>
                <mo>+</mo>
                <mn>4</mn>
              </mrow>
            </mfrac>
          </mrow>
        </msup>
        <mo fence="false" stretchy="false">&#x230B;<!-- ⌋ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5cbecc881f1b8637b3d4d4527fd1671f5be252fa" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:14.059ex; height:4.176ex;" alt="{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }"></span>, 
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ni}^{*}={\frac {1}{k^{*}}}\left[1+{\frac {d}{2}}-{\frac {d}{2{k^{*}}^{2/d}}}\{i^{1+2/d}-(i-1)^{1+2/d}\}\right]}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <msup>
              <mi>k</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>&#x2217;<!-- ∗ --></mo>
              </mrow>
            </msup>
          </mfrac>
        </mrow>
        <mrow>
          <mo>[</mo>
          <mrow>
            <mn>1</mn>
            <mo>+</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mi>d</mi>
                <mn>2</mn>
              </mfrac>
            </mrow>
            <mo>&#x2212;<!-- − --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mi>d</mi>
                <mrow>
                  <mn>2</mn>
                  <msup>
                    <mrow class="MJX-TeXAtom-ORD">
                      <msup>
                        <mi>k</mi>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mo>&#x2217;<!-- ∗ --></mo>
                        </mrow>
                      </msup>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mo>/</mo>
                      </mrow>
                      <mi>d</mi>
                    </mrow>
                  </msup>
                </mrow>
              </mfrac>
            </mrow>
            <mo fence="false" stretchy="false">{</mo>
            <msup>
              <mi>i</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
                <mo>+</mo>
                <mn>2</mn>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>/</mo>
                </mrow>
                <mi>d</mi>
              </mrow>
            </msup>
            <mo>&#x2212;<!-- − --></mo>
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
            <msup>
              <mo stretchy="false">)</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
                <mo>+</mo>
                <mn>2</mn>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>/</mo>
                </mrow>
                <mi>d</mi>
              </mrow>
            </msup>
            <mo fence="false" stretchy="false">}</mo>
          </mrow>
          <mo>]</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ni}^{*}={\frac {1}{k^{*}}}\left[1+{\frac {d}{2}}-{\frac {d}{2{k^{*}}^{2/d}}}\{i^{1+2/d}-(i-1)^{1+2/d}\}\right]}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fbfa4058134234385c31544db3e657c4b242ab34" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.505ex; width:50.643ex; height:6.176ex;" alt="{\displaystyle w_{ni}^{*}={\frac {1}{k^{*}}}\left[1+{\frac {d}{2}}-{\frac {d}{2{k^{*}}^{2/d}}}\{i^{1+2/d}-(i-1)^{1+2/d}\}\right]}"></span> for <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle i=1,2,\dots ,k^{*}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>2</mn>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i=1,2,\dots ,k^{*}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ea7e974f7466c9dedfe409ea013bc719264872b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:14.703ex; height:2.676ex;" alt="{\displaystyle i=1,2,\dots ,k^{*}}"></span> and 
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{ni}^{*}=0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ni}^{*}=0}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6bd3d5b77d7fef0dabd4326ee85b04fa244fa988" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:7.711ex; height:2.843ex;" alt="{\displaystyle w_{ni}^{*}=0}"></span> for <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle i=k^{*}+1,\dots ,n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
        <mo>=</mo>
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mo>+</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i=k^{*}+1,\dots ,n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7c2f2e5c12f3febcefe0aae189d44031daf8e79" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:16.742ex; height:2.676ex;" alt="{\displaystyle i=k^{*}+1,\dots ,n}"></span>.
</p><p>With optimal weights the dominant term in the asymptotic expansion of the excess risk is <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>4</mn>
                <mrow>
                  <mi>d</mi>
                  <mo>+</mo>
                  <mn>4</mn>
                </mrow>
              </mfrac>
            </mrow>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07280735a8852d609ffd3942647d7e3255697f05" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:9.804ex; height:4.176ex;" alt="{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}"></span>. Similar results are true when using a <a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">bagged nearest neighbour classifier</a>.
</p>
<div class="mw-heading mw-heading2"><h2 id="Properties">Properties</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=6" title="Edit section: Properties"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p><i>k</i>-NN is a special case of a <a href="/wiki/Variable_kernel_density_estimation" title="Variable kernel density estimation">variable-bandwidth, kernel density "balloon" estimator</a> with a uniform <a href="/wiki/Kernel_(statistics)" title="Kernel (statistics)">kernel</a>.<sup id="cite&#95;ref-Terrell&#95;Scott1992&#95;13-0" class="reference"><a href="#cite_note-Terrell_Scott1992-13"><span class="cite-bracket">&#91;</span>13<span class="cite-bracket">&#93;</span></a></sup><sup id="cite&#95;ref-Mills2010&#95;14-0" class="reference"><a href="#cite_note-Mills2010-14"><span class="cite-bracket">&#91;</span>14<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate <a href="/wiki/Nearest_neighbor_search" title="Nearest neighbor search">nearest neighbor search</a> algorithm makes <i>k-</i>NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.
</p><p><i>k-</i>NN has some strong <a href="/wiki/Consistency_(statistics)" title="Consistency (statistics)">consistency</a> results. As the amount of data approaches infinity, the two-class <i>k-</i>NN algorithm is guaranteed to yield an error rate no worse than twice the <a href="/wiki/Bayes_error_rate" title="Bayes error rate">Bayes error rate</a> (the minimum achievable error rate given the distribution of the data).<sup id="cite&#95;ref-:1&#95;2-1" class="reference"><a href="#cite_note-:1-2"><span class="cite-bracket">&#91;</span>2<span class="cite-bracket">&#93;</span></a></sup> Various improvements to the <i>k</i>-NN speed are possible by using proximity graphs.<sup id="cite&#95;ref-15" class="reference"><a href="#cite_note-15"><span class="cite-bracket">&#91;</span>15<span class="cite-bracket">&#93;</span></a></sup>
</p><p>For multi-class <i>k-</i>NN classification, <a href="/wiki/Thomas_M._Cover" title="Thomas M. Cover">Cover</a> and <a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart</a> (1967) prove an upper bound error rate of
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle R^{*}\ \leq \ R_{k\mathrm {NN} }\ \leq \ R^{*}\left(2-{\frac {MR^{*}}{M-1}}\right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mtext>&#xA0;</mtext>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mtext>&#xA0;</mtext>
        <msub>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="normal">N</mi>
              <mi mathvariant="normal">N</mi>
            </mrow>
          </mrow>
        </msub>
        <mtext>&#xA0;</mtext>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mtext>&#xA0;</mtext>
        <msup>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mi>M</mi>
                  <msup>
                    <mi>R</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mo>&#x2217;<!-- ∗ --></mo>
                    </mrow>
                  </msup>
                </mrow>
                <mrow>
                  <mi>M</mi>
                  <mo>&#x2212;<!-- − --></mo>
                  <mn>1</mn>
                </mrow>
              </mfrac>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R^{*}\ \leq \ R_{k\mathrm {NN} }\ \leq \ R^{*}\left(2-{\frac {MR^{*}}{M-1}}\right)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3a20a21397df93ca13f8098a25731888f763efe7" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.505ex; width:34.566ex; height:6.176ex;" alt="{\displaystyle R^{*}\ \leq \ R_{k\mathrm {NN} }\ \leq \ R^{*}\left(2-{\frac {MR^{*}}{M-1}}\right)}"></span>
where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle R^{*}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R^{*}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a74b1bc0fa98794b6460254044f8e7b75e6d84f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.818ex; height:2.343ex;" alt="{\displaystyle R^{*}}"></span> is the Bayes error rate (which is the minimal error rate possible), <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle R_{kNN}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mi>N</mi>
            <mi>N</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R_{kNN}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f3666637c81f02c1bfeb4e1969828745062aeba" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:5.771ex; height:2.509ex;" alt="{\displaystyle R_{kNN}}"></span> is the asymptotic <i>k-</i>NN error rate, and <span class="texhtml mvar" style="font-style:italic;">M</span> is the number of classes in the problem. This bound is tight in the sense that both the lower and upper bounds are achievable by some distribution.<sup id="cite&#95;ref-16" class="reference"><a href="#cite_note-16"><span class="cite-bracket">&#91;</span>16<span class="cite-bracket">&#93;</span></a></sup> For <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle M=2}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>M</mi>
        <mo>=</mo>
        <mn>2</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle M=2}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2801ac77396e68de0b640087e1531a2329067e9f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:6.703ex; height:2.176ex;" alt="{\displaystyle M=2}"></span> and as the Bayesian error rate <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle R^{*}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R^{*}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a74b1bc0fa98794b6460254044f8e7b75e6d84f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.818ex; height:2.343ex;" alt="{\displaystyle R^{*}}"></span> approaches zero, this limit reduces to "not more than twice the Bayesian error rate".
</p>
<div class="mw-heading mw-heading2"><h2 id="Error_rates">Error rates</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=7" title="Edit section: Error rates"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>There are many results on the error rate of the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbour classifiers.<sup id="cite&#95;ref-PTPR&#95;17-0" class="reference"><a href="#cite_note-PTPR-17"><span class="cite-bracket">&#91;</span>17<span class="cite-bracket">&#93;</span></a></sup> The <span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbour classifier is strongly (that is for any joint distribution on <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle (X,Y)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo>,</mo>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (X,Y)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/41f29b9537685f499713112d6802e811cbf51bba" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:6.597ex; height:2.843ex;" alt="{\displaystyle (X,Y)}"></span>) <a href="/wiki/Bayes_classifier" title="Bayes classifier">consistent</a> provided <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k:=k_{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>:=</mo>
        <msub>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k:=k_{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0fccbcda1dd871e437cca392a4294e6affdb5692" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:7.386ex; height:2.509ex;" alt="{\displaystyle k:=k_{n}}"></span> diverges and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k_{n}/n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k_{n}/n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4e9a4dfc54979e5190d6baa3cc552824357edca1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:4.987ex; height:2.843ex;" alt="{\displaystyle k_{n}/n}"></span> converges to zero as <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle n\to \infty }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>n</mi>
        <mo stretchy="false">&#x2192;<!-- → --></mo>
        <mi mathvariant="normal">&#x221E;<!-- ∞ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle n\to \infty }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0d55d9b32f6fa8fab6a84ea444a6b5a24bb45e1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:7.333ex; height:1.843ex;" alt="{\displaystyle n\to \infty }"></span>.
</p><p>Let <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C_{n}^{knn}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C_{n}^{knn}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3f2588b76da9c7429b56cec54535a18a8cae6386" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:4.859ex; height:2.843ex;" alt="{\displaystyle C_{n}^{knn}}"></span> denote the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbour classifier based on a training set of size <span class="texhtml mvar" style="font-style:italic;">n</span>. Under certain regularity conditions, the <a href="/wiki/Bayes_classifier" title="Bayes classifier">excess risk</a> yields the following asymptotic expansion<sup id="cite&#95;ref-Samworth12&#95;12-1" class="reference"><a href="#cite_note-Samworth12-12"><span class="cite-bracket">&#91;</span>12<span class="cite-bracket">&#93;</span></a></sup>
<span class="mwe-math-element mwe-math-element-block"><span class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{knn})-{\mathcal {R}}_{\mathcal {R}}(C^{\text{Bayes}})=\left\{B_{1}{\frac {1}{k}}+B_{2}\left({\frac {k}{n}}\right)^{4/d}\right\}\{1+o(1)\},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>Bayes</mtext>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <mrow>
            <msub>
              <mi>B</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>1</mn>
                <mi>k</mi>
              </mfrac>
            </mrow>
            <mo>+</mo>
            <msub>
              <mi>B</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msub>
            <msup>
              <mrow>
                <mo>(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mfrac>
                    <mi>k</mi>
                    <mi>n</mi>
                  </mfrac>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>4</mn>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>/</mo>
                </mrow>
                <mi>d</mi>
              </mrow>
            </msup>
          </mrow>
          <mo>}</mo>
        </mrow>
        <mo fence="false" stretchy="false">{</mo>
        <mn>1</mn>
        <mo>+</mo>
        <mi>o</mi>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo fence="false" stretchy="false">}</mo>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{knn})-{\mathcal {R}}_{\mathcal {R}}(C^{\text{Bayes}})=\left\{B_{1}{\frac {1}{k}}+B_{2}\left({\frac {k}{n}}\right)^{4/d}\right\}\{1+o(1)\},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ddcac0be444f220c76c4d4f4e75c48c18211b871" class="mwe-math-fallback-image-display mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:61.991ex; height:7.509ex;" alt="{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{knn})-{\mathcal {R}}_{\mathcal {R}}(C^{\text{Bayes}})=\left\{B_{1}{\frac {1}{k}}+B_{2}\left({\frac {k}{n}}\right)^{4/d}\right\}\{1+o(1)\},}"></span>
for some constants <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle B_{1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>B</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle B_{1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fa091eb428443c9c5c5fcf32a69d3665c89e00c" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;" alt="{\displaystyle B_{1}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle B_{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>B</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle B_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/199944d59dcc18842dfd1deab6000a1d1dadcbae" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;" alt="{\displaystyle B_{2}}"></span>.
</p><p>The choice <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k^{*}=\left\lfloor Bn^{\frac {4}{d+4}}\right\rfloor }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mrow>
          <mo>&#x230A;</mo>
          <mrow>
            <mi>B</mi>
            <msup>
              <mi>n</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mfrac>
                  <mn>4</mn>
                  <mrow>
                    <mi>d</mi>
                    <mo>+</mo>
                    <mn>4</mn>
                  </mrow>
                </mfrac>
              </mrow>
            </msup>
          </mrow>
          <mo>&#x230B;</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k^{*}=\left\lfloor Bn^{\frac {4}{d+4}}\right\rfloor }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/175e72cb7ca054e85a64d707b85b98e99f660057" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.505ex; width:14.705ex; height:6.176ex;" alt="{\displaystyle k^{*}=\left\lfloor Bn^{\frac {4}{d+4}}\right\rfloor }"></span> offers a trade off between the two terms in the above display, for which the <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle k^{*}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k^{*}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc02328d1105a031ca024abcb86629ea4edb3cc8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.265ex; height:2.343ex;" alt="{\displaystyle k^{*}}"></span>-nearest neighbour error converges to the Bayes error at the optimal (<a href="/wiki/Minimax" title="Minimax">minimax</a>) rate <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {O}}\left(n^{-{\frac {4}{d+4}}}\right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>
          </mrow>
        </mrow>
        <mrow>
          <mo>(</mo>
          <msup>
            <mi>n</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>&#x2212;<!-- − --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mfrac>
                  <mn>4</mn>
                  <mrow>
                    <mi>d</mi>
                    <mo>+</mo>
                    <mn>4</mn>
                  </mrow>
                </mfrac>
              </mrow>
            </mrow>
          </msup>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {O}}\left(n^{-{\frac {4}{d+4}}}\right)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b40602503f4787c2f08c8d1d98e14d2f0f8e214" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.505ex; width:11.803ex; height:6.176ex;" alt="{\displaystyle {\mathcal {O}}\left(n^{-{\frac {4}{d+4}}}\right)}"></span>.
</p>
<div class="mw-heading mw-heading2"><h2 id="Metric_learning">Metric learning</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=8" title="Edit section: Metric learning"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The K-nearest neighbor classification performance can often be significantly improved through (<a href="/wiki/Supervised_learning" title="Supervised learning">supervised</a>) metric learning. Popular algorithms are <a href="/wiki/Neighbourhood_components_analysis" title="Neighbourhood components analysis">neighbourhood components analysis</a> and <a href="/wiki/Large_margin_nearest_neighbor" title="Large margin nearest neighbor">large margin nearest neighbor</a>. Supervised metric learning algorithms use the label information to learn a new <a href="/wiki/Metric_(mathematics)" class="mw-redirect" title="Metric (mathematics)">metric</a> or <a href="/wiki/Pseudometric_space" title="Pseudometric space">pseudo-metric</a>.
</p>
<div class="mw-heading mw-heading2"><h2 id="Feature_extraction">Feature extraction</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=9" title="Edit section: Feature extraction"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called <a href="/wiki/Feature_extraction" class="mw-redirect" title="Feature extraction">feature extraction</a>. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying <i>k</i>-NN algorithm on the transformed data in <a href="/wiki/Feature_space" class="mw-redirect" title="Feature space">feature space</a>.
</p><p>An example of a typical <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> computation pipeline for <a href="/wiki/Facial_recognition_system" title="Facial recognition system">face recognition</a> using <i>k</i>-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with <a href="/wiki/OpenCV" title="OpenCV">OpenCV</a>):
</p>
<ol><li><a href="/wiki/Haar_wavelet" title="Haar wavelet">Haar</a> face detection</li>
<li><a href="/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a> tracking analysis</li>
<li><a href="/wiki/Principal_Component_Analysis" class="mw-redirect" title="Principal Component Analysis">PCA</a> or <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Fisher LDA</a> projection into feature space, followed by <i>k</i>-NN classification</li></ol>
<div class="mw-heading mw-heading2"><h2 id="Dimension_reduction">Dimension reduction</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=10" title="Edit section: Dimension reduction"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>For high-dimensional data (e.g., with number of dimensions more than 10) <a href="/wiki/Dimension_reduction" class="mw-redirect" title="Dimension reduction">dimension reduction</a> is usually performed prior to applying the <i>k</i>-NN algorithm in order to avoid the effects of the <a href="/wiki/Curse_of_Dimensionality" class="mw-redirect" title="Curse of Dimensionality">curse of dimensionality</a>.<sup id="cite&#95;ref-18" class="reference"><a href="#cite_note-18"><span class="cite-bracket">&#91;</span>18<span class="cite-bracket">&#93;</span></a></sup>
</p><p>The <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a> in the <i>k</i>-NN context basically means that <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same).
</p><p><a href="/wiki/Feature_extraction" class="mw-redirect" title="Feature extraction">Feature extraction</a> and dimension reduction can be combined in one step using <a href="/wiki/Principal_Component_Analysis" class="mw-redirect" title="Principal Component Analysis">principal component analysis</a> (PCA),  <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">linear discriminant analysis</a> (LDA), or <a href="/wiki/Canonical_correlation" title="Canonical correlation">canonical correlation analysis</a> (CCA) techniques as a pre-processing step, followed by clustering by <i>k</i>-NN on <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature vectors</a> in reduced-dimension space. This process is also called low-dimensional <a href="/wiki/Embedding" title="Embedding">embedding</a>.<sup id="cite&#95;ref-19" class="reference"><a href="#cite_note-19"><span class="cite-bracket">&#91;</span>19<span class="cite-bracket">&#93;</span></a></sup>
</p><p>For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional <a href="/wiki/Time_series" title="Time series">time series</a>) running a fast <b>approximate</b> <i>k</i>-NN search using <a href="/wiki/Locality_Sensitive_Hashing" class="mw-redirect" title="Locality Sensitive Hashing">locality sensitive hashing</a>, "random projections",<sup id="cite&#95;ref-20" class="reference"><a href="#cite_note-20"><span class="cite-bracket">&#91;</span>20<span class="cite-bracket">&#93;</span></a></sup> "sketches"<sup id="cite&#95;ref-21" class="reference"><a href="#cite_note-21"><span class="cite-bracket">&#91;</span>21<span class="cite-bracket">&#93;</span></a></sup> or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option.
</p>
<div class="mw-heading mw-heading2"><h2 id="Decision_boundary">Decision boundary</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=11" title="Edit section: Decision boundary"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Nearest neighbor rules in effect implicitly compute the <a href="/wiki/Decision_boundary" title="Decision boundary">decision boundary</a>. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.<sup id="cite&#95;ref-22" class="reference"><a href="#cite_note-22"><span class="cite-bracket">&#91;</span>22<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Data_reduction">Data reduction</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=12" title="Edit section: Data reduction"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p><a href="/wiki/Data_reduction" title="Data reduction">Data reduction</a> is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the <i>prototypes</i> and can be found as follows:
</p>
<ol><li>Select the <i>class-outliers</i>, that is, training data that are classified incorrectly by <i>k</i>-NN (for a given <i>k</i>)</li>
<li>Separate the rest of the data into two sets: (i) the prototypes that are used for the classification decisions and (ii) the <i>absorbed points</i> that can be correctly classified by <i>k</i>-NN using prototypes. The absorbed points can then be removed from the training set.</li></ol>
<div class="mw-heading mw-heading3"><h3 id="Selection_of_class-outliers">Selection of class-outliers</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=13" title="Edit section: Selection of class-outliers"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include:
</p>
<ul><li>random error</li>
<li>insufficient training examples of this class (an isolated example appears instead of a cluster)</li>
<li>missing important features (the classes are separated in other dimensions which we don't know)</li>
<li>too many training examples of other classes (unbalanced classes) that create a "hostile" background for the given small class</li></ul>
<p>Class outliers with <i>k</i>-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, <i>k</i>&gt;<i>r</i>&gt;0, a training example is called a (<i>k</i>,<i>r</i>)NN class-outlier if its <i>k</i> nearest neighbors include more than <i>r</i> examples of other classes.
</p>
<div class="mw-heading mw-heading3"><h3 id="Condensed_Nearest_Neighbor_for_data_reduction">Condensed Nearest Neighbor for data reduction</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=14" title="Edit section: Condensed Nearest Neighbor for data reduction"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Condensed nearest neighbor (CNN, the <i><a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart</a> algorithm</i>) is an algorithm designed to reduce the data set for <i>k</i>-NN classification.<sup id="cite&#95;ref-23" class="reference"><a href="#cite_note-23"><span class="cite-bracket">&#91;</span>23<span class="cite-bracket">&#93;</span></a></sup> It selects the set of prototypes <i>U</i> from the training data, such that 1NN with <i>U</i> can classify the examples almost as accurately as 1NN does with the whole data set.
</p>
<figure class="mw-halign-right" typeof="mw:File/Thumb"><a href="/wiki/File:BorderRAtio.PNG" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/e/e6/BorderRAtio.PNG" decoding="async" width="130" height="104" class="mw-file-element" data-file-width="159" data-file-height="127" /></a><figcaption>Calculation of the border ratio</figcaption></figure>
<figure class="mw-halign-right" typeof="mw:File/Thumb"><a href="/wiki/File:PointsTypes.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/e/e7/PointsTypes.png" decoding="async" width="130" height="59" class="mw-file-element" data-file-width="130" data-file-height="59" /></a><figcaption>Three types of points: prototypes, class-outliers, and absorbed points.</figcaption></figure>
<p>Given a training set <i>X</i>, CNN works iteratively:
</p>
<ol><li>Scan all elements of <i>X</i>, looking for an element <i>x</i> whose nearest prototype from <i>U</i> has a different label than <i>x</i>.</li>
<li>Remove <i>x</i> from <i>X</i> and add it to <i>U</i></li>
<li>Repeat the scan until no more prototypes are added to <i>U</i>.</li></ol>
<p>Use <i>U</i> instead of <i>X</i> for classification. The examples that are not prototypes are called "absorbed" points.
</p><p>It is efficient to scan the training examples in order of decreasing border ratio.<sup id="cite&#95;ref-MirkesKnn&#95;24-0" class="reference"><a href="#cite_note-MirkesKnn-24"><span class="cite-bracket">&#91;</span>24<span class="cite-bracket">&#93;</span></a></sup> The border ratio of a training example <i>x</i> is defined as 
</p>
<style data-mw-deduplicate="TemplateStyles:r996643573">.mw-parser-output .block-indent{padding-left:3em;padding-right:0;overflow:hidden}</style><div class="block-indent" style="padding-left: 1.5em;"><span class="texhtml"><i>a</i>(<i>x</i>) = <style data-mw-deduplicate="TemplateStyles:r1214402035">.mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num{display:block;line-height:1em;margin:0.0em 0.1em;border-bottom:1px solid}.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0.1em 0.1em}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}</style><span class="sfrac">&#8288;<span class="tion"><span class="num">&#x2016;<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x'-y</i></span>&#x2016;</span><span class="sr-only">/</span><span class="den">&#x2016;<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>&#x2016;</span></span>&#8288;</span></span></div>
<p>where <span class="texhtml">&#x2016;<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>&#x2016;</span> is the distance to the closest example <i>y</i> having a different color than <i>x</i>, and <span class="texhtml">&#x2016;<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x'-y</i></span>&#x2016;</span> is the distance from <i>y</i> to its closest example <i>x' </i> with the same label as <i>x</i>.
</p><p>The border ratio is in the interval [0,1] because <span class="texhtml">&#x2016;<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x'-y</i></span>&#x2016;</span> never exceeds <span class="texhtml">&#x2016;<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>&#x2016;</span>. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes <i>U</i>. A point of a different label than <i>x</i> is called external to <i>x</i>. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is <i>x</i> and its label is red. External points are blue and green. The closest to <i>x</i> external point is <i>y</i>. The closest to <i>y</i> red point is <i>x' </i>. The border ratio <span class="texhtml"><i>a</i>(<i>x</i>) = &#x2016;<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x'-y</i></span>&#x2016; / &#x2016;<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>&#x2016;</span>is the attribute of the initial point <i>x</i>.
</p><p>Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet.<sup id="cite&#95;ref-MirkesKnn&#95;24-1" class="reference"><a href="#cite_note-MirkesKnn-24"><span class="cite-bracket">&#91;</span>24<span class="cite-bracket">&#93;</span></a></sup>
</p>
<ul class="gallery mw-gallery-traditional">
	<li class="gallerycaption">CNN model reduction for k-NN classifiers</li>
		<li class="gallerybox" style="width: 235px">
			<div class="thumb" style="width: 230px; height: 150px;"><span typeof="mw:File"><a href="/wiki/File:Data3classes.png" class="mw-file-description" title="Fig. 1. The dataset."><img alt="Fig. 1. The dataset." src="//upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/250px-Data3classes.png" decoding="async" width="182" height="120" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/330px-Data3classes.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/500px-Data3classes.png 2x" data-file-width="602" data-file-height="397" /></a></span></div>
			<div class="gallerytext">Fig. 1. The dataset.</div>
		</li>
		<li class="gallerybox" style="width: 235px">
			<div class="thumb" style="width: 230px; height: 150px;"><span typeof="mw:File"><a href="/wiki/File:Map1NN.png" class="mw-file-description" title="Fig. 2. The 1NN classification map."><img alt="Fig. 2. The 1NN classification map." src="//upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/250px-Map1NN.png" decoding="async" width="183" height="120" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/330px-Map1NN.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/500px-Map1NN.png 2x" data-file-width="603" data-file-height="397" /></a></span></div>
			<div class="gallerytext">Fig. 2. The 1NN classification map.</div>
		</li>
		<li class="gallerybox" style="width: 235px">
			<div class="thumb" style="width: 230px; height: 150px;"><span typeof="mw:File"><a href="/wiki/File:Map5NN.png" class="mw-file-description" title="Fig. 3. The 5NN classification map."><img alt="Fig. 3. The 5NN classification map." src="//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/250px-Map5NN.png" decoding="async" width="183" height="120" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/330px-Map5NN.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/500px-Map5NN.png 2x" data-file-width="602" data-file-height="396" /></a></span></div>
			<div class="gallerytext">Fig. 3. The 5NN classification map.</div>
		</li>
		<li class="gallerybox" style="width: 235px">
			<div class="thumb" style="width: 230px; height: 150px;"><span typeof="mw:File"><a href="/wiki/File:ReducedDataSet.png" class="mw-file-description" title="Fig. 4. The CNN reduced dataset."><img alt="Fig. 4. The CNN reduced dataset." src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/250px-ReducedDataSet.png" decoding="async" width="182" height="120" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/330px-ReducedDataSet.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/500px-ReducedDataSet.png 2x" data-file-width="608" data-file-height="402" /></a></span></div>
			<div class="gallerytext">Fig. 4. The CNN reduced dataset.</div>
		</li>
		<li class="gallerybox" style="width: 235px">
			<div class="thumb" style="width: 230px; height: 150px;"><span typeof="mw:File"><a href="/wiki/File:Map1NNReducedDataSet.png" class="mw-file-description" title="Fig. 5. The 1NN classification map based on the CNN extracted prototypes."><img alt="Fig. 5. The 1NN classification map based on the CNN extracted prototypes." src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/250px-Map1NNReducedDataSet.png" decoding="async" width="183" height="120" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/330px-Map1NNReducedDataSet.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/500px-Map1NNReducedDataSet.png 2x" data-file-width="611" data-file-height="401" /></a></span></div>
			<div class="gallerytext">Fig. 5. The 1NN classification map based on the CNN extracted prototypes.</div>
		</li>
</ul>
<div class="mw-heading mw-heading2"><h2 id="k-NN_regression"><i>k</i>-NN regression</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=15" title="Edit section: k-NN regression"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1320445320" /><div role="note" class="hatnote navigation-not-searchable">Further information: <a href="/wiki/Nearest_neighbor_smoothing" class="mw-redirect" title="Nearest neighbor smoothing">Nearest neighbor smoothing</a></div>
<p>In <i>k</i>-NN regression, also known as <i>k</i>-NN smoothing, the <i>k</i>-NN algorithm is used for estimating <a href="/wiki/Continuous_variable" class="mw-redirect" title="Continuous variable">continuous variables</a>.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2019)">citation needed</span></a></i>&#93;</sup> One such algorithm uses a weighted average of the <i>k</i> nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows:
</p>
<ol><li>Compute the Euclidean or <a href="/wiki/Mahalanobis_distance" title="Mahalanobis distance">Mahalanobis distance</a> from the query example to the labeled examples.</li>
<li>Order the labeled examples by increasing distance.</li>
<li>Find a heuristically optimal number <i>k</i> of nearest neighbors, based on <a href="/wiki/RMSE" class="mw-redirect" title="RMSE">RMSE</a>. This is done using cross validation.</li>
<li>Calculate an inverse distance weighted average with the <i>k</i>-nearest multivariate neighbors.</li></ol>
<div class="mw-heading mw-heading2"><h2 id="k-NN_outlier"><i>k</i>-NN outlier</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=16" title="Edit section: k-NN outlier"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The distance to the <i>k</i>th nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>. The larger the distance to the <i>k</i>-NN, the lower the local density, the more likely the query point is an outlier.<sup id="cite&#95;ref-25" class="reference"><a href="#cite_note-25"><span class="cite-bracket">&#91;</span>25<span class="cite-bracket">&#93;</span></a></sup> Although quite simple, this outlier model, along with another classic data mining method, <a href="/wiki/Local_outlier_factor" title="Local outlier factor">local outlier factor</a>, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.<sup id="cite&#95;ref-CamposZimek2016&#95;26-0" class="reference"><a href="#cite_note-CamposZimek2016-26"><span class="cite-bracket">&#91;</span>26<span class="cite-bracket">&#93;</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Validation_of_results">Validation of results</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=17" title="Edit section: Validation of results"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>A <a href="/wiki/Confusion_matrix" title="Confusion matrix">confusion matrix</a> or "matching matrix" is often used as a tool to validate the accuracy of <i>k</i>-NN classification. More robust statistical methods such as <a href="/wiki/Likelihood-ratio_test" title="Likelihood-ratio test">likelihood-ratio test</a> can also be applied.<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="Please clarify the preceding statement or statements with a good explanation from a reliable source. (July 2020)">how?</span></a></i>&#93;</sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="See_also">See also</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=18" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1266661725">.mw-parser-output .portalbox{padding:0;margin:0.5em 0;display:table;box-sizing:border-box;max-width:175px;list-style:none}.mw-parser-output .portalborder{border:1px solid var(--border-color-base,#a2a9b1);padding:0.1em;background:var(--background-color-neutral-subtle,#f8f9fa)}.mw-parser-output .portalbox-entry{display:table-row;font-size:85%;line-height:110%;height:1.9em;font-style:italic;font-weight:bold}.mw-parser-output .portalbox-image{display:table-cell;padding:0.2em;vertical-align:middle;text-align:center}.mw-parser-output .portalbox-link{display:table-cell;padding:0.2em 0.2em 0.2em 0.3em;vertical-align:middle}@media(min-width:720px){.mw-parser-output .portalleft{margin:0.5em 1em 0.5em 0}.mw-parser-output .portalright{clear:right;float:right;margin:0.5em 0 0.5em 1em}}</style><ul role="navigation" aria-label="Portals" class="noprint portalbox portalborder portalright">
<li class="portalbox-entry"><span class="portalbox-image"><span class="skin-invert-image noviewer" typeof="mw:File"><a href="/wiki/File:Nuvola_apps_edu_mathematics_blue-p.svg" class="mw-file-description"><img alt="icon" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Nuvola_apps_edu_mathematics_blue-p.svg/40px-Nuvola_apps_edu_mathematics_blue-p.svg.png" decoding="async" width="28" height="28" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Nuvola_apps_edu_mathematics_blue-p.svg/60px-Nuvola_apps_edu_mathematics_blue-p.svg.png 1.5x" data-file-width="128" data-file-height="128" /></a></span></span><span class="portalbox-link"><a href="/wiki/Portal:Mathematics" title="Portal:Mathematics">Mathematics portal</a></span></li></ul>
<ul><li><a href="/wiki/Nearest_centroid_classifier" title="Nearest centroid classifier">Nearest centroid classifier</a></li>
<li><a href="/wiki/Closest_pair_of_points_problem" title="Closest pair of points problem">Closest pair of points problem</a></li>
<li><a href="/wiki/Nearest_neighbor_graph" title="Nearest neighbor graph">Nearest neighbor graph</a></li>
<li><a href="/wiki/Segmentation-based_object_categorization" title="Segmentation-based object categorization">Segmentation-based object categorization</a></li></ul>
<div style="clear:right;" class=""></div>
<div class="mw-heading mw-heading2"><h2 id="References">References</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=19" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1327269900">.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}body.skin-vector-2022 .mw-parser-output .reflist-columns-2{column-width:27em}body.skin-vector-2022 .mw-parser-output .reflist-columns-3{column-width:22.5em}.mw-parser-output .references[data-mw-group=upper-alpha]{list-style-type:upper-alpha}.mw-parser-output .references[data-mw-group=upper-roman]{list-style-type:upper-roman}.mw-parser-output .references[data-mw-group=lower-alpha]{list-style-type:lower-alpha}.mw-parser-output .references[data-mw-group=lower-greek]{list-style-type:lower-greek}.mw-parser-output .references[data-mw-group=lower-roman]{list-style-type:lower-roman}.mw-parser-output div.reflist-liststyle-upper-alpha .references{list-style-type:upper-alpha}.mw-parser-output div.reflist-liststyle-upper-roman .references{list-style-type:upper-roman}.mw-parser-output div.reflist-liststyle-lower-alpha .references{list-style-type:lower-alpha}.mw-parser-output div.reflist-liststyle-lower-greek .references{list-style-type:lower-greek}.mw-parser-output div.reflist-liststyle-lower-roman .references{list-style-type:lower-roman}</style><div>
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite&#95;note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1333433106">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#bf3c2c)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#bf3c2c)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}</style><cite id="CITEREFFixHodges1951" class="citation report cs1">Fix, Evelyn; Hodges, Joseph L. (1951). <a rel="nofollow" class="external text" href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a800276.pdf">Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties</a> <span class="cs1-format">(PDF)</span> (Report). USAF School of Aviation Medicine, Randolph Field, Texas. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20200211065121/https://apps.dtic.mil/dtic/tr/fulltext/u2/a800276.pdf">Archived</a> <span class="cs1-format">(PDF)</span> from the original on February 11, 2020.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Discriminatory+Analysis.+Nonparametric+Discrimination%3A+Consistency+Properties&amp;rft.pub=USAF+School+of+Aviation+Medicine%2C+Randolph+Field%2C+Texas&amp;rft.date=1951&amp;rft.aulast=Fix&amp;rft.aufirst=Evelyn&amp;rft.au=Hodges%2C+Joseph+L.&amp;rft&#95;id=https%3A%2F%2Fapps.dtic.mil%2Fdtic%2Ftr%2Ffulltext%2Fu2%2Fa800276.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-:1-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_2-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFCoverHart1967" class="citation journal cs1"><a href="/wiki/Thomas_M._Cover" title="Thomas M. Cover">Cover, Thomas M.</a>; <a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart, Peter E.</a> (1967). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20181223080756/http://ssg.mit.edu/cal/abs/2000_spring/np_dens/classification/cover67.pdf">"Nearest neighbor pattern classification"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Information Theory</i>. <b>13</b> (1): <span class="nowrap">21–</span>27. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.2616">10.1.1.68.2616</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTIT.1967.1053964">10.1109/TIT.1967.1053964</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:5246200">5246200</a>. Archived from <a rel="nofollow" class="external text" href="http://ssg.mit.edu/cal/abs/2000_spring/np_dens/classification/cover67.pdf">the original</a> <span class="cs1-format">(PDF)</span> on 2018-12-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2018-05-24</span></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.atitle=Nearest+neighbor+pattern+classification&amp;rft.volume=13&amp;rft.issue=1&amp;rft.pages=21-27&amp;rft.date=1967&amp;rft&#95;id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.68.2616%23id-name%3DCiteSeerX&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A5246200%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1109%2FTIT.1967.1053964&amp;rft.aulast=Cover&amp;rft.aufirst=Thomas+M.&amp;rft.au=Hart%2C+Peter+E.&amp;rft&#95;id=http%3A%2F%2Fssg.mit.edu%2Fcal%2Fabs%2F2000&#95;spring%2Fnp&#95;dens%2Fclassification%2Fcover67.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text">This scheme is a generalization of linear interpolation.</span>
</li>
<li id="cite&#95;note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFHastie,&#95;Trevor.2001" class="citation book cs1">Hastie, Trevor. (2001). <i>The elements of statistical learning&#160;: data mining, inference, and prediction&#160;: with 200 full-color illustrations</i>. Tibshirani, Robert., Friedman, J. H. (Jerome H.). New York: Springer. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-387-95284-5" title="Special:BookSources/0-387-95284-5"><bdi>0-387-95284-5</bdi></a>. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/oclc/46809224">46809224</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+elements+of+statistical+learning+%3A+data+mining%2C+inference%2C+and+prediction+%3A+with+200+full-color+illustrations&amp;rft.place=New+York&amp;rft.pub=Springer&amp;rft.date=2001&amp;rft&#95;id=info%3Aoclcnum%2F46809224&amp;rft.isbn=0-387-95284-5&amp;rft.au=Hastie%2C+Trevor.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFJaskowiakCampello2011" class="citation journal cs1">Jaskowiak, Pablo A.; Campello, Ricardo J. G. B. (2011). "Comparing Correlation Coefficients as Dissimilarity Measures for Cancer Classification in Gene Expression Data". <i>Brazilian Symposium on Bioinformatics (BSB 2011)</i>: <span class="nowrap">1–</span>8. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.208.993">10.1.1.208.993</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Brazilian+Symposium+on+Bioinformatics+%28BSB+2011%29&amp;rft.atitle=Comparing+Correlation+Coefficients+as+Dissimilarity+Measures+for+Cancer+Classification+in+Gene+Expression+Data&amp;rft.pages=1-8&amp;rft.date=2011&amp;rft&#95;id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.208.993%23id-name%3DCiteSeerX&amp;rft.aulast=Jaskowiak&amp;rft.aufirst=Pablo+A.&amp;rft.au=Campello%2C+Ricardo+J.+G.+B.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text">Helliwell, J. F., Layard, R., Sachs, J. D., Aknin, L. B., De Neve, J.-E., &amp; Wang, S. (Eds.). (2023). World Happiness Report 2023 (11th ed.). Sustainable Development Solutions Network.</span>
</li>
<li id="cite&#95;note-Coomans&#95;Massart1982-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-Coomans_Massart1982_7-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFCoomansMassart1982" class="citation journal cs1">Coomans, Danny; Massart, Desire L. (1982). "Alternative k-nearest neighbour rules in supervised pattern recognition&#160;: Part 1. k-Nearest neighbour classification by using alternative voting rules". <i><a href="/wiki/Analytica_Chimica_Acta" title="Analytica Chimica Acta">Analytica Chimica Acta</a></i>. <b>136</b>: <span class="nowrap">15–</span>27. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/1982AcAC..136...15C">1982AcAC..136...15C</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2FS0003-2670%2801%2995359-0">10.1016/S0003-2670(01)95359-0</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Analytica+Chimica+Acta&amp;rft.atitle=Alternative+k-nearest+neighbour+rules+in+supervised+pattern+recognition+%3A+Part+1.+k-Nearest+neighbour+classification+by+using+alternative+voting+rules&amp;rft.volume=136&amp;rft.pages=15-27&amp;rft.date=1982&amp;rft&#95;id=info%3Adoi%2F10.1016%2FS0003-2670%2801%2995359-0&amp;rft&#95;id=info%3Abibcode%2F1982AcAC..136...15C&amp;rft.aulast=Coomans&amp;rft.aufirst=Danny&amp;rft.au=Massart%2C+Desire+L.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Everitt, Brian S.; Landau, Sabine; Leese, Morven; and Stahl, Daniel (2011) "Miscellaneous Clustering Methods", in <i>Cluster Analysis</i>, 5th Edition, John Wiley &amp; Sons, Ltd., Chichester, UK</span>
</li>
<li id="cite&#95;note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFNigschBendervan&#95;BuurenTissen2006" class="citation journal cs1">Nigsch, Florian; Bender, Andreas; van Buuren, Bernd; Tissen, Jos; Nigsch, Eduard; Mitchell, John B. O. (2006). "Melting point prediction employing k-nearest neighbor algorithms and genetic parameter optimization". <i>Journal of Chemical Information and Modeling</i>. <b>46</b> (6): <span class="nowrap">2412–</span>2422. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1021%2Fci060149f">10.1021/ci060149f</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/17125183">17125183</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Chemical+Information+and+Modeling&amp;rft.atitle=Melting+point+prediction+employing+k-nearest+neighbor+algorithms+and+genetic+parameter+optimization&amp;rft.volume=46&amp;rft.issue=6&amp;rft.pages=2412-2422&amp;rft.date=2006&amp;rft&#95;id=info%3Adoi%2F10.1021%2Fci060149f&amp;rft&#95;id=info%3Apmid%2F17125183&amp;rft.aulast=Nigsch&amp;rft.aufirst=Florian&amp;rft.au=Bender%2C+Andreas&amp;rft.au=van+Buuren%2C+Bernd&amp;rft.au=Tissen%2C+Jos&amp;rft.au=Nigsch%2C+Eduard&amp;rft.au=Mitchell%2C+John+B.+O.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-HPS2008-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-HPS2008_10-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFHallParkSamworth2008" class="citation journal cs1">Hall, Peter; Park, Byeong U.; Samworth, Richard J. (2008). "Choice of neighbor order in nearest-neighbor classification". <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>36</b> (5): <span class="nowrap">2135–</span>2152. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/0810.5276">0810.5276</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2008arXiv0810.5276H">2008arXiv0810.5276H</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1214%2F07-AOS537">10.1214/07-AOS537</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:14059866">14059866</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Choice+of+neighbor+order+in+nearest-neighbor+classification&amp;rft.volume=36&amp;rft.issue=5&amp;rft.pages=2135-2152&amp;rft.date=2008&amp;rft&#95;id=info%3Aarxiv%2F0810.5276&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A14059866%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1214%2F07-AOS537&amp;rft&#95;id=info%3Abibcode%2F2008arXiv0810.5276H&amp;rft.aulast=Hall&amp;rft.aufirst=Peter&amp;rft.au=Park%2C+Byeong+U.&amp;rft.au=Samworth%2C+Richard+J.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Stone-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-Stone_11-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFStone1977" class="citation journal cs1">Stone, Charles J. (1977). <a rel="nofollow" class="external text" href="https://doi.org/10.1214%2Faos%2F1176343886">"Consistent nonparametric regression"</a>. <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>5</b> (4): <span class="nowrap">595–</span>620. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1214%2Faos%2F1176343886">10.1214/aos/1176343886</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Consistent+nonparametric+regression&amp;rft.volume=5&amp;rft.issue=4&amp;rft.pages=595-620&amp;rft.date=1977&amp;rft&#95;id=info%3Adoi%2F10.1214%2Faos%2F1176343886&amp;rft.aulast=Stone&amp;rft.aufirst=Charles+J.&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.1214%252Faos%252F1176343886&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Samworth12-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-Samworth12_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Samworth12_12-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFSamworth2012" class="citation journal cs1">Samworth, Richard J. (2012). "Optimal weighted nearest neighbour classifiers". <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>40</b> (5): <span class="nowrap">2733–</span>2763. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1101.5783">1101.5783</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1214%2F12-AOS1049">10.1214/12-AOS1049</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:88511688">88511688</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Optimal+weighted+nearest+neighbour+classifiers&amp;rft.volume=40&amp;rft.issue=5&amp;rft.pages=2733-2763&amp;rft.date=2012&amp;rft&#95;id=info%3Aarxiv%2F1101.5783&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A88511688%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1214%2F12-AOS1049&amp;rft.aulast=Samworth&amp;rft.aufirst=Richard+J.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Terrell&#95;Scott1992-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-Terrell_Scott1992_13-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFTerrellScott1992" class="citation journal cs1">Terrell, George R.; Scott, David W. (1992). <a rel="nofollow" class="external text" href="https://doi.org/10.1214%2Faos%2F1176348768">"Variable kernel density estimation"</a>. <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>20</b> (3): <span class="nowrap">1236–</span>1265. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1214%2Faos%2F1176348768">10.1214/aos/1176348768</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Variable+kernel+density+estimation&amp;rft.volume=20&amp;rft.issue=3&amp;rft.pages=1236-1265&amp;rft.date=1992&amp;rft&#95;id=info%3Adoi%2F10.1214%2Faos%2F1176348768&amp;rft.aulast=Terrell&amp;rft.aufirst=George+R.&amp;rft.au=Scott%2C+David+W.&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.1214%252Faos%252F1176348768&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-Mills2010-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-Mills2010_14-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFMills2012" class="citation journal cs1">Mills, Peter (2012-08-09). <a rel="nofollow" class="external text" href="https://archive.org/details/arxiv-1202.2194">"Efficient statistical classification of satellite measurements"</a>. <i>International Journal of Remote Sensing</i>. <b>32</b> (21): <span class="nowrap">6109–</span>6132. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1202.2194">1202.2194</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F01431161.2010.507795">10.1080/01431161.2010.507795</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Remote+Sensing&amp;rft.atitle=Efficient+statistical+classification+of+satellite+measurements&amp;rft.volume=32&amp;rft.issue=21&amp;rft.pages=6109-6132&amp;rft.date=2012-08-09&amp;rft&#95;id=info%3Aarxiv%2F1202.2194&amp;rft&#95;id=info%3Adoi%2F10.1080%2F01431161.2010.507795&amp;rft.aulast=Mills&amp;rft.aufirst=Peter&amp;rft&#95;id=https%3A%2F%2Farchive.org%2Fdetails%2Farxiv-1202.2194&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFToussaint2005" class="citation journal cs1">Toussaint, Godfried T. (April 2005). "Geometric proximity graphs for improving nearest neighbor methods in instance-based learning and data mining". <i>International Journal of Computational Geometry and Applications</i>. <b>15</b> (2): <span class="nowrap">101–</span>150. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1142%2FS0218195905001622">10.1142/S0218195905001622</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Computational+Geometry+and+Applications&amp;rft.atitle=Geometric+proximity+graphs+for+improving+nearest+neighbor+methods+in+instance-based+learning+and+data+mining&amp;rft.volume=15&amp;rft.issue=2&amp;rft.pages=101-150&amp;rft.date=2005-04&amp;rft&#95;id=info%3Adoi%2F10.1142%2FS0218195905001622&amp;rft.aulast=Toussaint&amp;rft.aufirst=Godfried+T.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">Devroye, L., Gyorfi, L. &amp; Lugosi, G. A Probabilistic Theory of Pattern Recognition. Discrete Appl Math 73, 192–194 (1997).</span>
</li>
<li id="cite&#95;note-PTPR-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-PTPR_17-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFDevroyeGyorfiLugosi1996" class="citation book cs1">Devroye, Luc; Gyorfi, Laszlo; Lugosi, Gabor (1996). <i>A probabilistic theory of pattern recognition</i>. Springer. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-3879-4618-4" title="Special:BookSources/978-0-3879-4618-4"><bdi>978-0-3879-4618-4</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+probabilistic+theory+of+pattern+recognition&amp;rft.pub=Springer&amp;rft.date=1996&amp;rft.isbn=978-0-3879-4618-4&amp;rft.aulast=Devroye&amp;rft.aufirst=Luc&amp;rft.au=Gyorfi%2C+Laszlo&amp;rft.au=Lugosi%2C+Gabor&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBeyer" class="citation journal cs1">Beyer, Kevin; et&#160;al. <a rel="nofollow" class="external text" href="https://minds.wisconsin.edu/bitstream/handle/1793/60174/TR1377.pdf?sequence=1">"When is "nearest neighbor" meaningful?"</a> <span class="cs1-format">(PDF)</span>. <i>Database Theory—ICDT'99</i>. <b>1999</b>: <span class="nowrap">217–</span>235.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Database+Theory%E2%80%94ICDT%2799&amp;rft.atitle=When+is+%22nearest+neighbor%22+meaningful%3F&amp;rft.volume=1999&amp;rft.pages=217-235&amp;rft.aulast=Beyer&amp;rft.aufirst=Kevin&amp;rft&#95;id=https%3A%2F%2Fminds.wisconsin.edu%2Fbitstream%2Fhandle%2F1793%2F60174%2FTR1377.pdf%3Fsequence%3D1&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFShawJebara2009" class="citation cs2">Shaw, Blake; Jebara, Tony (2009), <a rel="nofollow" class="external text" href="http://www.cs.columbia.edu/~jebara/papers/spe-icml09.pdf">"Structure preserving embedding"</a> <span class="cs1-format">(PDF)</span>, <i>Proceedings of the 26th Annual International Conference on Machine Learning</i> (published June 2009), pp.&#160;<span class="nowrap">1–</span>8, <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F1553374.1553494">10.1145/1553374.1553494</a>, <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781605585161" title="Special:BookSources/9781605585161"><bdi>9781605585161</bdi></a>, <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:8522279">8522279</a></cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+26th+Annual+International+Conference+on+Machine+Learning&amp;rft.atitle=Structure+preserving+embedding&amp;rft.pages=1-8&amp;rft.date=2009&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A8522279%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1145%2F1553374.1553494&amp;rft.isbn=9781605585161&amp;rft.aulast=Shaw&amp;rft.aufirst=Blake&amp;rft.au=Jebara%2C+Tony&amp;rft&#95;id=http%3A%2F%2Fwww.cs.columbia.edu%2F~jebara%2Fpapers%2Fspe-icml09.pdf&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span><span class="cs1-maint citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Citation" title="Template:Citation">citation</a>}}</code>:  CS1 maint: work parameter with ISBN (<a href="/wiki/Category:CS1_maint:_work_parameter_with_ISBN" title="Category:CS1 maint: work parameter with ISBN">link</a>)</span></span>
</li>
<li id="cite&#95;note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBinghamMannila2001" class="citation book cs1">Bingham, Ella; Mannila, Heikki (2001). "Random projection in dimensionality reduction". <i>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '01</i>. pp.&#160;<span class="nowrap">245–</span>250. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F502512.502546">10.1145/502512.502546</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/158113391X" title="Special:BookSources/158113391X"><bdi>158113391X</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:1854295">1854295</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Random+projection+in+dimensionality+reduction&amp;rft.btitle=Proceedings+of+the+seventh+ACM+SIGKDD+international+conference+on+Knowledge+discovery+and+data+mining+-+KDD+%2701&amp;rft.pages=245-250&amp;rft.date=2001&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1854295%23id-name%3DS2CID&amp;rft&#95;id=info%3Adoi%2F10.1145%2F502512.502546&amp;rft.isbn=158113391X&amp;rft.aulast=Bingham&amp;rft.aufirst=Ella&amp;rft.au=Mannila%2C+Heikki&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text">Ryan, Donna (editor); <i>High Performance Discovery in Time Series</i>, Berlin: Springer, 2004, <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-387-00857-8" title="Special:BookSources/0-387-00857-8"><bdi>0-387-00857-8</bdi></a></span>
</li>
<li id="cite&#95;note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFBremnerDemaineEricksonIacono2005" class="citation journal cs1">Bremner, David; <a href="/wiki/Erik_Demaine" title="Erik Demaine">Demaine, Erik</a>; Erickson, Jeff; <a href="/wiki/John_Iacono" title="John Iacono">Iacono, John</a>; <a href="/wiki/Stefan_Langerman" title="Stefan Langerman">Langerman, Stefan</a>; <a href="/wiki/Pat_Morin" title="Pat Morin">Morin, Pat</a>; <a href="/wiki/Godfried_Toussaint" title="Godfried Toussaint">Toussaint, Godfried T.</a> (2005). <a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00454-004-1152-0">"Output-sensitive algorithms for computing nearest-neighbor decision boundaries"</a>. <i>Discrete and Computational Geometry</i>. <b>33</b> (4): <span class="nowrap">593–</span>604. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00454-004-1152-0">10.1007/s00454-004-1152-0</a></span>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Discrete+and+Computational+Geometry&amp;rft.atitle=Output-sensitive+algorithms+for+computing+nearest-neighbor+decision+boundaries&amp;rft.volume=33&amp;rft.issue=4&amp;rft.pages=593-604&amp;rft.date=2005&amp;rft&#95;id=info%3Adoi%2F10.1007%2Fs00454-004-1152-0&amp;rft.aulast=Bremner&amp;rft.aufirst=David&amp;rft.au=Demaine%2C+Erik&amp;rft.au=Erickson%2C+Jeff&amp;rft.au=Iacono%2C+John&amp;rft.au=Langerman%2C+Stefan&amp;rft.au=Morin%2C+Pat&amp;rft.au=Toussaint%2C+Godfried+T.&amp;rft&#95;id=https%3A%2F%2Fdoi.org%2F10.1007%252Fs00454-004-1152-0&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFHart1968" class="citation journal cs1"><a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart, Peter E.</a> (1968). "The Condensed Nearest Neighbor Rule". <i>IEEE Transactions on Information Theory</i>. <b>18</b>: <span class="nowrap">515–</span>516. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTIT.1968.1054155">10.1109/TIT.1968.1054155</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.atitle=The+Condensed+Nearest+Neighbor+Rule&amp;rft.volume=18&amp;rft.pages=515-516&amp;rft.date=1968&amp;rft&#95;id=info%3Adoi%2F10.1109%2FTIT.1968.1054155&amp;rft.aulast=Hart&amp;rft.aufirst=Peter+E.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-MirkesKnn-24"><span class="mw-cite-backlink">^ <a href="#cite_ref-MirkesKnn_24-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-MirkesKnn_24-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Mirkes, Evgeny M.; <a rel="nofollow" class="external text" href="http://www.math.le.ac.uk/people/ag153/homepage/KNN/KNN3.html"><i>KNN and Potential Energy: applet</i></a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20120119201715/http://www.math.le.ac.uk/people/ag153/homepage/KNN/KNN3.html">Archived</a> 2012-01-19 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>, University of Leicester, 2011</span>
</li>
<li id="cite&#95;note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFRamaswamyRastogiShim2000" class="citation conference cs1">Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000). "Efficient algorithms for mining outliers from large data sets". <i>Proceedings of the 2000 ACM SIGMOD international conference on Management of data - SIGMOD '00</i>. Proceedings of the 2000 ACM SIGMOD international conference on Management of data – SIGMOD '00. pp.&#160;<span class="nowrap">427–</span>438. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F342009.335437">10.1145/342009.335437</a></span>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1-58113-217-4" title="Special:BookSources/1-58113-217-4"><bdi>1-58113-217-4</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Efficient+algorithms+for+mining+outliers+from+large+data+sets&amp;rft.btitle=Proceedings+of+the+2000+ACM+SIGMOD+international+conference+on+Management+of+data+-+SIGMOD+%2700&amp;rft.pages=427-438&amp;rft.date=2000&amp;rft&#95;id=info%3Adoi%2F10.1145%2F342009.335437&amp;rft.isbn=1-58113-217-4&amp;rft.aulast=Ramaswamy&amp;rft.aufirst=Sridhar&amp;rft.au=Rastogi%2C+Rajeev&amp;rft.au=Shim%2C+Kyuseok&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
<li id="cite&#95;note-CamposZimek2016-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-CamposZimek2016_26-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFCamposZimekSanderCampello2016" class="citation journal cs1">Campos, Guilherme O.; Zimek, Arthur; Sander, Jörg; Campello, Ricardo J. G. B.; Micenková, Barbora; Schubert, Erich; Assent, Ira; Houle, Michael E. (2016). "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study". <i>Data Mining and Knowledge Discovery</i>. <b>30</b> (4): <span class="nowrap">891–</span>927. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10618-015-0444-8">10.1007/s10618-015-0444-8</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/1384-5810">1384-5810</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:1952214">1952214</a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Data+Mining+and+Knowledge+Discovery&amp;rft.atitle=On+the+evaluation+of+unsupervised+outlier+detection%3A+measures%2C+datasets%2C+and+an+empirical+study&amp;rft.volume=30&amp;rft.issue=4&amp;rft.pages=891-927&amp;rft.date=2016&amp;rft&#95;id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1952214%23id-name%3DS2CID&amp;rft.issn=1384-5810&amp;rft&#95;id=info%3Adoi%2F10.1007%2Fs10618-015-0444-8&amp;rft.aulast=Campos&amp;rft.aufirst=Guilherme+O.&amp;rft.au=Zimek%2C+Arthur&amp;rft.au=Sander%2C+J%C3%B6rg&amp;rft.au=Campello%2C+Ricardo+J.+G.+B.&amp;rft.au=Micenkov%C3%A1%2C+Barbora&amp;rft.au=Schubert%2C+Erich&amp;rft.au=Assent%2C+Ira&amp;rft.au=Houle%2C+Michael+E.&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></span>
</li>
</ol></div></div>
<div class="mw-heading mw-heading2"><h2 id="Further_reading">Further reading</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=20" title="Edit section: Further reading"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFDasarathy,&#95;Belur&#95;V.1991" class="citation book cs1"><a href="/wiki/Belur_V._Dasarathy" title="Belur V. Dasarathy">Dasarathy, Belur V.</a>, ed. (1991). <i>Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques</i>. IEEE Computer Society Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0818689307" title="Special:BookSources/978-0818689307"><bdi>978-0818689307</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Nearest+Neighbor+%28NN%29+Norms%3A+NN+Pattern+Classification+Techniques&amp;rft.pub=IEEE+Computer+Society+Press&amp;rft.date=1991&amp;rft.isbn=978-0818689307&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1333433106" /><cite id="CITEREFShakhnarovich,&#95;GregoryDarrell,&#95;TrevorIndyk,&#95;Piotr2005" class="citation book cs1">Shakhnarovich, Gregory; Darrell, Trevor; Indyk, Piotr, eds. (2005). <i>Nearest-Neighbor Methods in Learning and Vision</i>. <a href="/wiki/MIT_Press" title="MIT Press">MIT Press</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0262195478" title="Special:BookSources/978-0262195478"><bdi>978-0262195478</bdi></a>.</cite><span title="ctx&#95;ver=Z39.88-2004&amp;rft&#95;val&#95;fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Nearest-Neighbor+Methods+in+Learning+and+Vision&amp;rft.pub=MIT+Press&amp;rft.date=2005&amp;rft.isbn=978-0262195478&amp;rfr&#95;id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm" class="Z3988"></span></li></ul>
<!-- 
NewPP limit report
Parsed by mw‐web.codfw.main‐6d8c8cffd‐gjgzj
Cached time: 20260224001207
Cache expiry: 85682
Reduced expiry: true
Complications: [vary‐revision‐sha1, prevent‐selective‐update, show‐toc]
CPU time usage: 0.529 seconds
Real time usage: 0.764 seconds
Preprocessor visited node count: 3041/1000000
Revision size: 33127/2097152 bytes
Post‐expand include size: 68055/2097152 bytes
Template argument size: 5131/2097152 bytes
Highest expansion depth: 13/100
Expensive parser function count: 10/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 97772/5000000 bytes
Lua time usage: 0.295/10.000 seconds
Lua memory usage: 6705691/52428800 bytes
Number of Wikibase entities loaded: 0/500
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  508.702      1 -total
 57.28%  291.401      1 Template:Reflist
 21.49%  109.336     14 Template:Cite_journal
 18.01%   91.625      1 Template:Cite_report
 12.20%   62.071      1 Template:Short_description
  8.37%   42.557      4 Template:Fix
  8.28%   42.128      3 Template:Citation_needed
  7.55%   38.384      2 Template:Pagetype
  6.39%   32.485      5 Template:Cite_book
  4.40%   22.394      1 Template:Distinguish
-->

<!-- Saved in parser cache with key enwiki:pcache:1775388:|#|:idhash:canonical and timestamp 20260224001207 and revision id 1328986210. Rendering was triggered because: page_view
 -->
</div><noscript><img src="https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?useformat=desktop&amp;type=1x1&amp;usesul3=1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=1328986210">https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=1328986210</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw-interface=""><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></li><li><a href="/wiki/Category:Search_algorithms" title="Category:Search algorithms">Search algorithms</a></li><li><a href="/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></li><li><a href="/wiki/Category:Statistical_classification" title="Category:Statistical classification">Statistical classification</a></li><li><a href="/wiki/Category:Nonparametric_statistics" title="Category:Nonparametric statistics">Nonparametric statistics</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_maint:_work_parameter_with_ISBN" title="Category:CS1 maint: work parameter with ISBN">CS1 maint: work parameter with ISBN</a></li><li><a href="/wiki/Category:Webarchive_template_wayback_links" title="Category:Webarchive template wayback links">Webarchive template wayback links</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_is_different_from_Wikidata" title="Category:Short description is different from Wikidata">Short description is different from Wikidata</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_March_2013" title="Category:Articles with unsourced statements from March 2013">Articles with unsourced statements from March 2013</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_December_2008" title="Category:Articles with unsourced statements from December 2008">Articles with unsourced statements from December 2008</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_September_2019" title="Category:Articles with unsourced statements from September 2019">Articles with unsourced statements from September 2019</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_July_2020" title="Category:Wikipedia articles needing clarification from July 2020">Wikipedia articles needing clarification from July 2020</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 22 December 2025, at 23:56<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a href="/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" title="Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License">Creative Commons Attribution-ShareAlike 4.0 License</a>;
additional terms may apply. By using this site, you agree to the <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use" class="extiw" title="foundation:Special:MyLanguage/Policy:Terms of Use">Terms of Use</a> and <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy" class="extiw" title="foundation:Special:MyLanguage/Policy:Privacy policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a rel="nofollow" class="external text" href="https://wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-legal-safety-contacts"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Legal:Wikimedia_Foundation_Legal_and_Safety_Contact_Information">Legal &amp; safety contacts</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
	<li id="footer-places-mobileview"><a href="//en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://www.wikimedia.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/static/images/footer/wikimedia-button.svg" width="84" height="29"><img src="/static/images/footer/wikimedia.svg" width="25" height="25" alt="Wikimedia Foundation" lang="en" loading="lazy"></picture></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/w/resources/assets/poweredby_mediawiki.svg" width="88" height="31"><img src="/w/resources/assets/mediawiki_compact.svg" alt="Powered by MediaWiki" lang="en" width="25" height="25" loading="lazy"></picture></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class="vector-header-container vector-sticky-header-container no-font-mode-scale">
	<div id="vector-sticky-header" class="vector-sticky-header">
		<div class="vector-sticky-header-start">
			<div class="vector-sticky-header-icon-start vector-button-flush-left" aria-hidden="true">
				<button class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-sticky-header-search-toggle" tabindex="-1" data-event-name="ui.vector-sticky-search-form.icon"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
			</button>
		</div>
			
		<div role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box">
			<div class="vector-typeahead-search-container">
				<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail">
					<form action="/w/index.php" id="vector-sticky-search-form" class="cdx-search-input cdx-search-input--has-end-button">
						<div  class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
							<div class="cdx-text-input cdx-text-input--has-start-icon">
								<input
									class="cdx-text-input__input mw-searchInput" autocomplete="off"
									
									type="search" name="search" placeholder="Search Wikipedia">
								<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
							</div>
							<input type="hidden" name="title" value="Special:Search">
						</div>
						<button class="cdx-button cdx-search-input__end-button">Search</button>
					</form>
				</div>
			</div>
		</div>
		<div class="vector-sticky-header-context-bar">
				<nav aria-label="Contents" class="vector-toc-landmark">
						
					<div id="vector-sticky-header-toc" class="vector-dropdown mw-portlet mw-portlet-sticky-header-toc vector-sticky-header-toc vector-button-flush-left"  >
						<input type="checkbox" id="vector-sticky-header-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-sticky-header-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
						<label id="vector-sticky-header-toc-label" for="vector-sticky-header-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
						</label>
						<div class="vector-dropdown-content">
					
						<div id="vector-sticky-header-toc-unpinned-container" class="vector-unpinned-container">
						</div>
					
						</div>
					</div>
			</nav>
				<div class="vector-sticky-header-context-bar-primary" aria-hidden="true" ><i>k</i>-nearest neighbors algorithm</div>
			</div>
		</div>
		<div class="vector-sticky-header-end" aria-hidden="true">
			<div class="vector-sticky-header-icons">
				<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-talk-sticky-header" tabindex="-1" data-event-name="talk-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbles mw-ui-icon-wikimedia-speechBubbles"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-subject-sticky-header" tabindex="-1" data-event-name="subject-sticky-header"><span class="vector-icon mw-ui-icon-article mw-ui-icon-wikimedia-article"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-history-sticky-header" tabindex="-1" data-event-name="history-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-history mw-ui-icon-wikimedia-wikimedia-history"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only mw-watchlink" id="ca-watchstar-sticky-header" tabindex="-1" data-event-name="watch-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-star mw-ui-icon-wikimedia-wikimedia-star"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-edit-sticky-header" tabindex="-1" data-event-name="wikitext-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-wikiText mw-ui-icon-wikimedia-wikimedia-wikiText"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-ve-edit-sticky-header" tabindex="-1" data-event-name="ve-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-edit mw-ui-icon-wikimedia-wikimedia-edit"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-viewsource-sticky-header" tabindex="-1" data-event-name="ve-edit-protected-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-editLock mw-ui-icon-wikimedia-wikimedia-editLock"></span>

<span></span>
			</a>
		</div>
			<div class="vector-sticky-header-buttons">
				<button class="cdx-button cdx-button--weight-quiet mw-interlanguage-selector" id="p-lang-btn-sticky-header" tabindex="-1" data-event-name="ui.dropdown-p-lang-btn-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-language mw-ui-icon-wikimedia-wikimedia-language"></span>

<span>24 languages</span>
			</button>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive" id="ca-addsection-sticky-header" tabindex="-1" data-event-name="addsection-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbleAdd-progressive mw-ui-icon-wikimedia-speechBubbleAdd-progressive"></span>

<span>Add topic</span>
			</a>
		</div>
			<div class="vector-sticky-header-icon-end">
				<div class="vector-user-links">
				</div>
			</div>
		</div>
	</div>
</div>
<div class="mw-portlet mw-portlet-dock-bottom emptyPortlet" id="p-dock-bottom">
	<ul>
		
	</ul>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw-web.eqiad.main-6f597bcf-59xv7","wgBackendResponseTime":251,"wgPageParseReport":{"limitreport":{"cputime":"0.529","walltime":"0.764","ppvisitednodes":{"value":3041,"limit":1000000},"revisionsize":{"value":33127,"limit":2097152},"postexpandincludesize":{"value":68055,"limit":2097152},"templateargumentsize":{"value":5131,"limit":2097152},"expansiondepth":{"value":13,"limit":100},"expensivefunctioncount":{"value":10,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":97772,"limit":5000000},"entityaccesscount":{"value":0,"limit":500},"timingprofile":["100.00%  508.702      1 -total"," 57.28%  291.401      1 Template:Reflist"," 21.49%  109.336     14 Template:Cite_journal"," 18.01%   91.625      1 Template:Cite_report"," 12.20%   62.071      1 Template:Short_description","  8.37%   42.557      4 Template:Fix","  8.28%   42.128      3 Template:Citation_needed","  7.55%   38.384      2 Template:Pagetype","  6.39%   32.485      5 Template:Cite_book","  4.40%   22.394      1 Template:Distinguish"]},"scribunto":{"limitreport-timeusage":{"value":"0.295","limit":"10.000"},"limitreport-memusage":{"value":6705691,"limit":52428800}},"cachereport":{"origin":"mw-web.codfw.main-6d8c8cffd-gjgzj","timestamp":"20260224001207","ttl":85682,"transientcontent":true}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"K-nearest neighbors algorithm","url":"https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm","sameAs":"http:\/\/www.wikidata.org\/entity\/Q1071612","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q1071612","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2005-04-21T15:50:19Z","dateModified":"2025-12-22T23:56:10Z","headline":"classification algorithm"}</script>
</body>
</html>